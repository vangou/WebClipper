# Day3a
关键词

大语言模型

预训练任务

神经网络

自注意力

下游任务

预训练模型

多头注意力

矩阵

向量数据库

位置编码

仇钧潼

00:10

啊，当然好，那个时间到了，那我们下面呢？开始第三天的课程。大家听到吧！

王蓓

00:21

声音很清晰。

风铃之音

00:21

嗯，可以听见，可以听见可以听见。

仇钧潼

00:21

喂，听到吗？诶好的嗯好行。我先说一下今天的内容吧，今天内容真的是很多。呃，我上午的话呢，大概会把大语言模型的这个原理呃，还有这个。呃，一些微调的技术啊，它的这个到底是怎么样的，会给大家先介绍一下。那涉及到我们首先讲最基础的东西呢，就是现在构成几乎所有大语言模型的部件穿双锚啊，那这个也是目前最火热的这个神经网络的架构了。然后呢，就给大家介绍这个几种这个大语言模型比较流行的啊，一个是google的。啊，那这个是最早出现的这个效果比较好的一种大语言模型，然后就是我们现在比较熟悉的这个GPT系列了，还有我们将会用到这个清华GOM，那清华GOM呢，可能今天讲不了太多，那争取下一次就明天的课呢，能够。啊，能够把它讲清楚。因为今天这个幻灯片都已经有100几页了，估计可能上午都不一定能讲完啊，那这个能讲多少就算多少。

仇钧潼

01:29

然后呢？后面呢还有这个微调啊，那微调的话呢，涉及到几种不同的微调的方式了。然后下午何老师呢会给大家呃做实验啊，带着大家呢去把这个清华的这个GOM模型呢，把它部署一遍。然后呢，就会讲这个微调了，但是这个。呃，何老师讲那个微调呢，跟我这里说的微调呢，可能有点不大一样，等一下呢，我再给大家解释。然后呃，何老师呢会给大家讲讲这个。呃，怎么样用这个？还有这个向量数据库一起来实现一个问答，机器人就是我在。第一天的时候给大家画了一张草图上的一个解决方案啊，那这个会从头到尾实现一遍。那说实话呢，上午跟下午的内容呢，以前可能是上午我讲。稍微比较理论比较idea一点的东西，下午何老师可能就会去具体的把这些idea去实现，那今天讲的东西的话呢，可能就有点不一样了，上午跟下午了，可能是一个犬牙交错的关系，这这个能重叠的地方不多。啊，那大概就是我可能呃，侧重于这种原理的讲解，那何老师可能会侧重于这种实实验的展展现。那大家呢彼此呃，各讲一部分吧，反正这个重复性呢，这个也不是很高，那甚至何老师讲的一些内容呢，可能要我。在明天这个呃才能有时有时间去把这个道理给大家讲一讲。比如说基于指定的微调的这个是什么意思？那今天估计都没有来得及讲啊，因为能讲完这些内容啊，就已经很不错了。

仇钧潼

03:11

好，那下面呢，我们开始今天的内容啊，第一个呢，首先给大家介绍一下这个。那穿穿的大概是20？118年左右这个出来的东西啊，那到今天到现在的话呢，已经有五年以上的时间。那它是这个大语言模型的基础构建，你要搞清楚什么是大语言模型，就一定要搞清楚什么是？transformer那这个对你理解，大语言模型能干什么，不能干什么，怎么才能干得好啊，那这个是非常有帮助的。

仇钧潼

03:43

呃，这个方法的这个原始论文呢是这一篇。那它的中心思想呢，就是说，呃，我们以前搞过什么很多这个卷积神经网络LSTM啊，这类的啊，还有这个，包括这个注意力机制，那它这里呢发现了一种呢叫自注意力机制，那他认为呢，其实你只要把这个自注意力机制这个事情搞。那这个一切都可以搞定，那包括这个卷积神经网络，最终呢都可以归结为自助意力机制的一个特例。啊，就是有1.1统天下的意思了，那在应用上来看呢，它确实也有点像这个一统天下的感觉，比如说它可以用在自然语言处理里面。完了这个大语言模型本身就是自然语言处理的东西啊。另外呢，它也可以用在计算机视觉里面。那现在以这个transformer作为基础的这种计算机视觉的这个呃，一些技术啊，对这个发展太迅猛了。那所以这个一个偶尔的发现呢，就突然间就好像把整个人工智能的这个面貌把它给改变了。那这个在2018年的时候呢，我就其实就很想给大家讲这个，因为看着呢就很有前途，但是呢，你单独拿这个东西出来讲了，好像又太单薄，不能组成一个课程，那这么一拖就拖了几年时间，那刚好最近呢，这个大语言模型大爆发。

仇钧潼

05:04

那刚好给这个传方法呢，有一个走上这个舞台正面的机会，那所以我今天呢就给大家介绍一下啊，那这里这个原创者呢，基本上都是google的。然后呃，其中有一个人我忘了那个是谁了，好像是第二号作者还是什么，就有一个人还跳槽去了open AI。啊，然后这个后后来好像又返回google，那这里就觉得有点惋惜，就是google呢是最先发现发明这个transformer的这个厂商，但是呢，它在这个大语言模型的探索上呢。没有这个open AI的运气那么好，那其实说实话，在技术能力上来比的话呢，呃，或者说经济能力上来比的话呢，那这个open AI。就跟那个呃，谷狗相比，就跟那个稳腿跟牛腿相比一样，那基本上是大大不如的，但不知道什么这个策略上的一些原因啊，可能这个，这个就是采取的这种方向不一样吧，就是它重视的方向不一样吧。然后呢？结果呢给这个openai捷足先登在大语言模型上呢，做出一个更好的效果。

仇钧潼

06:16

那大家也可以看到了，这个在平时很多这个自媒体里面啊，这个一些文章里面你也看到那个。呃呃，一些消息啊，比如说这个hinton，那它是google的这个名誉的这个。呃，一个领导人吧，就是也是这个神经网络里面一个很资深的元老，那今年都已经70多岁了，那这个BP神经网络呢，他就是有分一起参与这个发明的。那他后后来这个大元模型出来以后呢，他就离开了google。他就辞职了。那这里面呢，不禁让人有一种猜想，就是说，呃，是不是他觉得自己的存在，因为他比较年纪比较大了，那年纪大的话呢，就会有一种固执啊，可能会妨碍一些新生事物的发展，那他觉得自己是不是在google里面爱上了？就说，或者说他没有选择出一个给这个google指出一条正确的道路，那使得这个google在竞争里面呢，处于一种比较尴尬的一个位置。那所以他就让言了，他就辞职不干了啊，他他就回这个学校里面去任教啊，那这个不在google了啊，那这个是最近发生了一个八卦的一个事情了啊。

仇钧潼

07:26

好，那下面呢，我们来看一看这个方法到底是什么东西啊？那如果大家看了那个。呃，之前我们课前预习的那个深度学习的那个。幻灯片啊，那个那个那个那个录录像，然后呢，这个对深度学习还有神经网络有一定了解的话呢？呃，这个对于这个理解起来一点也不难，那如果你听过之前那个GPT的特训营的话，那就更加容易了，因为在特训营上面呢，其实我们也讲过，但是呢，今天讲呢，可能在之前讲的这个基础上呢，那个连贯性会更加好，然后我也增加了一些。那你听起来可能会更有新的收益。就如果你之前上次那一次听了半懂不懂的话呢？那这一次听完以后呢，我相信应该可以达到一个八成这么一个地步。

仇钧潼

08:15

好，那这个他在？屏幕的左边这个部分。那大家可以先稍微看一看，这个太远了，就是看那个摄像的话呢，估计看不清楚，那你得要看那个同步的那个直播，看你自己的幻灯片才行啊。那这个穿的方法呢？分成了左右这里两个部分。那它是一个所谓的一个架构。他一开始瞄准的目标是做机器翻译。就是一开始它定位的任务呢，是做机器翻译啊，来设计的。那左边这里呢是encode就是编码器，那我们输进去的是一句话。那当然我们要把这些这句话呢，变成一些数值化的东西，你不数值化的东西它没办法计算，那我们数字化以后呢，就变成输入一个矩阵，那这个矩阵是怎么回事呢？等一下我会给大家详细的解猜啊，然后进去以后呢？

仇钧潼

09:12

它这里注意有一个东西叫做多头注意力。那这个多头注意力是什么意思？那我也一会给大家解释，其实并不是一个很复杂的东西啊，然后后面的话呢，就是我们比较常见的。那这里有一个残差链接，就是它从下面跳上来。就是这个经过这个多头注意力以后的输出，跟这个原先的输入直接相加，那这个就是我们很熟悉的一个残差网络的结构嘛，还有一个NORM那这个NORM的意思就是正则化。那它这里的意思应该是成正则化，就是说这个输出在这个成这个水平上呢，我们首先做一个规划。呃，这个处理，然后我们再往下面走啊，那这些都是比较常见的一些呃，神经网络里面所采用的手段了。好，然后接下来的话呢，就是一个前馈神经网络，就是一个普通的BP神经网络了，那这个也是一个很简单的结构，没有什么。呃，可以说的，然后再往下的话呢，那就是也是一个残差的一个跳转啊，那这里有两个残差的跳转，然后这样的一个整体呢，我们称之为叫做一个穿层或者一个穿。一个快啊，那这么理解就行了。

仇钧潼

10:30

那大家注意一下呢，左边这里还有一个啊！那就表示说这个结构呢还不是一层，它可以是N层，那NN是等于多少呢？缺省的情况底下N是等于六，但是呢，在大语言模型底下呢，这个N一般是会远远的超过六的。

仇钧潼

10:49

呃，比如说这个在最小规模的能够用的大语言模型里面就能能有点看头的大语言模型里面呢？N一般是等于12，那如果在网上的话，那这个数字呢会更高，比如说等于。呃，这个二四二四八九十六啊，啊，那这个数字还会往上涨。那一般随着这个AN的增长呢，那这个穿长就越来越多，那你这个系统的那个参数呢，也会越来越多，那你可以有更复杂的表现力，因为神经网络能够拟合什么东西，取决于它的这个参数的数量。那参数越多，他就能够学习到越多的知识，能够记录越多的知识，但是呢，你训练他所要消耗的算力也就越大啊，那这个也就这样子了。

仇钧潼

11:35

那所以这里的话呢，目前比较。呃，常见的数字这个N的数字呢，一般都是两位数啊，在两位数这个范围里面。好，那这个部分。那这个一个呢，有一个比较。呃，显著的特点呢？就是你输进去的东西维度是什么？它最后输出来的维度也是一样，比如说你输进去的是？呃，105乘以512的一个矩阵。那105呢代表你输进去这句话有105个单词。然后512呢是那个嵌入空间的维度，那这个也是一个超参数了，就是你预先定义的一个超参数。那你这么105个单词组成的话，变成矩阵化以后呢，它就变成105行了，这105行乘512列啊。那这512呢是一个超三数，也不一定是512很多时候会是768啊。那我假设是512吧，那就是105行乘512列的矩阵。那经过这一堆变化以后，你别看它中间的变化这么复杂，最后出来它依然还是105行乘。512的矩阵这个维度是不变的，就进去是什么，出来还是同样大小的东西，那当然这个内容就全变了，这个内容就完全就不一样了。那你经过多少层，这个穿石方的，你经过多少层以后，它的那个大小还依然是一样。那这个最后出来还依然是105亿512啊，那这个是它的特点。

仇钧潼

13:11

那这个是左边的？那大家也先不用着急，等一下呢，我还会细讲里面的每一个结构啊！好，然后呢。这个左边的银扣的输出呢，会往右边这里来送。那这个右边是怎么回事啊？等一下我再给大家解释啊。好，那我们看看右边这个结构，那右边这个结构呢明显要比。

仇钧潼

14:15

卖给我们。另外一个是没问题的诶好了。

仇钧潼

14:34

我这边显示我能听再输，再输出声音。

王蓓

14:36

现在声音很好。

风铃之音

14:37

啊，这会儿有声音呢！

仇钧潼

14:39

不是不是这个是还没连，要不还是用这个吧，这个方便一点，这个会不会等一下这面已经试过好几次？喂喂，听到吗？

风铃之音

15:01

啊，这会可以，黄老师学员说您是讲到那个右边结构的时候没有声音的。然后嗯。

仇钧潼

15:09

啊好行行好的好的行，我从。

风铃之音

15:11

然后还有就是说他们说看不到你的那个鼠标，所以就不知道您讲到哪了。

仇钧潼

15:17

哦，行，我我会用手指，我会用手指好吧，我会用手指的屏幕啊，那大家就知道了啊，行，我会在屏幕上画一下这个我讲了什么地方啊？

仇钧潼

15:29

好，那现在呢，我们讲右边这个结构啊！那刚才讲了左边这个是吧是吧，我们输进去一句话出来。出来的是就我们把这句话编码成一个105个单词，这个是随意的一个数字的，这个AN等于105了，你可以等于106啊，或者等于150这些都是可以的啊。但是呢，它的这个列的数字一定是一样的，就512或者768。这个是一个固定的超参数，这个是我们嵌入空间的维度啊，那左边这个你输进去什么，这里就输出来什么同样维度的矩阵。好，那这个是输入。以及输出这个编码矩阵的呃，这么一个环节。好，那我们下面呢再看一下右边在干什么事情啊。那右边呢是一个dda的结构，那dda就是解码器了。那这个解码器呢，一样也是输入那句话的矩阵。但是呢，它跟左边这个输入呢，是有一个很显著不一样的地方。左边这个输入呢，你是把那句话编码成矩阵以后？整个塞进去。就一次并行的把整句话把它塞进去，那叭叭叭的一直往上走，就走到这个地方。啊，那这个并没有一个像那个lstm那种。呃，反复循环的过程。

仇钧潼

16:50

但右边这个解码器呢，就得要做这个过程了，它是反复的输入那一句话的矩阵。那比如说啊，这个那句话呢是I have a cat。就我有一只猫。那本来应该是I have AC四个单词，并应该是四行是吧，再乘以512这么一个矩阵。那我现在输入进去是什么呢？

仇钧潼

17:15

那输进去了，大概是这样子啊！本来应该是。A cat. 每个单词对应于这里一行嘛，那首先我输进去呢，会输入第一呃是首先输。啊，这里应该还还有一行。那这一行呢是一个begin标记。一个开始标记啊，那你首先把这个begin标记把它输进去，下面这些都会被屏蔽着。那屏蔽是什么意思呢？就是说你可以胡乱给它写一些什么值，比如说你让它全部为零啊，什么都好，或者你就写上这个呃，一些别的什么东西。然后呢，这个屏蔽的意思呢，就是说它无论你写什么，它在后面的过程里面都是不发挥作用的，那这个我们后面再给大家讲，它是怎么做到屏蔽的啊，好，那你送入一个begin进去。这个对应的这个矩阵啊，在这里升进去，它往上走，那最后这里会出来什么呢？出来一个soft max向量。就说它就是我们上一堂课讲的那个。

仇钧潼

18:21

呃，字典的向量那个词向量啊，就基于字典的就十几万维度的那个向量那种形式。那每一个维度是什么呢？是那个词出现的概率有多高啊？就是说比如说啊，出现I的可能性80%出现有的可能性70%啊，出现别的这个可能性可能是3%等等，那它会出出出。出来一个概率向量，那我们就从这个概率向量里面呢，挑出概率最大的那个单词，比如说，唉，这里出来一个I。好，那我得到第一个单词了，哎，出来了啊！好，那这个是第一步好，然后第二步是做什么呢？那我现在第一个已经begin这个标记我输进去以后我得到一个I。那我在这里呢，在？再输入第一跟第二行。这两个是不屏蔽的，就跟I是我刚才得出来的，对吧。好，后面这些呢，你你写什么都无所谓，它有一个max的方法呢，可以使到你写什么都没有用的，你在这里你你你爱写什么就写什么都行。好，那我再把。包含I这个信息的矩阵往这里往里面送。好的，经过这个。这个网上的这个计算以后呢，它就出来下一个单词啊就。记错了，下一个单词have。啊，那这里的话呢？那这个这个矩阵在下一次失误呢，它又变了啊，那这里就就要把。要把它归纳到这个矩阵里面，那如此这个反复的循环。

仇钧潼

19:58

那最后呢，我们得到一个结束标记就按标记。那我这句话就结束了，那这个工作的过程呢就完成。那这个时候呢，我们就左边输进去，这句话通过右边这个decode的工作呢，我们最后就。呃，得到了一句他翻译的一个结果啊，它这个过程呢就这样子。那大家注意左边是可以并行输入，它的速度是很快的，但右边这个呢就有点像LSTM那样子，要一步一步来循环，因为我们。每一步循环的说我要在这里面来选择呀！你选择的是不是一定是概率最高那个单词呢？其实也未必，因为其实这些概率有时候都差很少很少的，就是差一点点啊，那你你有可能说有有好几个并列的，那咋办呢？有几个数字都一样的。或者这个数字仅仅只是相差很小的一点，那这个时候呢，其实你有很多选择，你可以选啊，也可以选E啊，因为他们的可能性都是0.5%，那你两个随便选，一个都都是有可能的，那你随着这个选择不同啊，后面的这个发展的方向。就有可能不一样了是吧，那所以呢，他这里就只能够。扔一个。Taken进去啊，我们把这个单词可以称之为叫一个token我扔一个token进去，然后呢，我得到一个返回以后呢，我再把它作为新的token扔进去，那这样不断的循环，那我就可以得到一个。

仇钧潼

21:27

这个一句话了啊，那这个整个工作的那个流程啊，大致上是这个样子。那我们也可以看一看右边这个结构啊！那右边这个结构呢？这个部分。那这个就是一个多投资注意力啊，那我们反复的提到这个词，但我们还没讲这个是什么意思，等一下我就给大家解释啊。然后这里的话呢，这边也有一个长叉的链接过来啊，那这边也做了一个常规一化。那大家注意呢，在往下一步的时候呢？它就不是直接往上了，本来是不是应该直接往上，对吧。但是呢，它这里面有有两路，是从左边这个西这里过来的，<笑声>那这个这个是这三为什么都是三叉戟的这种形状啊，大家看看这里有一个三叉戟啊，这里也有一个三叉戟啊。啊，唯独这里不是三叉戟啊，它它它的这个有一些是从左边过来，这个什么意思？等一下再给大家解释啊。然后后面呢，又是多头自主利益又是长。

仇钧潼

22:27

那最后出来呢，就是前馈神经网络，bp神经网络。再加一个长。然后后面出来这里呢是什么呢？一个线性层，那线性层呢，其实你可以理解为是一个全年阶层，就是一个BP神经网络的全年阶层，它是没有激活函数的。那所以我们称之为叫线性层，那最后输出来的就是一个soft mass向量。那这个大概是一个概率向量啊，就是说呃你。你这个。我们应该选择哪个词？分别是什么概率啊，它会列出一个向量给你。那大概这个结构呢，就是这样子了，那下面呢，我们逐个细节，这个这个细节的部件呢，来给大家细讲啊。

仇钧潼

23:08

那我这里讲的东西呢，在网上也能找到一些这种中文的材料。好了，大家可以参考这里的两个链接，那我在今天的这个课程里面呢，讲到了一些内容呢，也来自其中的一篇文章，因为画图自己画的你总不如人家画好的好看，对吧！那我就直接借用了别人博客上的那个图了啊，那大家呢可以。呃，再等一些了，幻灯片里面看到这些图。那我也把这个相关的这个transform的文章在群里面发给大家了。那有一篇文章呢，就叫做transform我忘了编号是多少了，我发在群里面。那就就是transforma点pdf那篇文章呢，就是把其中的某一篇博客我忘了是哪一篇了。我把它整个拷贝下来转变成为一个PDF文件发给大家，这个是中文的，那看起来应该算是比较简单的。

仇钧潼

23:59

那另外呢，还有传是方法的论文啊，这个就刚才说了all you need is attention啊，那个也发给大家了，都在群里面啊，大家可以。看一看啊。好，那这里呢描述了就是刚才所讲的那个结构。那这个是六成？然后呢，这个呢是六层啊，这边是。它这里不是有个N嘛，其实是六层。然后呢，他现在这个也是六层。也是六层啊，它也是很可以。不断的堆叠很多层上去。

仇钧潼

24:35

那这个if的那个解码过程在这里也描述了，那最后呢就以作为结束，那就表示这句话结束了，我们的解码过程就已经完结了啊。好，那下面呢，我们逐个部件来给大家讲啊，那第一个呢，我们首先讲一讲这个输入在这个部分呢是怎么编码的？是怎么变成一个矩阵？我们怎么把一句话变成一个矩阵？那比如说啊，现在我有一句话，我有一只猫啊，就I have a cat啊中文。英文都好。都无所谓了。那反正这个过程都差不多。那如果是中文的话呢，你可以把这句话先分词，那它就变成词了，对吧？那英文它天生就是词啊，那这个不要紧。

仇钧潼

25:30

那个这个ifc呢是原始的字符串，那一般来说呢，我们通常都会按照那个字点向量的方式，首先把它变成向量。那所谓字典向量的意思，之前给大家解释过了，就是一个很长很长的向量。大概有十几万围这么长。有十几万个这样的元素跟那个词典的长度是一样。比如说啊，现在在词典里面呢，第三个单词，这个单词。这个单词呢是？然后然后这个cat的单词cat这个单词的编码呢，就是001000。因为其他单词它都没有出现啊！

仇钧潼

26:16

那这个这个就是字典向量的这个编码方法，我们也称为叫毒热向量，在上一次课堂给大家讲过了。然后呢，这个毒热向量呢？我们可以通过某种的编码方式。就是的算法，我们把它变成一个稍微短一点的向量。那比如说我们这个穿石风宝呢，选择的这个嵌入空间的维度呢是512维。那我们这个应该就把它变成一个512位的向量啊512位。那你选择什么的方法呢？这个都无所谓。大致上差不多啊，那你可以选择我们上次课讲过的。就单词变向量这个软件，那你也可以选择别的东西啊都行。那总而言之，你就先把它变成这个嵌入空间里面的一个向量再说啊。啊，这里说这里说错了，这个512位应该不是这个应该是上面这个，这个是512位啊。这个是500。好，那这个是第一个要做的事情，把每个单词做embedding。那第二个事情呢，我们要做一个位置embedding。啊，那这个讲起来就比较。比较复杂了。是什么意思呢？就说我们要生成一个向量。这个向量呢，同样其实也是。512的维度的？它同样也是512维度的。然后呢，这个向量代表了某种位置的信息。

仇钧潼

27:59

为什么我们需要这个位置向量呢？那原因是我们这个传的编码。其实就是把一个原始的输入矩阵映射到另外一个矩阵，它的维度是一样的。但是呢，它的精准性不一样。就原始的矩阵呢，这这个这个矩阵呢可能。它仅仅只是对这个词做了一个。那这个词里面呢，有一些可能是重复的。那你做embedding的时候呢，它embedding出来那个向量就可能会一模一样。那我举个例子啊，比如说啊，那只猫。这个呃。警惕的猫着腰。盯着莱克。那这里这句话里面有两个猫是吧，那第一个是那只猫，那个猫，那第二个是猫子妖，那个猫。都叫猫，那如果你做的话，那它肯定得出来的东西是一模一样的，那这个毫无疑问的，对吧。但是呢，很明显是猫非比猫。那第一个猫是一个名词，它代表这种动物。那第二个猫呢，代表的是一个动作，你是弓着腰的意思。那所以你应该那是不一样。那这个穿石方宝的好处呢，就是它能够根据这个上下文的情况。这个上下文甚至还可以捕捉到很远，那water它只能够捕捉到临近的一个窗口，一个小窗口里面的这种上下文的依赖关系。而且呢，它还是一个总体上的，就是经过一个统计，对所有总体上的这个上下文来进行估算，那这个就相当的粗糙了，然后穿石方把它可以直接根据这一句话的上下文关系。

仇钧潼

29:43

绝对位置跟相对位置的关系来决定我这个词是什么意思？它通常解决两种问题，那一个呢就是一词多义的问题就说啊，刚才说的那只猫警惕的猫着腰啊，怎么怎么那两个猫，那这两个猫不是同一个词是吧？那我们怎么区分这两个词使到它的？不一样，那这个是传解决的其中一项任务。那第二项任务呢？就是那个。那个代词子？啊，比如说老王。这个拍着张三的肩膀。那对他说啊叭叭叭什么什么之类的。那这个后面这里有一个？那这个他是指谁呢？那你如果不区分的话呢，这个它。就做一个我embedding，那无论什么，它都是指向同一个embedding的向量。对吧，那只要是它就指向一个的向量，那这个这个很明显就失去了这个准确性了，对吧。那因为这个代词呢，在我们的语句里面是经常出现的，如果代词的做的不准，会对我们这个句子的最后的语义。产生极大的影响，就说我们整个翻译或者其他下游任务都会变得不准确啊，会产生很大的影响。那所以呢，我们要解决代词指代的问题，我们要捕捉到前面那个。

仇钧潼

31:06

呃，张三或者老王那我要决定这个，他究竟是指张三还是老王呢？那这个就是transforma通过他的训练得到的经验来最后决定的，那其实transforma要解决的就是这些问题。它通过更精细的这种位置的捕捉，更复杂的网络结构，能够决定出更好的。那我们经过transform编码的这个呢，要比经过这个的这个好到10万8000里去了，然后最后呢，你就可以得到比较精准的结果。那大家现在使用那个TPT的时候呢，都非常。叹服于啊，就为什么一个机器能够做到这么好的效果？他说话真的跟真人是一模一样，就看起来是一模一样，他为什么能够有那么好的效果，而且他理解这个。这个问题的那个意图就我问什么东西，它的理解力也很强，那这种理解力呢，就来源于这个传输方法本身它的这个构造啊。

仇钧潼

32:10

好，那我们扯远了，那我们回到我们的这个正题里面来。那刚才呢，我们讲到为。我们已经对这个词做了embed了，就是一个普通的。那现在呢，我们要对这个位置来做。embedding那我们下面看一下呢，这个事情呢是怎么做的啊？

仇钧潼

32:42

大家先看一看这里啊！

仇钧潼

32:43

那这一段的话有点长，然后呢？下面这个公式有点复杂，那你可以先看一看这个公式啊。我们不是现在输入有一个矩阵吗？那这个矩阵呢是I？每一行呢，就代表一个词的，这个已经做好了，已经做好了。它是四行。然后呢有512个列？那现在下面呢，我们来做位置。

仇钧潼

33:23

那这个位置也是四行512个列？然后呢，我把位置跟这个词相加。形成我们最后的embedding的结果，那我们下面看一下这个位置是怎么定义的。大家注意啊P！就是positionbedding。就位置的意思啊，那这个是一个矩阵，所以它有航标。这个是航标，这个是列标。那position的话呢？行标那这个就是呃，这个词在语句里面的第几个位置，那其实就是在矩阵里面的第几行了是吧？然后这个列标呢？分两种情况二，I的意思就是说它的列是偶数列，它数数的时候是偶数列。那二I加一的话呢？那就说明它是奇数列啊，那是这个意思。那如果是二a的话，它是偶数列的话。那它的那个值应该是等于什么呢？等于一个很奇葩的一个一个一个值，一个设计的值啊。就是扣31万除以D那D这里就是指512了。

仇钧潼

34:33

就是我们那个空间空间的维度啊，二I就是你的是第几个列？那这个公式我就不练了，大家都能看啊，那这个position就是你的行数。然后基数的时候呢是等于这个？那我们就得到一个矩阵，那这个就是位置编码矩阵。<笑声>那大家这个大家听起来可能觉得，哎呀，这个怎么想出来的这个东西？那我也觉得很奇怪，这个是怎么想出来的？

仇钧潼

35:02

好，那这里给大家解释一下这个位置跟马取证的一些事情，那第一个我们为什么需要这个位置跟马取证呢？

仇钧潼

35:11

那刚才说过了，就是一个词，它真正的啊，其实不光跟这个词长什么样子有关系。有很多时候呢，这个词是长成同一个样子，但是你在句子里面的位置不一样。以及它相对于其他东西的位置不一样，它可能就会有不同的embedding不同的意思，不同的结果就像刚才那个猫啊，那我这个例子已经。已经举过了啊，这个猫两个猫这个单词是一样的是吧？但是很明显它们的意义应该是不一样。还有这个比较常见的就是代词指代。那这个。呃哦，我这里可能放在后面的页里面的，就那个它，那你究竟是指哪个它呢？那你要结合上下文才能搞得清楚啊，那我们主要是解决这么这么一些问题。

仇钧潼

36:09

那这里我图解说一下那个？为什么我们需要这个位置？embedding那这个有点超前，因为我现在还没有讲那个自助意力机制。自注意力机制呢是这样的啊，这个是我们的输入矩阵。那这个是那个自注意力的一个变换的矩阵，这个是变换的矩阵。为了我们要做两次变换，第一次变换呢是。是从上面这个矩阵，然后呢把它变成右边这个样子。那第二个就是从下面这个矩阵让它变成右边这个样子。

仇钧潼

36:48

那总而言之，就是说，你如果这个原始的输入矩阵里面有两个单词，它的embedding是完全一模一样的话呢，那在乘上这个矩阵以后得到的结果呢，这两。两行对应的两行它也是一模一样的，那这边也是一样，这两行也是一模一样。然后最后呢，我们要把这个Q。那你这两者乘上以后呢，它就变成这些是一模一样，这两行是两列是一模一样。那一模一样的话，会产生什么问题呢？就变成那个注意力机制里面那个注意力是一样，就你不能区分这些单词。在你最终那个解释那个句子里，那个语义里面所占的权重，你不能区分的，就只要这两个单词是一模一样的，那你将来在这个未来的句子里面，它起到的这个意义的权重也是一模一样的。那这个是明显不合理的，我们这个位置编码呢，其实就是要解决这个问题啊，那等一下呢，我会在讲完自助意力的时候再回来，回到这张图里面来啊，我再解释一下这个事情。好，那我们看一下这个。

仇钧潼

38:06

编码的方法，那这个编码的方法能够体现出什么优越性呢？呃，首先我把它的值写下来。就是。我用不同的颜色代表不同的值，用红色。跟蓝色啊，就是这边代表的就是。因为它是C跟三啊，那它一定是负一到一之间，那用偏红的颜色呢？代表负数值，偏蓝的颜色呢？代表这个正数值，然后呢，用它颜色的深浅，你越深，这个值的绝对值就越大。就越越靠这两端啊，这个红色也是一样。

仇钧潼

38:42

那我们可以观察一下呢，这个值的分布情况，那我们可以发现呢？这个矩阵当然这个右边这看起来好像都差不多是吧，都是深蓝色，但是你仔细看上去呢，它还是有一点点区别的，就这个矩阵的话呢，大概里面每一个元素的值都是不一样的。只要你那句话不会有1万个单词这么长。那这个举证是不可能重复的。它是不可能重复的，那所以呢，每一个矩阵里面的值都有自己的一个独特的一个值出来。那这个值呢就代表了你的位置，那使到比如说啊，我这个位置。跟这个位置。有两个单词，它是一模一样。他这个这个这个单词是一模一样的啊，就是说啊，这个单词。这个单词。一行代表一个单词，跟这个单词是一模一样的。但由于你这个位置的编码，矩阵里面的这个行不一样。那你加上去以后呢，就使到尽管这个单词的是一样的，但是你汇总以后呢，这个值依然是不一样的，我们就能够把它区分出来。那这里有一个我们觉得很奇葩的原因啊，刚才我们解释了第一点啊，就这个。就1万以内编码都不会重复，那第二个我们为什么要用三跟这么奇怪的东西呢？那第一个单就是为了它的值不重复啊，那它的这个上跟口上的值呢，确实是不重复，那这个大家也已经知道了。

仇钧潼

40:17

还有一个原因呢是？它的相对位置值是比较容易推算。比如说。我们。有一个这个单词是位于这个位置，就它在另外一个单词，这个position在偏移L的位置上面。那它的这个这个这个东西的算值呢？按照这个三角函数的和公式，它就等于这个。那其中。这个。都是之前我们已经算过了。然后扣三这两个东西呢？就是等于说。我心的这个位置。它的那个位置值我可以用旧的位置通过线性组合。这两个东西是组合系数。来得到。说我可以通过旧的东西用这种线性组合能够把它推出来。那其实它就是能够有这个好处。啊，它是能推算的，那这个推算单不是人推算的，我们希望是什么事情呢？我们希望的是我这个神经网络在训练的过程里面，他自己就能学会我定的那个编码规则。他自己就能学会，并且知道这个向量在句子里面的什么地方。以及相对于其他的词。他在什么地方？他自己就能学会了，那以刚才那个穿石方法的复杂度呢？那应该认为只要你的数据量足够多啊，那做到这个事情呢，呃，看起来是不是很困难的一件事情。

仇钧潼

42:00

啊，那这个位置编码呢？大概就是这个意思了啊，就它的用途以及它为什么该写成这个样子？那我就给大家解释了一遍啊！好，那下面呢，我们说一下这个in扣的那个过程啊，那刚才这个图已经给大家解释过了，我这里就不重复了。

仇钧潼

42:20

那这个是的结构，左边这个是的结构。那下面呢，我们把里面的部件呢给大家解释一下啊。

仇钧潼

42:46

左边这个流程图大家先留个印象啊！好，下面我们看一下这个所谓qkv的问题。就是所谓注自注意力的问题啊！那我们输进去的是X这个？在这里也是X，这里也是X。我们把它做三个变换。都是呈上一个矩阵。那这个X呢是am，比如说乘以512。

仇钧潼

43:21

N就是你输进去那句话的单词是？那512呢，就是你嵌入空间的维度，那其他也是一样的，所有的X都是一样的啊，这里是N啊，这里也是512。这三个矩阵都是相等，都是一样的矩阵。输入进去的东西啊！

仇钧潼

43:39

好，那假设呢，我们现在有wqwkwv三个矩阵。那这个wq大概是什么样子呢？那大概就是512啊，乘上一个数值。那这个数值呢，应该是嵌是一个深度嵌入空间的一个维度，比如说呃，256支之类的吧。啊，所以它是一个固定大小的，这个WQ是固定大小的一个矩阵。那其他几个矩阵也一样的，都是512乘256。这个512就是嵌入空间的维度，256的话呢，就是自注意力。

仇钧潼

44:18

空间的这个维度。那这个256呢也是一个超参数的，我们自己可以选定。但总而言之，无论怎么说都好呢，它都是一个固定尺寸的矩阵。这个矩阵是固定？然后这个矩阵里面的这些元素。分别是什么呢？他们都是训练出来的。啊，他们全都是训练出来他，你一开始呢，可以给他一些胡乱，给他一些值。然后呢，我们对他进行训练啊，那我们就得到这个矩阵具体的值了啊。

仇钧潼

44:51

好，那下面我们看一看它这里是怎么把QKV弄出来啊？我们输入X。乘上这个512乘256的WQ以后呢，我们就变成Q矩阵，那这个Q矩阵的维度呢，看起来应该是AN乘以256。A N就是你锯子的长度256的就是你自助的空间的维度啊，那这里也是一样，这个是a N我们得到K。那这个也是N乘以256？我们得到V，那这里也是N乘以256。

仇钧潼

45:25

好，那下面呢，我们做一件事情。这件事情是做什么呢？我们做这个计算。Q乘以K的转置我们做这个计算。那刚才这个QKV都是同一个维度的，都是AN乘以256对吧？所以这个K的转置转出来以后呢，应该就是256乘以。那你这么一层的话呢，就得到一个N乘N的矩阵呢？是吧。那这个N乘N的矩阵我们要做一个。这怎么做呢？那你就把它里面的每一行。看成一个向量。

仇钧潼

46:05

对这一行做一个soft soft M就不需要给大家解释了，因为在其他视频里面已经多次讲过了，我给大家已经假设了这个李思懂深度学习的就是说把这个东西概率化，无论它是正数也好，负数也好。那我们得到了这里呢，就是一个概率向量概率向量的意思就是里面每个元素都是大于零的，然后加起来都是等于一。好，那每一行都是这样子。那这个是以后的这个矩阵？那大家注意一下呢！这里的值下面还除以一个根号DK，那这个DK是什么呢？就是512。就嵌入空间的维度。那这个公式呢？大概的意思就是说我们的经验告诉。告诉我们那那这里是取512？开方啊，那这个是最好的。那这个qk转制里面的每一个元素都除上这个以后呢，再去做这个softmaster的这个效果了，最最理想。好，那接下来的话呢，那就是这个结果啊！

仇钧潼

47:05

就是N乘N的矩阵。这个V呢是N乘以五百五百一十二的矩阵，那最后得到一个以，那这个以呢跟这个QKV的维度是一样的。就变成N乘以512的矩阵。那这里这个V最后是怎么得到的呢？呃，这个这个以赛最后是怎么得到的呢？那你可以想象啊，我们这里面的第一行，这里的第一行。就是这样的一行。那大家可以看到了，这四个值加起来都等于一，而且它们都是大于零。那一般我们在以后的理解呢，就是把这些都看成概率。好，那这个这这个向量乘上这个V这个矩阵以后呢，我们就得到这个以色一得到里面的一行。得到这里的一行啊，这里的一行，那其他第二第三第四行都是怎么得到的？那这个就是所谓在里面的自注意力机制。

仇钧潼

48:09

那我们好奇葩的，这个好像听起来很复杂啊！我们把输进去的一句话变成了一个矩阵。然后呢，分别做了三个线性映射，这三个线性映射呢，把它映，把这个矩阵硬生生的映射成QKV三个不同的矩阵。那这个线性映射的这个映射的方法就这个系数。是待定的，是我们训练得到的。那我们得到QKV以后呢，就做刚才一系列魔幻般的变化，就做这个事情。那最后我们就得到一个已知的矩阵。

仇钧潼

48:46

好，那大家听完这里的话呢，很可能想知道QKV到底是个嘛东西呢？到底是个什么东西了啊？下面给大家解释一下。大家看一下这个图啊！QKV呢，他们都有自己的意义。Q大概大概代表的就是query就是询问。K代表的是key？V代表的是value？那我们的意思呢就是。有点像我们第一堂课讲的那样子。第一堂课的时候不是告诉大家啊，我们做一个向量数据库。然后呢，你问的任何一条问题我都跟向量数据库里面的问题求一下夹角于。看看跟哪个问题最相似？或者跟哪些问题最相似，然后我就把他的那个。那个对应的答案组合起来。那我就得到这个？最最终的结果可以回送给你的啊，那我还记得这个事情吧，对吧？

仇钧潼

49:45

其实呢，这个qkv这个东西呢？他本身也是从问答系统里面来的。假如我们把Q这个快。里面的每一行。把它里面的每一行。看成是这个问题这句话的。那这个embedding不光单词可以做，句子也是可以做的，对吧。你对句子做的embedding跟你对单词做的embedding它的维度是一样的。那你对这个问题做了一个？好，那这个就是Q矩阵的东西啊，现在我有N条问题。那我把AN条问题呢，分别做了以后呢，就得到一个AN乘512的矩阵。好，然后这个是什么呢？那就是你的知识库，你的记忆。那就相当于啊，我在向量数据库里面有很多的。问题或者不叫问题吧，叫关键字答案对。就说啊，这里是关键字，这个是？那这边呢，就是value这边就是相应的答案。那我假设这种key value对一共有AM条这么多。那这边是M啊，那这边也是M。那他们的啊，全部都是512。都是512这么大？好，那现在我看一看我这个基于向量数据库的问答系统是怎么回答？

仇钧潼

51:17

这个这个回答这个问题的。那我就把这个。这个问题。跟这个key。求一个夹角于弦对吧？求求相似度。那这个相当于什么呢？不就是Q乘以K的转制吗？大家想一想你，你把这两者求内。那不就是qk？那不就这个意思吗？所以呢，这个QKT里面的每一个元素，你乘出来这个元素。其实相当于什么了？比如说第一行第J列的这个元素就相当于第二个问题，跟第这个key的相关度有多高。呃，我们还不能叫概率，它还没有。它你只能够叫它相关度，那这个这个这个值越大的话呢，这个相关度就越高。啊，那代表的是这个意思啊？然后呢，后面这个QKT转制完以后除以跟它512在so。那就相当于把刚才这个相关度矩阵把它概率化了。那概率化以后再乘V是什么意思呢？那大家想一想，这个概率化的那个矩阵，这个概率矩阵就是N乘以N的矩阵呢，是吧？代表的是这边是N条问题，这边是M个key。那这个问题跟key之间的关联程度有多高？那现在我把它概率化了。

仇钧潼

52:51

好，那我怎么找每一个问题的答案呢？啊，比如说第一条问题，那它的答案应该是什么呢？那我不就是把这一个行乘上右边这个矩阵？乘上右边这个矩阵。就是说这个右边这个矩阵是value是每一个key对应的答案。那我把。每一行。按照左边这个概率值。做一个加权，把它们全部累加起来。那就相当于是这个行。

仇钧潼

53:23

乘上右边这个矩阵，那我们就得到这个结果里面的第一行，那第一行是什么呢？就是回答你这个问题的答案的。嵌入在语义空间里面那个向量。它是综合完所有的value以后，根据这个全职综合完所有的value以后得出来的一个答案。这个答案不是具体的文字，它是一个就是说你用这个东西最适合来回答这个问题的。那这个就是你答案？

仇钧潼

54:00

那最后我怎么把这个答案的embedding还原出？原先的这个答案出来就还原出文字形式的答案呢，那这个就是解码的过程了，那这个是另外一回事了，那反正我知道，哎呀，回答你这个东西用这个向量最合适了。那就ok了。

仇钧潼

54:17

那其实这个道理的是很容易明白的啊，那我们现在就明白了，刚才那一堆东西。啊，其实它本质上是什么意思呢？就是Q就是问题，然后呢，最后我们得到的那个这个这个东西啊。这一块东西就是回答这些，根据你的这个KV对所组成的数据库。回答这一条问题对应的答案所在的那个语义空间的嵌入向量。其实是这个关系。嗯。哪个您您您说的是哪个？呃，在我这个解释里面呢，是M成员在我，因为因为我这个解释里面呢是呃跟还没有谈到自注意力，我只是说这个QKV的来历是什么？就它的来历呢，其实也是问答系统啊啊！对512就没有了，对512就没有了，对512就没有了，是的啊！对，但是你在乘上V以后，它又有了，这512又跑出来了。就乘上V以后。因为V啊，这个V一本身呢？是那个。A M乘以512？512所以你你再把它乘上它以后呢，这512又跑出来了啊，就变成N乘512了啊，那这个是V的。好，那这个是QKV的来历啊，那这里。这个QKV呢？还不是那个？不是那个自助领域里面的QKV就是原始的。那这个QKV的意思，它其实呢还是很朴实的，它的背景是很朴实的，它也是用来做这个问答系统用的。好，那现在呢这个原始这个这个自助一里面的QKV是什么意思呢？

仇钧潼

56:19

唉，其实这个说说说白了就是呃，他的思想是来源于早期的这个问答系统里面的QKV。但是呢，它跟这个问答系统的QKV的也有很多不一样的地方。那第一个它是词嵌入，而不是句子嵌入。刚才我们是把问题嵌入了，把这个key跟value这些都是句子都都嵌入了。但我们在这个呃。在这个transforma里面呢，它是词的，那不是句子的啊。然后我们最终想达到的目的呢，就是使到。这个我们一个句子里面的词落到多个不同的空间或者组合以后，落到不多个不同的语义空间里面以后呢，再算一算它们之间的关联关系。从而呢，可以更精确的使到我们能够理解这个句子里面的每一个词。就是解决我们刚才本质上解决我们刚才的所谓10多亿的问题啊，两个猫。我们怎么区分这两个猫或者呢那个代词？这个纸袋的问题啊，比如说这个它。那这个它里任何一个句子，这个它都是一样的它。那你，你做的时候做的时候，这个一定是一样的，对吧？那现在呢，我们就能够根据上下文以及这个它出现在句子里面的哪个位置。那我就能够使到它的点不一样，经过transformer以后的不一样，那使到它的意思会更加精确。啊，那这个目的呢，我们就达到了啊！好，那这个是？这个qkv啊，这这个自助一的意思。

仇钧潼

58:03

那下面呢，再给大家解释一下呢，什么叫多？自注意力波头注意力！那多投注意力的意思就是。我们其实这个QKV呢有很多套。那一套QKV就称为一个头，一套QKV就称为一个头，那我们可以有八个头，那八个头就是我们有八套QKV。那如果12个头的话呢，那就是12套。那这个参数就越来越多了，就随着你这个套数越来越多。那你的这个参数的规模也就啪嗒的就上去了啊！

仇钧潼

58:37

好，那我们看看这个多头呢是怎么起作用的？那首先我们的输入是X。那我们刚才不是经过这个qkv的变换以后得到一个矩阵嘛，对吧，得到一个以矩阵，那这个以矩阵呢？是N乘以。呃，比如说这个256这样子吧，我256这个是？随便取的那主要是想区别于那个512啊？主要是想区别于那个512。那现在有多个头啊，举个例子，我们有八个头。那这里就是有QEKEVE啊，这个是Q二K二V二啊，一直到这个Q八。K八V八。有八套不同的这个QKV矩阵。那他们就可以分别得出不同的以色啦，这这个叫以色一啊，这个叫以色21，直到以18。有八个不同的以。那我们把这八个不同的。这么做并在一块？定在一块，那这里是多少呢？它的函数函数是N啊，函数是N，列数呢大概是256乘八。等于多少了就是2048吧！

仇钧潼

59:56

20是吧，这个是一个固定的数字，它的列数是固定的数字。那我们把这八个头的输出结果就并在一块成了一个很大的矩阵。然后呢，我再把它乘上另外一个矩阵，那这个矩阵的维度大概是多少呢？第一个2048。就跟这个数字是一样的。这两个数字是一样的。这两个数字是一样的。就是固定的数字。然后呢，它的维度呢也也是512维度，就就是它的列数也是512。那总而言之，这个矩阵呢，就是一个固定大小的矩阵。因为你要学习的参数，所组成的这个矩阵必须是固定维度的，你不固定维度呢？我怎么学怎么训练呢？你连训练公式你都写不好，你怎么训练的对吧？那所以这个维度是固定。那这个矩阵呢，现在是2048乘512，因此呢，是一个固定大小的矩阵。那我们把里面的参数当成是待定系数，那我们就通过训练来学它。啊，那就ok了。

仇钧潼

01:01:03

那最后呢，我们就得到一个最终的输出。那这个是多头自助意力机制的结果啊！那大家回到刚才那张画我们讲到哪呢？那我们刚才在这个地方啊，这个输入是怎么编码的？给大家讲了这个embedding啊，然后这里位置编码是，这个也给大家讲了，两者相加呢就得到真正的X这个输入。然后呢，在这个三叉戟这个地方呢，其实就是qkv了，对吧，这里就是qkv。然后这个所谓多头注意力机制的话呢，那其实就是QEKEV一直到QQ八K八V八啊，这个组合，那现在接下来就是normal。

仇钧潼

01:02:00

那我们继续来讲一讲这个。这个很简单。那无非就是你多投资注意力机制的输出。那就再加上一个X，那这个就是唉，对一个残差的连接。然后呢，我们对它来做一个常规。做一个常规异化，那这些都是深度学习里面常用的技巧，我这里也不多加解释了，那常规异化的意思就是对这个常的输出，呃，做一个这个归一化处理。

仇钧潼

01:02:35

好，然后下面这里也是一样的啊，就是说对于这个呃这个神经网络这个BP神经网络的输出，我们也做一个残差处理。那这一步就做完了？那我们回到刚才那个图？那这一步呢，也被我们做完？就说我们刚才讲了很长时间在这里讲QKV啊，这里讲多头。然后这里呢，这里的话呢是长插连接。那一下都被我们讲完。

仇钧潼

01:03:09

然后再过去下面都很简单，那再过去那这里又是一个bp神经网络。然后这里又来一个BP呃，又来一个长。这里来一个铲车啊！好，那这个输出。就搞完了。那输出呢，依然是N乘以510。那这个输出最后就出来了，好累啊，讲了这个听了估计也很累，那如果没跟上的话呢，回去可以听一听那个。啊。那肯定相关。呃，都都有关系，就是你选择的方式如果是比较复杂。嗯嗯。主要还是跟这个AN有关系吧，就你句子越长的话，那这个呃肯定它的计算量越大，但实实际上。真正有关系的是另外一个N就是这个N这个N。旁边这个这里不是有个？就说你有多少层？这个关系很大。因为你每经过一层计算呢，它都是蛮复杂的啊！哦，字母这个好像一般没有人这么用的，都是用单词来做里面，因为字母本身它没有没有含义，它没有含义。对，就汉字不一样，汉字每一个字是有含义的。但这个字母呢是没有含义的。是吧。嗯，对，没错。嗯嗯嗯嗯，对。对，但一般来说，我们最后都很少会用问号来直接计算的，一般来说都是用语义空间来计算，然后最后通过soft M再还原出问号，因为我们要通过最后的那个soft M来选择单词嘛，那你这个时候不不问号，也只能问号了。嗯。嗯。有有很多。

仇钧潼

01:05:23

有很多人啊，这个在家里面都都能玩，都可以玩，就是有很多人也在研究不同的这个embedding的方法，这个本身也是可以做的，那这个很简单的，就是你如果训练好一套了啊，就这套系统。整个把它训练好了，那我就拿它的部分，我输入一个东西，你就不是输出一个东西嘛，对吧。那输出这个东西呢，我也不输过来，不，不用过来这边了，我就直接就用这个输出的东西作为他其实也也是一个很。嗯啊。动态隐白的那可能。

仇钧潼

01:06:10

呃，静态embedding呢就是说我我我就是给你一个像我这样的软件。你你塞给一个单词，我就给你一个embedding嘛，对吧，那动态的话呢，很可能是跟你的这个训练语料是有关系的。就是我我这个呢是直接在训练过程里面得的。啊，就是说比如说啊，我这个transform是用来做翻译的，呃，我现在用它，你可以把左边看成是一个也行啊，对吧？左边其实也是一个，你不是塞入进去的东西，我也给出一个东西嘛，那这个的方法其实就是。在你训练过程里面动态得到。啊，它就不是一个静态的方法。呃，他在对。他在一开始输入的时候就刚开始输入的那个矩阵的时候呢，这个可以是用静态的。啊，当然也可以用其他的方法。啊，那那是要的，那是要的，对，那也有把这个它的这个输入这个因为因为所谓的这个，那不就是你的向量乘上一个矩阵嘛，对吧？那就是那个矩阵的系数是带训练的，是这个意思，对吧？那其实你也可以把这个基础部分的这个就是这个这个最开始这个部分的也把它是训练训练出来的。就是你你你。给它增加更加多的参数啊，多了一个矩阵，这个矩阵也是需要训练。就这个意思。

仇钧潼

01:07:43

好一口气讲了很多啊，这个我们讲完了银扣的那部分，大家看看有没有什么问题啊？可以开放几个问题来提问，因为今天内容比较多，可能回答不了太多的问题，可以明天再回答多一些啊！

王蓓

01:07:59

好的，同学们可以举手提问啦！

仇钧潼

01:08:01

嗯嗯。哦，那个是大语言模型的事情。就完整的transform了，它是包括了部分，然后后面我们讲了大语言模型呢，这个but就用了的部分，然后GPT呢就用了decode的这个部分。啊，他就把它拆开了。大家看有没有什么问题啊？

仇钧潼

01:08:35

对，没错，就我们需要的其实是神经网。

王蓓

01:08:36

王八就可以解除静音发言。

契匠.Daniel

01:08:41

王老师能听到吗？

仇钧潼

01:08:43

诶，你好听到听。

契匠.Daniel

01:08:44

哎，刚才我这信号不好啊，我这网络断了一会儿，就是你那个讲那个Q的时候那个在自注意力下的，然后我这个网络断了，没听到能再稍微再讲一下吗啊。

仇钧潼

01:08:55

呃，我看看刚才那个图。

契匠.Daniel

01:08:56

自注意力机制下的对，呃有一篇，然后刚好我这网络段没看到你的ppt里面也没有。呃，在后边后边？

仇钧潼

01:09:05

而这也是一个后？

契匠.Daniel

01:09:10

这应该是新加的注意力的啊！

仇钧潼

01:09:12

啊，我给大家解释了这个qkv的原意，其实就是做问答系统了，对吧？

仇钧潼

01:09:17

那这个自助义的QKV的话呢，实际上是借用了它这个技巧，但是它实现的目的是不一样的。很明显我们这里并不是直接做问答嘛。因为传值方法的原意是做翻译嘛，对吧。那这个就跟问答没有什么关系，那我们现在自助的QKV呢？就相当于用三个不同的映射，把原始的编码映射到三个不同的空间啊，就说你这个是X。然后呢，我们映射到三个不同的空间，分别是这个QKV映射为啊，这里是乘上wq。这里是乘上，这里是乘上。然后再通过这个qkv之间来进行一些计算。来这个得到，使到它的这个嵌入更加精准，这么一个目的。

仇钧潼

01:10:04

那这个qkv自己进行计算，它主要的目的是什么呢？那第一个呢，是要通过这个位置或者这个。相对位置来判断一个单词，它真正的这个意义是什么？那这里主要要排除的是两种东西，那第一个呢是？呃，相同的词汇，但它具有不同的意思。比如说啊，那只猫猫着腰怎么怎么，那这里有两个单词都是猫是吧？但是一个猫是名词，一个猫是动词，它们明显不应该到同一样东西。然后呢还有一个是代词指代，比如说呃老王跟张三怎么怎么啊，然后他怎么怎么，那这个他到底是什么呢？那如果你光是做的话，这个代词的效果一般都是非常差的。就是他究竟是指老黄老王还是张三还是指什么别人呢？那这个这个这个这个代词的这个嵌入效果一般不，不是很好，那实际上这个自注意力呢，就是在它的这种多重映射。跟训练的过程里面呢，把这些embedding把它理清。就是说把它跟这个相对未知，绝对未知之间的关系，怎么影响这个？词的把这个规律把它给理清楚。

契匠.Daniel

01:11:20

啊，好的好，谢谢谢谢！

仇钧潼

01:11:21

啊，谢谢。对，没错。啊，对，因为你连续几个矩阵相乘的话，那其实就等于一个矩阵相乘。啊啊。啊，这么这么理解，它没有激活函数，这个地方是没有激活函数的，呃这么理解也也可以，我觉得这么理解也可以。是吗。呃，可能可以从多个不同的角度来解释它，就是我，我现在这个角度呢，是从问答系统这个角度来解释，那也可能可以从别的角度来解释，其实我一直因为我都反复的在讲这些课程啊，会给别人讲解这种东西，给客户讲解啊，这些东西。那所以我都一直试图用比较直白的语言给大家解释这个QKV这个自助意力机制到底是什么？那我也寻找过很多书本的解释，跟这个网络视频的解释，比如说我会看这个邱锡鹏的书，因为这个是在这个国内用的比较多的一本。中文的教材那也在这个油管啊，这个B站上面呢，看各种关于自注意力的解释。唉，对这个很遗憾的，就是我就没看到一个解释，能够说清楚这个事情，都说的这个很很含糊。这个不是那么清楚，那我觉得自己我自己的解释可能也是。挺含糊的，就是也不是那么清楚，因为它里面有一些机制呢，说实在也说的不大明白，但是我能够把它的原意告诉你，就是它的。Value是用到什么地方？啊嗯。嗯。

仇钧潼

01:13:47

啊，不要碰它，不要碰它啊，对了。它的你可以这么解解释为这么意思，就是说，Q可能是一个单词或者一些单词的组合。这个key呢，可能是一个单词或者一些单词的组合，那我们根据它们的之间的相关性。那最后呢，我我来最后给这个Q啊，这个相关的单词呢，要附上一个什么含义呢？那就是由这个value的组合来决定它的最后的含义。那我们我们要捕捉的就是他们这些规律是什么，到底是什么？

仇钧潼

01:14:26

啊，大致上是这个意思。

王波

01:14:26

我我要问问题。

仇钧潼

01:14:29

听到。

仇钧潼

01:14:32

有诶，听到的听到。

王波

01:14:33

啊，不好意思，刚刚才有些家人然后开开门。

仇钧潼

01:14:38

啊，没事没事啊！

王波

01:14:39

啊，我我有个问题就是刚才有有那个有有有同学问就是说拿这个transform做那个词词向量词嵌入。

王波

01:14:49

呃，如果这样做的话，那个它最终那个损失函数应该怎么写？

仇钧潼

01:14:50

它的损失函数它降了就。首先我们会有一个。这个任务，比如说我这个船的任务是翻译，对吧？那你翻译的话呢，它应该有一对，有一个正确的话，你那边应该也会输出一句话，那它会算出这两句话之间的差异。

王波

01:15:09

嗯。

仇钧潼

01:15:17

那这个差异就是一个可能用一些这个交叉商啊之类的，来来定义它的。<笑声>因为它本质上是一个分类嘛，那所以比如说你可以用交叉商来定义它。

王波

01:15:30

可能根据不同的任务来做不同的词性函数是吧？

仇钧潼

01:15:32

对，就是你不同的任务呢，他可能会你选择了这个。呃，这个损失函数呢，可能会有有点不一样，因为这个本身也可以用来做分类的，等一下我们可以看到有一些做分类的例子，嗯。

王波

01:15:47

啊，好不好，不好意思啊，我我那个主持人还没跟我说呢，我我有人开门好好，谢谢嗯嗯。

仇钧潼

01:15:48

哈哈，没事。那我们先休息几分钟时间啊，谢谢嗯！

林展鹏

01:15:58

哎，黄老师。

仇钧潼

01:15:59

哎，好的好，继续问吧，那就好。

林展鹏

01:16:00

呃，花花老师他有一个问题，呃，我想问一下就是做那个soft的时候啊，呃。

林展鹏

01:16:08

下面它是有一个DK那个？是呃，把那个超参数给开方了，还有开根号嘛，然后这这一个它的那个目的是什么？为什么要要开一个根号？

仇钧潼

01:16:15

对对。我我觉得隐约，因为我没有细究这个事情，这个东西细节太多了，实在没有那么多精力去研究每一个细节，就这里。QKT呢，应该是求累积的意思，对吧，是求累积。那这个是没有疑问的，就是Q的一个行跟K的一个行求数学上的累积，但是我们真正的这个相似度相关度呢，一般来说是求这个夹角于弦。

仇钧潼

01:16:44

求角角余弦那所以累积跟夹角余弦还是不完全一样的，你还要除上一个它那个两个向量的长度。我有点怀疑这个DK就这个意思。但但这里又有一个。地方呢，这个Q跟K里面每一行。的那个长度是不是就是这个呢？那我也觉得这个是一个令人怀疑的事情。啊，那大概就是说可能会使到它更接近于夹角鱼钱嘛，就除上这个东西以后。

仇钧潼

01:17:17

这个是我的理解嗯！

林展鹏

01:17:19

好的，我我回头再慢慢理解一下吧！

仇钧潼

01:17:21

啊，它会对这个soft的这个结果有影响的，就是你除不除它对这个soft的差距。

仇钧潼

01:17:28

就是算出来这个概率的差距会有明很明显的影响，你不除它的话呢？它这个差距会变成很大的。那最后的结果呢？会不一样？

林展鹏

01:17:39

反正就是这个超超餐是什么就选什么来负责就行了。

仇钧潼

01:17:39

对，就有对。呃，没，我没听清楚，不好意思。

林展鹏

01:17:47

呃，就是这个就选回呃这个512的这个超餐来运算就可以了。

仇钧潼

01:17:50

嗯。啊是没错。

林展鹏

01:17:55

就经验经验告诉我们就是用这一个。

仇钧潼

01:17:57

对，但是在很多大语言模型里面呢，不是512，它可能是768。啊，这个等一下讲的。Word的gp对，就是emba的维度没错嗯！

林展鹏

01:18:08

哦，好好的呃，谢谢！

仇钧潼

01:18:09

好行，谢谢您，那我们先休息一下啊，过几分钟我们继续。

王蓓

01:24:54

嗯，黄老师休息结束了，可以开始上课啦！

仇钧潼

01:24:54

回家听了吗？

王蓓

01:24:59

能听到声音很清晰。

仇钧潼

01:25:01

嗯，好的行，那我们继续吧！那刚才呢，给大家解释的这个的这个部分啊，那下面呢我们。啊，下面呢，我们再看一看这个decode的这个部分啊。那抵扣部分呢？刚才说了跟银扣的部分呢是有点不一样的。因扣的话呢，是你整个句子扔进去。就变成一个矩阵扔进去，那它就能够一层一层这么变换，最后得到我们所需要的结果。但的话呢，你必须要一个词，一个词。然后每一个词扔进去以后呢，它得到下一个词。那最后呢，直到你输出N这个标记为止，那我们这句话的输出就结束了，就完整的这个输出了，呃，完成了这个输出的过程啊，那这个跟那个LSTM呢是很相似。呃，但是呢，这个由于这个在encode这个部分呢，它是整体性的思路，那所以。两相比较的话呢，这个创始方法的速度呢，还是比那个LSTM要快的。好，那下面呢，我们看一看这个decode的这个部分啊。

仇钧潼

01:26:20

那我们要做的事情呢就是。

仇钧潼

01:26:24

一个单词，一个单词的输入，那输入的时候呢，我们要做一个mask。就把其他的一些单词把它屏蔽掉。要只是给他我们现在。呃，当前输入到了那个单词或者那一些单词啊，这个是输入第一个to。然后呢，这个是得到了第一个token的返回，比如说我第一个token是begin。得到第一个token的访问I以后呢，我在这里再输入begin跟I这两个token，后面那些就mask。那这里大家可能有点奇怪了，这个mark这个这个屏蔽怎么去做呢？这个事情怎么做呢？啊，那它有它的一些技巧了，等一下就告诉大家啊。

仇钧潼

01:27:04

好，那我们先来看一看这个decode的这个整个结构。那我们输进去也是一个。跟位置编码就词向量词义编码跟这个位置编码加在一块的一个矩阵的输入啊，在这里。加在一起。然后呢，我们经过一个所谓的。Mask的多头注意力机制这个什么意思？等一下我给大家解释啊！这个这个mask到底是什么意思？然后后面呢也是长插连接跟归一化。然后我们把同样的事情在后面又再多做多一次，不一样的地方呢，就是这两个是从C这边过来的，就从左边这边过来的。然后最后就出来了就是。

仇钧潼

01:27:51

这个线性层最后出来就是softmas最后我们就得到我们的单词啊，那这个整个流程呢，大概是这个样子。好，那下面呢，我们看一看怎么样才可以做到。那这里。不难的，这里不难，那主要是我们要有一个max矩阵。那这个矩阵呢，大概是，比如说你这里的输入矩阵是N。乘以512。那这个max呢，应该就是N乘N。它这个样子呢，一定是这样的，绿色的地方为一啊，黄色的地方为零。它主要是用来遮挡做那个？呃，然后呢。比如说当我们算。弟弟一个词的时候。那他只跟。这个第零个词有关？第零个输入有关，那算第一个吃的时候呢，跟第零个第一个输入有关。算第二个的时候呢，跟第零第一第二个输入有关，那这个不就是我们想达到的效果吗？对吧，这个其实就是我们想达到的效果。啊，那这个等怎么作用啊，等一下大家就就能看到了啊！

仇钧潼

01:29:05

那我们看第一个，这个所谓的多头注意力。那这个过程跟之前的是一样的。呃，也是有这个qkv这些操作。那我们把Q跟K的转置相乘以后呢，就得到一个N乘N的矩阵。好，那这个时候呢，这个max呢就开始作用了。就两者呢，要按位相乘。那这些绿色的地方是一嘛，其他是零啊，对吧，其他地方是零。这里是1111。这些是零。那你乘上去以后呢，那黑掉那些地方就等于是没有值的。

仇钧潼

01:29:46

我们刚才是直接对这个Q。除上一个根号VK以后。呃，DK以后再做soft mass嘛，对吧。现在的区别呢？在dota里面的区别呢，就是我要把它乘上一个max矩阵，得到这么右边的一个矩阵以后，我再对这个来做soft mass。再对他来做这个soft。那区别呢，就是在在这样子，就是这这个softmas呢就是跟前面。这边这边这堆东西都没有关系了啊，那然后第二行这里的话呢，跟这堆东西也没有关系了。那这做的结果呢就不一样啊，那其实它就是通过这个地方来实现这个max的效果。那这个是唯一值得讲的地方，那其他都跟那个encoda没有什么很大的区别啊。那这里。这个所有的步骤呢，都是一样。跟那个是一样的，那除了这个max以外。然后呢，这个在前面这个架构这个地方。我们也看到。从这里啊。那这一步在这个地方啊，这个地方。这个Q。本来这个Q应该是。X乘上WQ嘛，对吧，但这里的话呢，这里的Q呢就不是不是X了，它被改成是CC，就是左边这里的这个encode的输出。然后呢，这个K也是一样，是等于C乘以WK。然后呢，这个右边这个V。那它还是下面上来的这个X乘上这个？啊，这个W。那这个是不一样的地方啊？那其他的话那就遗憾了，这个就不不解释了。好，那这里整个过程呢，就是这个样子啊！

仇钧潼

01:31:42

然后最后呢，就根据这个softmas呢，就预测一个又一个的单词。好，那关于这个传方法呢就讲到这里啊，那这个的话呢，这个结构看起来更复杂，但是你有了理解的基础的话呢，这个呢也没有什么难理解了。对，等一下我会讲这个事情。

仇钧潼

01:32:07

好了，那理解了transformer以后呢，这个关于大语言模型你就理解了一大半。那其实最难理解的就是那其他的都。很容易理解啊，都不不难理解。那我们下面呢，就讲一下这个大语言模型，我们重点讲的是but。跟这个gpt系列啊，因为现现在也是用这些比较多一些啊。那我们首先讲一讲这个。那but呢？它其实基于这个transforma里面左边这一块做成了，这个，我们拿它来做but。然后右边这一块呢？我们就用它来做ppt。那所以说这个跟GPT是兄弟啊，尽管它来自于不同的公司。这个but呢是google的，然后GP t呢是这个openai的，然后所做的事情呢也有点不一样呢，主要是做这个embedding这类的事情。然后GP t呢主要是做这个大语言模型。

仇钧潼

01:33:05

好，那我们先讲but，因为它稍微简单一点啊！那这个butter的原始论文呢是这一篇。那这篇论文呢已经发在群里面给大家了。那这个具体的编号不是很记得大家可以看那个论文的标题。啊，你就能理解啊！那大家如果回去以后想。啊，认真。学习一下这个butter的话呢，你可以看那篇文章。

仇钧潼

01:33:33

呃，这个预训练模型呢？其实。在。不是在包里面才第一次用到，在很久以前呢，就已经被用到了。那举个例子觉得我记得很多年前可能。可能七八年前。呃这哦，没有七八年前应该是五年前吧？那我们做过一个辨认中草药的一个软件，就大概的意思就是说，你我拿着一个。呃，摄像头我对着我拿一个手机啊，这个手机里面装了我们辨认中草药的这个。应用那我拿着这个手机拍摄你这个中草药，那它就直接能够在我们的app里面呢，识别出啊，这个中草药叫什么名字啊？它有什么特性等等，我们做了这样一个东西。那我们当时是怎么做的呢？我们是拿了这个vgg来作为我们的基座模型。那大家都知道这个vgg呢，它主要是用来做图像分类的。那无非就是啊，你输入这个图片。你输入这个图片啊，然后中间呢经过这个很多这个卷积层的变化，最后呢就来到一个全年阶层。那这个群呢，这些成最后就会出一个。然后呢，我们最后就可以决定这个分类了，就通过这个softmax的一个最大，那我们就可以决定它的分类。那这个V GG做的这个是这个事情。那这个vgg它是参加那个比赛的？所以它是一个已经训练好的模型，它用image类的数据集已经把它训练好了。那所以呢，我们可以把它称之为叫预训练模型的，这个也可以，因为它确实是已经训练好了。

仇钧潼

01:35:12

那我们现在要想辨认这个中草药有一个主要的问题呢，就是。呃，毫无疑问呢，我们肯定要用某种卷积神经网络来做这个事情。但是这个中草药能有多少呢？这个这个这个这个这个图片的量能有多少呢？我他大概掏了1500多块钱，在一家医院里面呢，把他这个药房里面的每一种药。都买了一遍，大概拿拿回来可能应该有100多种吧，这个药典里面有300多种，但是这个药房不大，可能全部都齐全，有100多种都给我买回来了。然后呢，我就拿出我的单反来，让大家拿着他们的手机啊，拼命地拍这个，拍了很多很多图片，但是这么努力的情况底下呢，也只是得到了大概有两三两三千张图片。然后我们用了各种的这个变换。这个包括旋转啊，这个调亮度啊，调对比度啊，加噪点啊等等，那大概可能衍生出这个几10万张图片，但几10万张图片呢，相对于这种大型的卷积神经网络来说，也是杯水车薪。

仇钧潼

01:36:16

那一般来说，你训练这个大型的卷积神经网络里通常都是要100万以上级别，甚至1000万级别的图片才够，那特别是这种。呃，这种分类任务比较复杂，比如说我达到上百类，而不是两三类的时候啊，那这个需要的参数量就更大了。那所以从本质上说呢，我是没有足够的这个数据去训练。呃，来来训练这个。这个这个模型的啊，但是呢，我们现在有了V GG以后呢，有一个方法是这样子，我先把这个训练好的，用image来数据训练好的这个V GG。拿出来啊，然后呢你？这里面这些参数都不是已经算好了吗？它本来是做那个1000分类的任务的。那我就不做这个，我把最后这个soft master把它去掉。那它就剩下这里有一层。是最后一个全年阶层，然后呢，我把这个softmas层呢改成啊！这个预测200分类就200种不同的这个中药，我把它最后一层的这个结构把它改掉。然后呢，我用那个中药的图片。输入进去对他重新进行训练。然后我最后再得到一个中药的一个分类器。

仇钧潼

01:37:32

啊，我用的是这个办法，那这个重新训练呢，有两种办法，第一种整个网络重新训练，所有的参数重新训练。那这个是一种方法，那第二种方法呢，就是我固定。里面的这个基座模型的参数我把它冻结了，不变。我只是把我加进去的那个200分内内层进行一些改变。或者说我冻结大部分成的参数。只剩下最后几层了，让我去训练来改变，那由于我的训练量呢，只有这么几层或者一层。所以我的那个中药图片尽管只有几10万张，但是这个数据量还是足够的，那我们就把这个模型训练好了。

仇钧潼

01:38:16

那最后呢，这个app的这个效果呢，还是相当不错的，那大概有个。89成准吧，因为这个中药的话呢，这个。随着你的光照条件，还有这个干燥条件不一样，它的外观呈现出来也有很多变化。那所以最后我们的这个发现这个辨认结果呢，已经不是跟我们的技术有关，那主要是跟你采集数据的这种多样性跟丰富程度有关。啊，那我们这个试验呢就做到啊，这么一个程度啊，后来就没有继续做下去了啊。

仇钧潼

01:38:48

好，那这里讲的是这个所谓的预训练任呃预训练模型。那同样的，我们对于这个自然语言处理呢，也可以做这样的一些事情，比如说我把这个transformer用一些预训练任务来训练它。然后呢，我就把训练好的模型呢，作为一个预训练模型，可以去做更多的事情。

仇钧潼

01:39:16

大家可以看一看这个图啊！这里呢是用传值方法来做一项自然语言处理的预训练任务。我们不是有很多这种文本的语料嘛，对吧，有很多很多的文章，有很多小说啊，这个这个网上的这个网页都是我们的语料。那这些语料呢？你很难想象我们会给他打什么标记，做什么情感分析之类的，那这个太花时间成本太高。但是呢，有一种天然形成的标记，就是他们的这个顺序，这个是一个天然的标记。所以呢，我很自然的就想出一些预训练任务。我希望这些预训练任务呢，能够得出一些不错的结果，可以提供给我们做这个下游的任务。那我们称这些预训练任务呢，叫上游任务，然后我们真正要做的叫下游任务。那比如说这个比较典型的下游任务，比如说我们要做情感分析啊，我给一句话，你分析一下这个是好评还是差评？呃，或者说我要做自然语言处理里面的一些任务，我要做命名实体识别，我给你一句话，你把里面的实体那些重要的名词全部帮我给找出来啊，那这个也是一个下游任务。但是呢，这个需要我有一个。用上游任务已经训练好了，模型才能做啊！

仇钧潼

01:40:44

那下面呢，我们看看这个but是怎么设计它的上游任务的？他的上游任务呢？主要是有。两个。主要是有两个。那第一个。猜谜游戏。就说我给一篇文章给你。我随便在里面挖出15%的单词，把它遮盖住，就是说我把那些单词把它扣掉，把它变成一个一个to，一个标号就是。就是说这个单是被我屏蔽了。相当于一个特殊的标记。然后呢，你就猜我这些标号是什么？我我我遮住这个地方的单词是什么？那这个如果画成一张这个示意图的话，大概这样子啊，这个是一个穿式方法，这个整体。这个这个整体是一个传输方法啊，这个整体。

仇钧潼

01:41:41

好，那我要做什么呢？那我在左边这里呢，就不断的输入这个文章。里面的所有的话就是一句一句话，这么输进去，里面有一些单词是被我遮挡住的。然后呢，在右边这里？我就输出那个单词，那个被遮盖的单词。那我通过比较你输出的单词跟那个光初始的区别，我就得到损失函数。那这个损失函数产生的这个误差信号呢？就可以用来训练整个传。那这个是第一项预训练任务啊？那还有一项预训练任务呢，是那个？这个。给出上一句话。你猜下一句话。那这个你可以把它看成是？问答的一个推广，那问答呢，不就是我提出一个问题，你回答我吗？说你猜我的回答吗？那我们把这个再拓展一下，那不一定是问答，那只要我给出前一句话，那你给我下一句话，比如说我对联，那我给出上联，你才下联，这个一起背唐诗，我给出上一句唐诗，你才下一句唐诗。是什么？然后呢，这个我给给出这个上一句，这个别人说的话，那你又猜我们后面会怎么说啊，那这个是另外一项预训练任务。

仇钧潼

01:43:01

那我们用很多很多的语料呢？把这个东西把它训练好了。那大家可以看那个，它里面有详细讲它的这个。啊，收集数据的过程。还有这个训练的这个过程啊？啊，那这里画了这张图呢，是上去上下去猜测。我们有语料啊，那你输入一句话，那你应该输出下一句话。那你的decode输出了下一句话跟光初始有什么区别呢？那这里也会产生一个误差，然后我们就可以对整个transformer来进行训练。好，那这个上游任务呢，就这样就做好了啊！啊。

仇钧潼

01:43:43

对，没错，就是同一个船方法，我们既用猜词猜谜游戏来训练它，也用给上去拆下去的游戏来训练它。都同时训练都同时训练啊！好，那下面呢，我们看一看。他的下游任务啊？他来下我任务。下游任务有很多了，等一下我逐向逐向给大家讲。一般呢，它都是通过这个。我们训练好了这个它的的部分。再额外的加上一些其他的简单结构来组成，用它来完成下游的任务。

仇钧潼

01:44:26

好，那我们先看第一个下游任务。一起的对训练的预训练的时候是一个完整的，transformer是一块的，就跟D是一块的，但是我们用来完成下游任务的时候呢就不用。那个我们只需要encode就可以了啊！好，那我们先看第一个下游任务啊，它大概在论文里面给出了11种下游任务，都是自然原处理里面很常见的下游任务啊。

仇钧潼

01:44:56

那第一种效任务呢？是这样的，就是聚对分类，我给你两个句子。那你你预测一下这两个句子。的关系就是第二个句子，相对于第一个句子。是包含的，就是他们是一个互相包容的关系呢？还是互相矛盾的关系呢？还是彼此没有关系呢？就完全这上文跟下文是完全。没没有任何意义，没有任何的这个意思。

仇钧潼

01:45:23

我我们要做这个事情。那我们的方法是什么呢？那我们的方法是这样的。那这个左边这个？这个其实就是里面的银扣的。但是我们不叫不叫transformer了，我们叫把它叫B，那就是拿猜谜游戏跟。给上去拆下去的游戏预训练好的。那个transforma的in的部分，我们把它称之为叫but。把它称之为叫but，那这个有什么作用呢？它就是11个啊。你给他一个输入。那它不就是有一个相同维度的输出嘛，对吧？

仇钧潼

01:46:04

那这个相同维度的输出呢？我们做这样的事情，大家看这个图就明白了啊！诶，那个图在这里啊，这个图在这里。就说我们的输入。这里。这个CLS呢是分类标签，这个非常重要，等一下我们重点利用这个部位啊！好，然后呢，这个地方是第一句话。我说的第一句话，这个是分隔符隔开两句话的。好，然后呢，这个是第二句话。好，那你这么一上去这些东西就token一上去以后，那你不就输出同样多的东西吗？对吧，就就是你输入多少东西，它应该就输出多少东西。

仇钧潼

01:46:51

那这个就是这里面的每一个绿色小块块，就是那个输出矩阵C里面的一行。好，那这个cios呢，我们称为叫分类标签。我们就拿这个分类标签做文章，那你最后出来的时候第一个这个C。对应的是分类标签的输出。然后呢，我们把这个C啊，大家看这个图我们输进去了。我们的输出啊，那这个就是对应于那个CLS的那个。出来那个都那个token？我们给它接上一个全连接的BP神经网络。最后呢，我们对它做一个得到一个分类变量。就现在不是三分类嘛，对吧？不是三分类吗？那你就是说啊，我有这个中立矛盾包容啊，三种关系。那你你这里这个CLS标标签？对应的那个输出的向量输入一个bp神经网络以后呢，我们最后得到三分，一个三分内。那我们就完成了这个工作了。那所有的用不来完成下游任务呢，都是这种技巧，就是说我们用作为基础。一般我们会加入一些。记号一些特殊的。然后呢，我们在输出的时候呢，捕捉这种特殊的token。那这个token呢就？承载了这个，你输入那句话的很丰富的信息在里面呢，然后我们再通过一个bp神经网络来得到最后的这个分类啊，然后看你的分类到底是什么？那他就完成了这个聚对分类的这个任务了啊，那这个是第一个下游任务。那第二个下游任务呢？是这个？

仇钧潼

01:48:37

单据分类那这个很简单，那比如说你输入一个评论。那这个评论到底是表示好评，差评还是中性呢？那他是什么态度呢？那这个也是一个三分类问题，那这个技巧也是一样的，我们再输入一句话啊，后面这个是这句话。前面我们加一个分类标签，这个是一个很。很重要的一个捕捉器啊，这个是我们的捕捉器。好，通过这个报以后把这个不就是传是环保的嘛，对吧，它最后输出应该是相同的东西。比如说你这个句子有AN的单词，再加上一个分类标签。那它的这个长度应该是N加一。好，那它出来的话也是N加一，那我们也是揪住这个分类标签对应的那个输出的token。然后给它接上一些结构。就是给它加上巴巴啊，这里有一个神经网络啊，最后输出就是三分内。那我们用它来训练这个就够了。那这些下游任务呢，一定是有监督学习了，就说我们肯定是已经有一个标签好的一些语料。那我输入。那得到这里的输出，然后我们再用它对应的标签来。来训练这个。全连接的这个bp神经网络。那训练好以后呢，我们就可以用来进行预测了啊，那这个是分类器啊。

仇钧潼

01:50:00

那还有这个？问答机器人。那问答机器人是做什么事情呢？我输入的是问题。以及可能包含答案的一段文字啊，这个听起来很熟悉啊，这个那我们在第一堂课不就是讲过这个东西吗？就说我们把这个可能包含的答案放在一个向量数据库里面，还有别人问的一个问题。<笑声>然后呢，我试图通过一些方法，从这个限量数据库里面把我的答案把它总结出来啊，那这个不原来也可以做这个事情。

仇钧潼

01:50:33

那输入问题，以及包含答案的一段文字。那我这个输出是什么呢？这个稍微粗糙一些就感觉。呃，让人感觉有点这个不，不大合适的一种感觉啊，它的输出是这样的，就是我要在这段文字里面标出这个答案所在的位置。

仇钧潼

01:50:52

那起始的位置，我们标个star结束的位置呢？标个N，然后在star跟N之间的那些位置呢，我们就打一个span。就说我们打一个连续的标号。就是从star开始啊，中间是，然后最后就是N。那这个事情呢是怎么做呢？呃，大概上就是。这个样子啊！我们也是有一个分类标签开头。然后呢，我们这里这一句这句话是question问题啊，这里是分隔符后面这里。后面这里呢是包含了答案的一段话。

仇钧潼

01:51:38

好，输进去以后呢？那我们就得到了很多的这个talk呢，对吧？也也是相同数量的。但这一次呢，我们不是在这个事情上做文章。那我们试图做的是？要给它打标标签，那上面这里应该还有一个lstm网络之类的。然后呢，我分别给这个TE到呃给这这边开始的TE到TM分别打上标签，就把这个部分在上面呢，再接上一个这个LSTM之类的结构。然后呢，分别给他们。输出一些标号。啊，那你可以想象最上面呢还可能有一个什么CF层啊，有这个维特比算法之类的，因为毕竟你这里一定需要用star来开始用a来结束啊，中间呢一定要给它，你不能够说AN开始结束。

仇钧潼

01:52:29

便开始然后大结束那这个都是？呃，违反规则的，所以你上面这里呢，应该还要加一个C。那我记得好像在给大家的预期材料里面应该讲过这个东西，就是加了这个cf层以后呢，就是我们可以从。因为你每一个东西。就这里每一个talk呢，它都会搜出一个向量。它不是输出唯一的一个soft向量，而是每个talken都会输出一个向量。它里面有一堆概率，就是说我在。每一个soft向量里面选择哪个标号？

仇钧潼

01:53:08

综合起来这个概率是最大的。啊，那这个这个CF层呢，就可以用来做这个用途。好，那选好了以后呢，就等于说我们的答案就已经出来了啊，那这个是另外一个下游任务。那还有一个任务呢，就是命名实体识别，那这个大概跟我们的知识图谱也有一些关系啊。那我们这里输进去的也是一个分类标签。然后呢，就是一大段话，后面这里就是一堆111大段文字。我要求把里面的实体把它找出来。那这里也是用标号的方法了？就是我们一般来说呃这个。怎么在一段话里面找出这个实体呢？比如说我。

仇钧潼

01:53:57

爱北京。天安门。这句话。我们找出怎么找出它的实体呢？它其实可以把这个转变成为一个标号问题啊！比如说我啊，这个肯定不是实体，那我就给它标个OO的意思呢是other。就是它，它是其他东西，它不是实体啊！然后这个四也应该标个O。北京这个是一个实体。北京是一个实体，它是一个地名。那这里我们要标一个B？那就表示它是一个loc一个地名。B代表地名开始了啊，然后经这里呢是一。LOC. 就代表这个地名结束了，那天然门也是一样，它也是一个地名啊！B。BELOC. 这个门呢是ELOC？然后这个R呢是地名中间的一个，那可以叫amloc。那我把每个字都打上一个标号以后呢，就相当于把这个实体。把它提取出来了。那这个命名实体识别的问题呢，一般来说都是这么处理的。我们把一个很抽象的这个自然语言处理里面的一个事情呢，转变成为一个序列型分类器。

仇钧潼

01:55:20

就说我这个分类器。不是对一个静态的东西来分类，而是对于一个长的序列来分类。那你在这里取什么标号呢？不光跟你这个字本身有什么关系，还跟你的前后左右是什么东西？那这个是有关系的啊，那这个？这个是自然源处理里面的一个很基本的任务，那这个but呢，也可以做这个事情，那就是无非你把这个语料塞进去了。我在这里呢就。也是。对，每一个token的那个输出来的那个。呃，把它做一个soft mass或者做一些这种LSTM之类的。那最后呢也是一个CRF层来判定它的标号是什么啊？那这个是？这个是另外一项下游任务啊，我们继续吧！哦，没有了。好，那这个就是这样子了。大概是这样子就是。怎么说呢，就是说。呃，刚才现场有听众问这个cf层到底是什么概念？那我在这里画一下吧。这个有点像什么就是。我每一个词你可以看成一道篱笆墙啊！他可能都有一些标号的选择。比如说我们有。有这个中间这个词。还有一个单字词啊，有这么几个选择，有BMB EMS四个选择，大家不用管这些选择是什么意思啊？你先。这么听着就行，然后第二个字对应的也是有。Bems四个选择。第三个对应的也是有B。B EMS. 这么几个选择，那这里每个字啊都是一样的。

仇钧潼

01:57:31

金童有没有办法能够帮我？

仇钧潼

01:57:33

行，我写在这吧！这样吧，我我我用这个例例子吧！比如说。我爱北京天安门。那我现在想对这句话来做一个分词。那分词的话呢，其实也可以看成是一个序列型标注器。就说单字词。一般来说呢，就是比如说我这个是一个单字词对吧，那一般可以标成SI，也是一个单字词，就标为S。北京这个是双字词第一个字。对应的是B？第二个字呢？对应的是一天安门，第一个字对应的是B，第二个字对应的C。第三个字对应的是一。那大概最后的结果就是这样子了，当然这个我们是未卜先知的，这个是最后的结果。那你如果把每个字都做了一个。标签的分类的话呢？那实际上我们这个分词的任务也就完成了。但是这里的分类呢，是有一定的这个规律的，对吧，你这个。必须B跟一是成对出现，你不能够B单独出现或者一跑到B的前面。那这个是不对的。

仇钧潼

01:58:43

好，那下面我们怎么怎么来来决定这个东西呢？那你可以看成每一个字，上面有一道篱笆墙。我应该在那个自然语言处理的那个预习的视频里面讲过这个东西啊。那这四个分别是B？Ems了。好，然后呢这个B EMS呢呃这个。从什么转到什么是有一定的这个概率的，比如说。可以转去一？但是。从B就绝对不能转去S是吧，就从B转到S的可能性是零。那这个一也是一样一的，下一个呢？一定是B或者是S，它不不可能从一转到M的。那这些都是有规律的。然后呢，我们通过这个磁频的统计。比如说我开始后面有多少一个是I呢？那这个我也是知道这个规律的，对吧，这个这个我可以通过这个语料的统计来得到这个统计规律。那所以呢，从左边这堵墙到右边这堵墙。它的这种状态的转移的概率我是知道的，比如说这里B啊！这里是一。啊，这个是0.5的概率啊，然后这里是B。这里是S。那这个转移概率就是零，那所有的这些转移概率呢，我们都可以求出来。所有的这些转移概率呢，我们都能求出来。

仇钧潼

02:00:25

那最后呢，我们根据这些转移概率呢，变成这样一个任务，就是我们球从最左边的这道篱笆墙到最右边的篱笆墙找一条路。找一条路，这条路的概率是最大的。那如果你用对数来转化的话呢，就是对数距离是最长。就找这么一条路。那这个就是。我们在自然源处理里面的维特比算法。维特比算法。

仇钧潼

02:01:01

我听明白了没有？那行，那这个慢慢再说吧，因为这个说来话长，这些都是比较深入的，知识比较绕，就不是这个。直接算直接算直接算啊，就说你有概率以后它就直接可以算出来了。好行，这个关于大家有没有什么问题啊？

仇钧潼

02:01:37

喂，对，可以开提问了。

王蓓

02:01:43

好的，有同学举手了。黄波同学可以提问啦！

仇钧潼

02:01:52

哎，你好。

仇钧潼

02:02:00

可以。

王波

02:02:00

哎，你好，就是那个就那个不不这这这个。

王波

02:02:05

右边那一块儿有一个安插，那个是那是多少块儿啊一般。

仇钧潼

02:02:12

呃，断断续续的没听清楚我。

王波

02:02:14

就那个右右边那一块就抵扣的那一块。

王波

02:02:18

抵抵扣的那一块儿一般是层数是多少块儿啊？那里面有多少块儿的？

仇钧潼

02:02:23

这个也是一个超参数，是由你你来定的，刚才我讲那个例子都是六。

仇钧潼

02:02:28

这左边是六，右边也是六。

王波

02:02:29

那我就想我就想确认一下，我理我理我的理解对不对啊，就是说。

仇钧潼

02:02:31

啊，这个是缺少？

王波

02:02:35

你从第一个单词预测第二个单词也是经过这个六个六个块是吧，六层。

仇钧潼

02:02:35

嗯。嗯。

王波

02:02:43

然后第三个单词就输入第一个，第二个单词也是经过六层预预测，第三个单词是这样吗？

仇钧潼

02:02:51

呃，所有单词经过的陈述都是一样，你讲的是decoder这个部分是吧？

王波

02:02:55

啊对对。

仇钧潼

02:02:55

因为decode才会分单词，如果是左边那个部分encode这个部分就不用分单词的，就整个矩阵就输进去了。嗯。

王波

02:03:03

对对对，我觉得抵扣这边那六层都要输过去，是不是也也挺慢的感觉？

仇钧潼

02:03:04

啊，对这个这个解码器是挺慢的，但好像也没有什么很好的办法，毕竟你要对它进行选择才可以做，下一步你不可能跳过这一步来做了。啊，就提提高了encode这一块的速度，但是这个decode好像也没有什么很好的办法。

王波

02:03:25

哦，明白了我就我就确认一下，我理解对不对啊，谢谢！

仇钧潼

02:03:27

嗯。啊，对的对的是。

仇钧潼

02:03:35

呃，看你怎么选，就是说如果你选择的标准都是最最高概率那个，那就应该是只有一种回答。它有一个温温度的参数，就是允许你可以，呃，在这个概率不是最高的，那那个里面也可以做选择，所以你可能看到不一样的结果。

仇钧潼

02:04:00

对，没错，因为有时候他差别不大的时候，你其实选其他也是可以的。好，大家看还有没有什么问题啊？

周涌

02:04:10

黄老师。

仇钧潼

02:04:11

诶，你好。

周涌

02:04:11

能听见吗。

仇钧潼

02:04:13

啊，能听到，谢谢谢啊，听到。

周涌

02:04:15

好的。呃，我有一个问题啊，就是上节课。上一次课程的时候方问。

仇钧潼

02:04:20

嗯。

周涌

02:04:22

就是您这边再介绍就方问是。

周涌

02:04:26

作为一个特征提取器对吧？然后这。

仇钧潼

02:04:29

啊，曾经有这么做过，现在可能已经不是这么做了，就曾经这么做过，就说把那个波特的一个。

周涌

02:04:32

嗯嗯。嗯嗯。

仇钧潼

02:04:37

呃，标签，然后最后再接上一个lsstm。来作为这个输出啊，当当时是这么做过。

周涌

02:04:42

嗯，对那，那我不知道您方不方便就是透露。

仇钧潼

02:04:51

哼哼。

周涌

02:04:52

方问现在是拿哪个，就是应该是用一个开源的模型，作为一个那个模型的基座。

周涌

02:05:00

呃，做追加训练的对吧？

仇钧潼

02:05:00

对，没错，因为这些大模型呢，都不是普通人能够有能力训练的，就是先不说算力了，就算你有算力，你也得要收集语料才行啊，对吧，那这个也是需要很大工作量，但是呢，我现在因为这个是一个公开的课程，就是我。

周涌

02:05:08

嗯嗯。

仇钧潼

02:05:16

我也不是很方便，在公开的环境底下说我们这种专利的技术是怎么做的，因为我们的员工都在听着。

周涌

02:05:19

嗯嗯。嗯啊，我可以理解。呃，没问题，没问题。对，那那我换个问题啊，就是您这边有试过基于其他的开源模型做训练吗？比如说清华的G IM。

周涌

02:05:35

就效果哪个会更好一点？

仇钧潼

02:05:37

呃，清华的gom呢？我们也试过，但是呢？这个感觉这个效果不大好，就是它相对于我们这个方问这个场景呢，它的规模有点太大了。

仇钧潼

02:05:49

那太大，有太大的坏处。

仇钧潼

02:05:51

那我举个例子，什么情况让我放弃了这个清华的这个glom呢？我有一个很印象深刻的一个。

仇钧潼

02:05:59

呃，这么一个案例啊，就是让他开处方。开药啊。

周涌

02:06:03

嗯。

仇钧潼

02:06:04

这个它开出了一个处方，里面包含了一种药呢，叫做耳环素酶。

周涌

02:06:04

嗯。

仇钧潼

02:06:10

呃黄就是那个？

周涌

02:06:10

嗯嗯。

仇钧潼

02:06:12

这个女士戴的那个耳环那个那个装饰的那个东西，就它把耳环当成是中药了。然后呢，这个这个剂量呢是树莓？

周涌

02:06:19

嗯嗯。

仇钧潼

02:06:21

那这个把我们都笑笑趴下了，都几乎这个这个是我们不敢再用它了，就是如果你把这个东西送到医院，这个医生看到这么一个处方，一截图在网上一说，那这个院长本身都不好下台了，这个我们不能卖，这样的东西是。

周涌

02:06:35

嗯嗯。

周涌

02:06:37

原因是不是因为它是一个呃？

周涌

02:06:42

LLM就是一个大的，类似于JA。

仇钧潼

02:06:46

嗯嗯。

周涌

02:06:47

A G C的一个大的模型，它不不适合做这样嗯嗯。

仇钧潼

02:06:49

对，没错。对，它不是一个专业领域，里面的东西，就是它拉七拉八的东西太多了，这结果这个导致输出的结果有点不可控。

仇钧潼

02:06:59

就说我们需要的只是一些比较。

仇钧潼

02:07:01

狭窄的一些输出就包包括它的格输出的格式，输出的内容都是相对比较固定的。当然这个这个输出的种类还是很大量的，但是相对于大语言模型能够完成来完成的能力来说，它也只是其中一个很小的子集，是吧？那这个GM的话呢，我觉得有点太太虚胖了，就是有点有点不扎实感觉。

周涌

02:07:08

嗯嗯。嗯。

周涌

02:07:24

哦，明白，明白，那方问就是方问我相信他的场景应该挺多的，就呃我们所知的。

周涌

02:07:32

就您之前有介绍过的。

周涌

02:07:34

应该就是他的问诊的就是问诊，并且出方的那个机器人。

周涌

02:07:39

对吧。

周涌

02:07:40

呃呃，我想知道一下就是您接的是but的哪一类下游任务呢？它有四大类嘛？

周涌

02:07:48

应该是。

仇钧潼

02:07:50

啊，它有很多类的下游任务，确实啊，有11类下游任务，我只是讲了几类啊。

周涌

02:07:52

对，那么。嗯，那么问诊机器人属于接就接的，应该是他的问答机器人还是命名实体还是分类还说都有。

仇钧潼

02:07:57

嗯。呃，应该都不都不属于，就是我们方问是单独一个下游任务，因为它也没有限于说这个必须只完成。那11种下游任务之一，你其实可以完成很多其他的下游任务，只要你给它合适的结构就行了。

周涌

02:08:03

明白明白好的嗯对，因为主要是看到现在的那个就清华的，就他风头很近了。

周涌

02:08:27

当我们在做实验前，要在在面临选型的时候，不太想走弯路，对方位方位因为作为一个成熟的，对吧，唉！

仇钧潼

02:08:27

啊对，就说如果你是做一个聊天机器人呢，就陪聊的那种呢，我觉得可以用大圆模型没关系。

仇钧潼

02:08:42

就是如果只是做一个专业的用途，比较垂直的那种呢，也不一定挑那么大的模型来去做它。

周涌

02:08:42

所以您的建议还是应该就我们这边是。做的领域还比较狭窄呢！就我们是只做。

仇钧潼

02:08:57

嗯，就是参数量要跟你做的场景要相适应。

周涌

02:08:57

呃对。

仇钧潼

02:09:01

就说你，你如果这个。

仇钧潼

02:09:04

呃，几个亿的参数就能搞定了，就不一定要搞到这个什么。几百几百个亿啊，这这这个这个量上去了。

周涌

02:09:12

嗯嗯，明白明白，嗯，对，因为熟悉一个模型还是有成本的嘛，呃，就是随着我们对探索的深入，有可能会出现被这个模型绑架的可能性。

仇钧潼

02:09:18

对，没错。

周涌

02:09:24

有有这个沉默沉默成本了就就。

仇钧潼

02:09:24

对，没错。是是是我们，我们这个方问在研发过程里面，它的这个输出结果是啥？奇葩的都有，我们都见过很多笑话。就是就什么奇葩的结果都出现过。

周涌

02:09:34

嗯嗯。嗯嗯。

周涌

02:09:38

对，那市面上就流行的那种预训练的呃，就B类的模型。

仇钧潼

02:09:43

嗯嗯。

周涌

02:09:47

它的开源许可协议是什么样的？你知道吗？

周涌

02:09:51

就是。

仇钧潼

02:09:51

这个我真的还没有仔细研究过，但大大家好像就拿着起来就照用，也没有谁征求过google的意见<笑声>就google呢，好像是相对比较友好的一家厂商，就好像从来也没听说过他拿着这个。

周涌

02:09:54

哈哈，好的好的好的。

仇钧潼

02:10:06

这个开开源协议去告谁也没发生过这样的事情，所以好像大家都没留意到那个条款是怎么写的，但是呢，你像那个oracle这样的厂商。

仇钧潼

02:10:17

就得要小心了，因为他是经常很喜欢去告别人啊，谢谢嗯！

周涌

02:10:17

嗯对对是的是的。好的好的，我没问题了，谢谢黄老师！

仇钧潼

02:10:25

啊，谢谢嗯，好，谢谢！

仇钧潼

02:10:28

好，那我们继续吧啊！

许涛

02:10:28

放放我问一个问题。

许涛

02:10:33

嗯，就是那个。嗯，刚才说那个就是在咱们那个就是呃方面里边，不是之前用那个的时候。他输出了一些什么耳环，一些甚至都是一些非专业的一些一些可能答案出来了。

许涛

02:10:48

我想问一下，那是不是如果把它用那个GPT的话，它也是一个比较通，所谓通用可能就是什么情况都可能发生过的一件事，是不是他也会可能出现这种？

仇钧潼

02:10:48

嗯。呃，这个gpt呢也是有类似的问题，这个gpt有一个特点呢，它特别能胡扯。就我们曾经试过用这个GPT来写方解，就是让他写一下你为什么要开这个处方，就写这么一个事情。哇，他洋洋万年说了好多东西啊，这个离了谱的都有，就就什么都都写，就太能吹了。<笑声>。

仇钧潼

02:11:20

这个所以所以这个PPT也有也是有它的问题的啊，我们现在这个写方解都不大，敢用敢用它。

许涛

02:11:30

呃，人家然后我再问一个问题就是说。

许涛

02:11:32

呃，这个这个transformer还有这个或者就是什么这个它本身这个这个理论本身这个东西它没有什么开源，不开源的问题是吧？就咱们可以随便用的是吧？

仇钧潼

02:11:44

呃，我没听清楚你的问题啊！

许涛

02:11:46

唉，就是这个，比如说这个transformer本身它这个那么有篇论文，它里边这种这种方法，这个本这个方法本身它是咱们是可以随便用的是吧？它没有什么专利保护啥的吧，这这这东西。

仇钧潼

02:11:58

呃，论文上面写的东西呢？它只要没有申请专利的，理论上你是可以直接拿来用的。

许涛

02:12:05

这个这个能看到它是不是有专利申请啊，这这种东西？

仇钧潼

02:12:09

有一些论文有的呀，我们见过很多论文，他后面都申请了专利啊。而且呢，它论文里面也挖了坑。就是它具体。呃，这个参数怎么选？它可能不会告诉你，或者告诉你一个错的，然后你要浮现这个论文的时候呢，就发现根本不能实现它所生成的效果。这种太多了，这种这种事情太多了。

许涛

02:12:29

那但是咱们基本课上可以理解为咱们现在讲的这些东西里边应该都还算是。

仇钧潼

02:12:36

嗯。

许涛

02:12:37

嗯，还可以正常去用的啊！

仇钧潼

02:12:40

呃，可以啊，就是现在讲的它本身就有开源的，就是直接在网上都可以下载代码，那个呢本身也有一个哈，根它里面。什么transform相关的功能都集成在里面？这个下午何老师应该会给大家讲，好像也已经讲过了，前两天。呃，前两次课好像就已经讲过那个transformers就后面有个S那个。就是用来实现这个传输方法有关的功能的。嗯。

仇钧潼

02:13:12

好嘞，谢谢嗯！

王蓓

02:13:21

目前线上没有人举手提问啦！

仇钧潼

02:13:22

嗯嗯。不一定。不一定，就是你可以用自己写一个检查器都行啊。那举个例子，比如说你，你输出的药方有不对的地方，是吧？那我可以因为因为这个中药的品种就是300多种这个药点规定的，对吧？那我可以列一个名字的名单，在那里，你不对的话，我就。就给你来个纠错对吧？那这这样就可以了。而crf它不算是一个检查器，它是一个一个优化的算法。它是求一个最长或者最短路径的算法。那就是维特比算法。它，它不是损失函数的概念，你可以把它理解为就是一个图论算法。就是一个图论算法嗯！就是你你。你现在模型输出来的就是一堆概率向量嘛，就是每一个地方都有一个概率向量。问题是你你你选择哪一个的那个问题嘛？选择哪一个标？每一个标号都有一个概率，那你应该选择哪个标号呢？那这个维特比就可以解决这个，这个事情。你可以找一找这个数学算法书，比如说那个。有一本书叫做统计学习方法。呃，里面就有专门讲这个维特比算法。李航那作者是李航。这个原理说来就太话长了，就就是一个数学证明。这个这个比解释维特比更麻烦。这比解释清索维特比较更更要花更长的时间。嗯。这个就是。

仇钧潼

02:15:24

呃，这个不一定，这个就是我等一下讲的大语言模型这个东西了啊行。好的。那没有问题的，我们先先列为休息几分钟继续啊，今天可能要拖点时间。

仇钧潼

02:24:42

喂，大家听到吗？喂喂喂听到吗？

王蓓

02:24:58

对的很清晰。

仇钧潼

02:24:59

诶，好是行，我们继续啊，我们继续。呃，下面呢给大家讲一讲这个gpt啊！那ppt呢？这个号称是一个大语言模型。呃，好像这个不是后来才这么号称，不一开始的时候也没有号称自己是大雨节模型，它只是说自己是一个预训练模型，那这个大语言模型要讲的话呢，就首先要给大家讲清楚什么是语言模型。

仇钧潼

02:25:25

啊，那这个页面呢，我画的稍微有点花了。那这个大家看这里这个文字吧，这个文字是从。这个治安员处理里的一本教科书叫做统计治安语言处理。是那个钟成庆老师写的，因为我们曾经在念书曾经上面的长。讲过这个自然语言处理的课程，我就直接把这个片子。借过来使用了就可以省一点事，不用写那么多东西啊！

仇钧潼

02:25:51

呃，什么叫语言模型呢？一句话总结，简单的说。语言模型就是用来预测一句话出现的概率。大概有多高，我要预测这个事情。那假如这句话呢，是由WEW21直到WN这N个单词组成的话呢？那我们实际预测的就是P。

仇钧潼

02:26:15

WW21直到WN啊，这个单词序列出现的可能性有多高？那这个东西看起来好来预测这个怎么预测呢？那大家可以看一下我怎么拆解它啊？如果我用贝叶斯公司的话呢？这个这个概率可以写成这种形式。这个形式。那为什么可以写成这个形式呢？你把这个右边这里。展开一下就知道了啊，第一项。

仇钧潼

02:26:43

这个保留啊？啊，在床上PWE这个条件底下发生W二。那这个是什么呢？那不就相当于P？除以pwr吗。PWR嘛对吧。好，再下一个。这个是。W二W三。再除以P。WEW啊。一直藏下去？那成了最后一项的话呢？那个分母呢？肯定是pww21，直到wn。那这些分子？这些分子跟这些分母都可以约掉。那约掉以后，最后就只剩下这一项跟前面这些项。啊，那所以所以你可以把这个公式这个用毕业式的这个呃，方法或者用这个条件概率的定义把它展开，然后最后呢，这个就啊变成算这个这个事情。那算这个东西其实也是很难算的，为什么很难算呢？如果算。WWW EW二那这个还比较好算。那这个刚才说过了，那这个不就是等于？哦哦。等于这个吗？等于这个。那这个我只要给你语料。里面任何一个单词出现的这个次数我是可以统计的，因此这个pw。这个单词就pwe这个单词，这个概率，这个分母是可以计算的，这个没有问题。然后wew二前后两个单词的组合，这个出现的次数我也是可以统计的。那这个我也是可以计算，并且可可以把它存储下来。那这个时候呢，还没有超出我们的这个。计算规模或者这个存储规模的能力。那假设我们的这个？

仇钧潼

02:28:46

英文里面有10万个词汇，那常用可能也没那么多啊，我就保守一点吧，比如说我们有1万个词汇。那这个PWE那我们要保存的就是保保存跟计算的就是1万个概率是吧？然后WEW二这个组合呢？大概1万乘1万，大概有一亿种可能，那我们要计算一亿次要保存一亿个东西。那所以他还是能保存的。那我们保存好。通过这个语料计算保存好这两样东西以后呢？那这个。这一项。这一项一定是能算出来的。但后面的这些呢就很难了。这个你保存三项，那如果单次有1万个，那就是1万乘1万乘1万已经1万亿了，那这已经有点勉为其难了，估计还行，就用现在的计算机的这种能力估计还能撑得住，那你再往后呢四项的五项的。一直到N上的那那些就确实是太难算了。

仇钧潼

02:29:44

啊，那所以呢，我们一般来说呢？呃，不会计算这么复杂的语言模型。我们一般会用恶缘文法。那这个就是所谓的这个二am近视。这什么意思呢？就是说。本来我们应该这个句子出现的概率是这个概率的连长嘛，刚才已经写过了，就后面这里越来越长啊。第一项呢是没有尾巴的。第二项的话呢，是WEW二，第三项是WEWRW31，直下去越来越长。我们就把那些太长的尾巴去掉。那就把它变成这样的一项连成，这样的东西连成。那这个东西告诉我们什么呢？告诉我们一个很粗暴的数学假设。那这个假设大致的意思就是。下当这个当前这个词会出现什么？只跟上面一个词有关，关系跟再往前的词都没有关系了。那他是做了这么一个假设？那这个假设当然是很粗暴的，很粗暴的，但是呢，在统计学那个年代，在数持平那个年代。那我们也仅仅只能这么做了，就说我们算1万乘1万在一亿这个规模上还可以算，还可以存，但是你在网上再提一个级别，那基本上已经很困难。那个时候呢，我们是不得已而为之。好，这个是统计学的时代的时候的事情。这几页纸呢，全部描述的都是统计学时代啊！

仇钧潼

02:31:18

好，那现在呢？我们看一下神经网络时代会怎么样呢？那神经网络时代就厉害了，那这个就好很多。我们可以构造一个神经网络，那这个结构呢，可能是各种奇怪的东西，那我们就先不管这个神经网络是什么。那我们可以输入？W21直到wn减一。一共N减一个单词，那这个窗口有多大，你可以自己来定义。然后我们就推通过这个神经网络往上推。最后呢，出来一个独热向量，就画了一个soft mass向量，跟那个字典一样长的。就说下一个单词。出现的那个？出现是某个单词的概率都体现在这个向量里面呢。这个wan就体现在这里了，这个可能是一系列的概率。比如说第100个位置上这个概率最高，那我就选择字典里面，第100位置上那个单词作为dw的单词。那这一种呢叫做神经网络语言模型？那这个就厉害了。那就是说我们其实不需要保存。这个什么1万乘1万乘1万乘1万这么多的这个组合的那个。那个个数来算那个概率？那我们只要保存有一个神经网络就够了。

仇钧潼

02:32:43

那这个nnlm的概念呢？是由这个benjo？发明的。那这个原始的论文我稍后可以给大家参考一下，不过现在读也没有什么意义了，因为当年他提出的神经网络呢也很简单，比这个还简单，那你现在再去读，它已经没有什么意义了。然后这个BN九呢，也因为这个工作呢，他获得了这个图灵奖。啊，他获得了图灵奖。

仇钧潼

02:33:08

好，那这个是语言模型的意思，那我们现在知道了，语言模型呢，就是预测一句话出现的概率大概有多高？那以前在统计学年代的时候呢，我们只能粗暴的假设我们当前的单词。只跟上一个单词有关系，跟其他都没有关系，那这样的情况底下呢，才能够近似的算出这个概率。那这个方法就是右边这条公式。但是到了神经网络时代的时候呢，我们可以通过语料来训练一个神经网络，那我们可以通过前面一长串的单词来预测后面那个单词是什么，然后从而我们可以判断出一句话，出现的概率大概有多高。好，那算这个一句话的概率有什么用呢？那有很多用处？那举个例子，比如说。我现在做语音识别啊！我现在已经识别出。这个。一句话，他在语音在这个拼音上这么写着，就他是研究生物的。那当然，这个其实语音识别识别出来的不是拼音，它是一些因素，我这里做一个。说明而已就不那么认真了啊，就大家听着就行了，但实际上这个云识别不是这么干的啊！那你你对应于这个发音的话呢？那对应的这个文字可以有很多，比如说踏实。

仇钧潼

02:34:35

研究生物的？那这个是一种？这个语音识别的一个结果是吧，还有它使烟酒，生物的。啊，它是研究生物的，这些都有可能。有各种各样的可能，那我最后我应该采用哪一个呢？好，那有了语言模型就好办了。你不是原模型能够算一句话出现的概率吗？那你就算一算这句话出现的概率有多高？再算一下这句话的概率出现出现有多高，呃出现的概率有多高？再算这句话出现的概率有多高？啊，那最后一算，哎呀，这句话出现的概率很高。那这个结果呢？那就是它是研究生物。那就出来了，那解决了一个语音识别的问题啊！

仇钧潼

02:35:20

那其他的比如说分词问题啊？那这一句话啊！它是研究生物的。那其实它分词呢，有很多不同分的方法，比如说这个。它是研究生这个词啊，物也是一个词，这也是一种分词的方法。还有其他的啊，它是研究生物的。那我们也可以分别算一下这两句话的概率。那结果发现呢，下面那句话出现的概率要高很多。那因为没，没有，没有人会这么说话的，就研究生后面跟个物。那这个可能性太低了。这这这这这这个组合出现的可能性太低了，那所以呢，我们就会选择这个作为正确的结果。那这里原模型呢，就在这里就发挥作用了。啊，那这个是语言模型的作用。

仇钧潼

02:36:14

那这里大家明白了，什么叫语言模型呢？下面我们就开始讲这个gpt了。

仇钧潼

02:36:20

那ppt本身呢就是一种语言模型，它是一种神经网络语言模型。那大概的意思就是，你给它前面的一些词通常是限制一个窗口的长度，比如说前面的L。个词那我要预测dl加一个词会出现什么？那我就做的就是这个事情。啊，这个gpt就做这个事情啊，那这里G ppt一对应的文章是这里给了这篇啊，那这个已经在群里面发给大家，那大家呢，可以在群里面找一找啊。

仇钧潼

02:36:57

那这里简单的介绍一下这个G。那首先第一个呢，它是这个openai公司基于这个transforma。发展起来的一个技术，那这个跟那个butter一样，butter跟GPT就是一对兄弟。就是波特呢也是。Google根据这个transforma进行预训练，拿出它的decode部分来作为这个。呃一个。单独的部件这么形成，那GPT呢也是一样，它也是对这个穿梭方法来进行预训练。但是它拿出来的呢，就不是encode这个部分，而是这个解码器这个decode这个部分。拿出来作为这个。啊，它的这个主要的这个神经网络的构建。那这个GPT跟那个先呢，其实是先有open AI的GPT，然后这个google为了超越这个GPT呢，它才引出了but。那这个GPT是最先有的一种所谓的大语言模型啊！那他做的事情呢，就是这个。他要预测我给出前面的。开一个词。然后。之后的那个位置就从I减一到I减K这个词。那预测这个wi出现某个词的可能性有多高，它就是一个大语言模型啊，就是一个这个。一个语言模型。

仇钧潼

02:38:46

我这里要找一下刚才那张图啊！好，回到这张图吧！那我们可以看到呢，这个gpt主要是用右边这个解码器这个部分。那这里有一个疑惑就是。呃，那如果你用右边这个部分的话呢，它不是有一个弯是从左边这边过来的嘛，对吧？这这这这这这个弯路。从左边过来的。那你这个难道把它消掉吗？它消掉以后呢？这里这个qkv怎么产生呢？那它其实呢是把这个地方的结构改了，它改成是这样子。就上来还是一个三叉？把它直接改成这样。那这样一改的话，大家也就明白了。

仇钧潼

02:39:36

那在哪里？这个GPT原始论文里面呢，它并没有说这个结构这么改，它只是提到了用了另外一篇论文里面类似的结构。那另外一篇论文呢，就是给大家放在群里面，那个标号为。320的那篇论文。啊，那它里面呢就提到了这个结构的样子，它有一个插图，那其中最左边那张图呢，就是我现在说的这种结构啊，把这个地方改动了。然后其他的就没有什么了。那个其实。其他的没什么可说的了，这个GPT。

仇钧潼

02:40:36

这个gpt的训练的话呢，它就只用了decoder，它没有用encode，因为它的预训练任务呢就是语言模型。就是我有一堆语料嘛，对吧，然后呢，我就为给你钱开个单词，让你预测下一个单词，它就只有这个任务了啊。那所以他他一辈子都在做这个事情，他并不需要有encode的这个部分。啊，就是就我们给出这个的部分。刚才这个图里面有了，那我们输进去的就是。WI减一我们预测的就是W。Wi这个单词是什么？那我们就不断做这件事情，就把这个decoda这个部分把它给训练好了。啊，那这样就搞定？那这里它所选择的参数呢？这个dota它会累积12层，然后这个呢是768位。那这个跟这个butter是一样的，参数量可能也差不多啊。好，那这个大概就是这样子了。那我们可以看一看它的一些下游任务。这个gpt能做了一些下游任务啊？

仇钧潼

02:41:52

那举个例子，比如说它可以做分类。那就好像我刚才说的啊，就是好评差评。还是没意见。那他这个是怎么做的呢？那他是这么来做的，就跟那个butter很相似。我输进去的内容是这个。输到那个解码器部分的内容是这个。就是首先输入一个。然后包裹住你给的那个那句话？最后呢，给一个一个。然后我就送到这个里面去了，对吧？那这个穿十号码送进去以后呢，你每每输入11堆东西。都会预测下一个东西是什么？当我输到最后要结束的时候，这个xram要结束的时候。那它不是对应有一个输出吗？那这个输出？这么画。这个是deco。那我会输入这个X一？啊，然后这个？那它一直你输到xchat的时候呢，这边有也会有一个输出，然后我在这里后面再接上一个线性层，再来做分类就可以了。就这样就搞定。

仇钧潼

02:43:23

那这里这张图呢，大概描画了每一个任务分别是怎么做的啊？这个是分类的任务。然后还有蕴含？蕴含的任务呢就是。呃，我给一段文本。就是前面这段文本我给出一个假设。那就是这个文本里面是否蕴含了这个假设？那这些当然都是有监督训练的，就是这个，这个gpt本身只是作为一个特征提取器而已。那我把它包裹成为这样的一个talk的系列。用star开始，然后就是你的文本。然后是中间的一个分隔符，再来你的假设最后。然后输到xchat的时候呢，对应出来输出来的东西我们会送到这个一个线性层这里。送到一个县城那里，然后呢这个？然后我们就会出这个。还有相似性的判断就判。判别两段文本是否相似，那这些都很相似的，就是就基本上那个技巧都是一样，那主要是利用了这个。

仇钧潼

02:44:45

还有可以做选择题啊？好，那这个是GPT一的，那大家可以看到呢，它的整个思想呢跟这个。Dot是比较类似的。但是这个butter的效果呢比GPT一要好，两者是规模类似的网络，但butter的效果呢要明显比GPT要一要好。为什么呢？原因很简单，因为gpte它是一个大语言模型，它是只能够往后看的。就是我通过之前的股价预测这个你后面的那个股价，那这个事情是比较困难的，对吧？而那个but呢是知道。前后左右就是你，你中间挖，突然间挖空了一个单词啊，你猜一下这个是什么，那我前后都知道了，那中间这个是比较容易知道的。我我如果知道今天就是昨天跟明天的股价，让我预测今天的股价，那这个相对来说比我要预测以后的股价，那肯定要容易很多，对吧。那从这个角度来看呢，那这个其实是取巧的。那他为了打倒这个gpt啊，就就是要证明自己的能力，这个结构要比这个GPT要强，我的参数规模比你要厉害。但是呢，我做出人来比你更好的效果，那我凭什么呢？那我就不往后看了，我就从中间抽一个出来，那我效果当然要比你好了啊，那大概是通过这个来实现了它的这个诉求。

仇钧潼

02:46:18

好，那下面呢，我们讲一讲这个gbt二啊G。

仇钧潼

02:46:35

那ctp的对应的是这篇文章？大家可以在群里面找一下。GPT二研究的动机是什么呢？刚才说过了，这个GPT一出来以后。呃，很快就被这个google搞出一个出来超越了。那这个gpt二呢就想翻身？那翻身的话呢，大概有两条路径。那第一条路径呢就是。你的参数更加多，你的参数比gp要更加多，比不要更多，那你的效果当然更好。但是呢，这个你参数更多，大家都比参数多，比算力。那这个其实意思不大，那大家都肯定知道你参数多，效果就好，那这个。

仇钧潼

02:47:16

Openai呢，就想到了，就是我参数不光多。而且呢，我还想我的这个能够做的事情更加多。那大家等一下可以看看了啊，他做什么事情了？他希望做以及多任务学习。什么意思？这个是一个很关键的思想？就说我们之前要做下游任务呢？都是用这个预训练模型截取出一些东西出来，再接一个下游任务的结构上去，我才能完成这个下游任务。那这个事情呢，其实是很有技巧性。并且呢是很需要有专业能力的。那对于一般人来说呢，可能是不可完成的任务。

仇钧潼

02:47:57

但是呢，我能不能够只是通过这个语言模型本身。比如说翻译。那我可能，哎呀，要接一个什么？LSTM啊什么呃一个什么这个注意力机制的一个下游任务的一个结构哇，才可以做好翻译这个工作是吧？那我能不能够把这个下游任务所需要的一个专门的结构也把把它省掉？我就告诉你，你帮我翻译这句话。那这个这个语言模型就能回答我了，就好像一个人一样，我命令你做什么啊？我你要做情感分析是吧？我也不不折腾了，我也不接什么下游的这种结构的框架了，我就问你啊，这句话到底是好评还是差评啊？你就告诉我吧！那他就回答好评啊，那那那就是好评，那这样就搞定了。那这个是我们现在要想达到的一个理想的状态，就是以后我们再也不用提什么专有结构这个事情了。你要做下游任务，你就好像人命令另外一个人一样，命令这个GPT你去做这个事情就行了。

仇钧潼

02:49:06

就像之前向大家演示的啊，本来。我要找这个命名实体，在知识图谱里面要找这个实体。那我还要做这个？这个这个九牛二虎之力要要搞一些什么下游结构啊之类的，对吧。但是呢，我在gbpt四里面怎么做呢？我就命令他。帮我分析这句话里面的命名实体。出来那他就叭叭啦就全部出来了是吧，我让他你帮我找出这句话，里面的三元组，结果他又全部就全部找出来。那这个我要实现的就是这个目的。那我们今天用了这个charge PPT以后呢，好像觉得这样的事情是理所当然的。就好像本来不就应该是这样子吗？但是在当年的时候呢？这个确实是一个很革命性的突破，就是我们抛弃了每一种下游任务，给他制定一个专门的结构，这种想法，而是直接命令这个系统你做什么他就做什么。

仇钧潼

02:50:03

好，那这里说了啊，在下游文物里面呢，我们不再加入X star这种特殊的标记。

仇钧潼

02:50:11

为什么加入它不好呢？原因很简单。因为你预训练的时候是用语料来预训练的，这些语料里面并没有这些符号啊，对吧，它有这些符号吗？它并没有这些符号，它用的全部都是自然的语言，所以你后面在做下游任务的时候给它这些。好呢，其实他就傻眼了，他绝对直接学傻了，他就觉得不合适啊，他就觉得不合适。刚才说到了，比如说翻译任务，那我们就不用什么下游的那种特殊符号了，我们直接就把这个发发出一个翻译指令就可以了。

仇钧潼

02:50:52

那在论文里面呢，说到了这个是有依据的，就是说他找到很多文章。其实呢，它在做翻译的时候呢，它里面呢就包含了一些这种翻译的指令。他直接都可以把这个翻译的指令。在这个语料里面就学到了，就不需要你画蛇添足的给它增加什么额外的结构啊，这类的东西。好，那这里的话呢，有它一个收集这个给GPT要收集语料的过程。因为这个GPT二呢是比GPT要大很多的一个模型。那他想了很多办法了。啊，比如说他找了一个叫做commoncore的一个开源的项目。那这个开源的项目呢，就是全世界的爬虫爱好者爬各种各样的网页。那爬回来的网页呢，它全部堆放在这个叫这个网站上面，给大家可以去进行这个数据分析跟挖掘。但是呢，由于这里面呢，这个的那个网站上面呢？这个垃圾太多了。那他又想了一个判断这个垃圾的方法。那大概的方法呢，就是把那大家都知道这个网这个网站，那这个网站呢，它是一个新闻聚合网站，它里面都是一条一条的新闻。那这个读者如果觉得哪个新闻比较好呢？你可以点击一下，它可以把它举一下。

仇钧潼

02:52:11

那它这个越好的新闻就会被点击的越多，它就会举到最上面，就自然而然的通过这个大众的众包的力量呢，可以筛选出好的新闻出来。那我们就在里面呢，找出那个新数比较多的那些新闻。那这些新闻呢，都是写作的质量比较好的，对吧，把这个作为正样本。然后呢，在这个里面？这个找了一些，随意抽了一些出来，大部分都是垃圾。随意抽了一些出来，那就假设那些都是垃圾，那这个作为副样本。然后呢，就做了一个打分器，做，做了一个分类器，就说你认为这篇网页有多大的概率，是一个好的文章呢？那会给出一个打分值，那我们就把这个open的一些网页全部扔给这个打分器打分。那那些分数高了才会被选进来？那最后呢，我们一共是搞了大概1500万个页面。

仇钧潼

02:53:06

大概八百万的文本40G的数据好像听起来也不是很多，感觉好像在我们自己家的那个3090或者4090上面都能够完成训练啊，那这个数据的情况大概就这样子了。然后这里呢是它使用到了好几个不同级别的模型。最基本的就是12层。768的这个人的位数。那最夸张的是这个？那这个大概是？

仇钧潼

02:53:38

十几个亿了，十几个亿规模的参数了。就是48层，然后呢，这个embedding的维度呢是1600。那大家可以看到了这些乘数。好像这个从12层到48层也没感觉到它变化很多是吧，你从768到1600，那不就是一倍多吗？结果它的参数呢？就涨了好多倍啊。涨了有那么多倍？好，那他用了这些不同的参数量的模型。分别来做了一些下游任务，这些下游任务呢都是。没有专门的这种训练没有用有？有监督，有标号的语料来进行训练，也没有设计，特殊的结构，都是用文字指令啊，我你要翻译，我就告诉你，你要翻译，你要做这个命名实体识别，我就告诉你做命名实体识别啊！那这些任务里面包括什么呢？包括综合阅读。那大概就是你读完一篇文章以后做选择题啊！然后做翻译？还有做这个写这个摘要啊？写这要另外还有这个问答。那这些都是常见的自然语言处理任务。

仇钧潼

02:54:50

那我们看一看它的精准度怎么样呢？如果用那个参数量比较少的模型，比如说117兆。大概是1.1亿个参数，用这个模型，那就真的是好差劲了。它的准确率才只有多少，只有个位数，这个都几乎为零了，去了就直播间几乎为零去了。那这些都是很可怜的数字对吧，这参数量很少的时候呈现出来的这个指标都很可怜。

仇钧潼

02:55:19

但是呢，一旦这个参数量上去了，到了这里。这个是15亿参数？15亿参数你看它的这个准准确度就直线上升。能够达到50%以上啊？这些都都比较厉害了。这个问答都能够达到一定的这个数量啊，完能够达到一定的这个正确数量。那这里暗示了我们一个什么东西呢？暗示了我们一个事情，就是大力出奇迹！就说只要我们的这个神经网络的这种。参数量上去了？它的那个智慧能力，它的准确度就自然就会上升。那刚才呢，在现场呢，我们几位？这个听众还在一起这个讨论这个智慧到底是什么东西？那这个在上次特训营里面呢，我也说过了，那智慧呢，很可能只是一种幻象。

仇钧潼

02:56:14

就是说这个人的神经元或者某个动物的神经元，你积累到一定的程度的时候。就会呈现出一种我们今天称之为叫智慧的。

仇钧潼

02:56:25

这么一种感觉。那这个我们做的这种所谓计算机神经网络，那其实也是一样，当它的神经元堆积到一定程度的时候呢？他在所有任务上的表现都开始呈现智慧化的特征。那我们就有一种幻想。那如果我的计算力越来越强。那我把这个参数量再乘上十倍100倍1000倍，那其实我只要有钱呢，我是完全能做这个事情，对吧！那是不是我们的智慧可以无限量的提高，直到超过所有的人类了。那这个也许是一个方向，那相信有很多的企业现在已经在尝试了啊。这个。这个。GBT五已经在路上了。好，那讲到这里的话呢，我们可以稍微停顿一下，看大家有没有什么问题啊。

仇钧潼

02:57:28

这个后面这个GPT三这后面的内容今天估计讲不完，那我们可以留到明天再讲啊。大家有没有什么问题不？

许涛

02:57:44

王老师。那个。

仇钧潼

02:57:45

可以。啊听到。

许涛

02:57:46

就我现在理解就是。嗯，咱们现在有了这个大模型以后，是不是之前之前的那些复杂的那些什么算法，这都不用考虑了，我这基本上就是后边就加加一个简单的一个网络神经，一个一个一个层就可以解决问题了吧。

仇钧潼

02:58:02

对的，就是之前呢，不是每一个下游任务都要给它加上一个特别的结构。

许涛

02:58:02

就就算了。

仇钧潼

02:58:07

来做后面的这个训练嘛，对吧，还要还要有监督训练，它是有需要有标号的语料，这些都是很困难的，就是你找有标号的语料跟做下面的一个合适的结构都是比较困难的事情，那现在这个GPT二以后呢？

仇钧潼

02:58:23

把这些工作全部都消除掉了，你只要问他问题就行。

许涛

02:58:32

啊，那比如说我即便是，但但是如果我要自己，比如说后边嗯。

仇钧潼

02:58:32

好的。

许涛

02:58:37

呃，我也给他就呃，在这个台还是有这个大模型，然后后边也是给他加了又加了一个transform的一个东西或者一个什么，那是不是可能更好一些，是可以这样理解？

仇钧潼

02:58:49

呃，是的，就是如果你有有监督的语料。呃，再加一个专门的这个结构上去，那我相信这个效果呢，应该比纯粹问他问题可能要好一些。可能要好一些，但是呢，也有一些比较的一些数据。呃，在另外一些文章里面曾经比较过，那发现呢，其实也差不了多少。那就是略微好一点点。

仇钧潼

02:59:20

好，大家看你还有没有问题啊？

王波

02:59:24

诶，黄老师，我问一个问题就是。

仇钧潼

02:59:25

哎，你好。

王波

02:59:27

为什么这个是一般浅层的记忆，学习什么的参数多就会容易过你和啊，然后然后这个这么多参数它过你和问题为什么没出现啊？

仇钧潼

02:59:33

呃，那可能是因为它的数据本身也比较多。它这个数据量大的话呢，确实也需要那么多的参数去表达。

王波

02:59:44

数据也在。

仇钧潼

02:59:48

哈哈，而且呢，它里面这个transform里面记不记得是有残差连接的？

王波

02:59:53

嗯。

仇钧潼

02:59:53

这个残差连接呢，可以使到一些多余的层啊，自动会失效。所以参数尽管出有很多，但是也是不容易过礼盒。你记不记得这个残差网络通常会达到多少层啊？都是1000层以上，甚至有上万层的都一样可以工作？

王波

03:00:07

嗯。

仇钧潼

03:00:08

都没有过礼盒？

王波

03:00:10

哦，明白，可能是因为这个。

王波

03:00:12

特殊的残杀结构让他。

仇钧潼

03:00:14

对，没错，它自动把一些不需要的部件把它跳过了啊。

王波

03:00:15

有可能他参数看起来多，可能实际没那么多哈。

仇钧潼

03:00:21

对，没错。

王波

03:00:22

嗯嗯，好，谢谢嗯！

仇钧潼

03:00:23

其实如果从过礼盒的这个角度来看呢，人的身体的绝对是过礼盒的。

王波

03:00:27

嗯。

仇钧潼

03:00:27

就是人人脑的神经元已经有500亿。它的相互连接呢，就更多了，就直奔直奔万亿这个级这个级别去了。那好像也没觉得这个人出现过礼盒的特征，好，谢谢！

王波

03:00:39

嗯，好像是。

许涛

03:00:44

王老师那个刚才说他那个就是刚才您说的那个就是残叉的那个地方哈，就在这个穿方面里边，这个插这个残叉用的是十几层的那个残叉呀。

仇钧潼

03:00:45

哎，你好。它那个是方，那个结构里面就包含了成网络。就是它有残差连接。

许涛

03:01:00

他那个厂。他是用了个几层的彩上网络，他那个那个？

仇钧潼

03:01:02

你看一下那个transform那个图里面不是有一些什么叫ADDA什么什么那个其实就是长差啊。

许涛

03:01:15

但但是说具体他在实际用的时候用了几层，这个咱们也没法去猜测是吧？他这个他这个穿，就他这个残差网络的这层数。

仇钧潼

03:01:23

长沙网络的好处呢，就是如果它。包裹的那个结构是不需要的，它会自动跳过去，不需要那个结构。

许涛

03:01:36

嗯，我是，我是想想了解它这个它自己用的这个残杀网络是是多少层的，不是可以接上千层，它这个每一个那个N里边不是，比如人家用六，但它每一个每一个块里边，它那个残杀网络肯定也不至于上千层这个残差网络吧。

仇钧潼

03:01:39

嗯。

仇钧潼

03:01:50

它不止一个，它每一层都有啊，每一层都有两个，每一层都有两个残差连接。

许涛

03:01:57

那他每一个都是1100都都是1000层左右的一个残差网络嘛？

仇钧潼

03:02:01

对，就是它可能参数有一一千多亿，但其实它因为这个残差跨过以后呢，它真正用到的应该是没那么多的。

许涛

03:02:10

呃，不不不是个数，就是它那个层数，不是它那个个就是1000层的一个残差网络嘛。

仇钧潼

03:02:16

所以它是城内的残杀网络对它层数。呃，没没有没有跳过这个没有跳过那个穿是方法乘这个乘数。我指的刚才说那个层啊，不是transform层，而是神经网络的层。就比如说像那个bp网网络的城啊？

仇钧潼

03:02:41

好，那我们趁还有一点时间把GPT三也讲一讲嘛。那我们继续了，听到吗？喂，听到吗？

王蓓

03:02:50

很清晰，继续讲课吧！

仇钧潼

03:02:51

好，那我继续吧，我把GPT三也讲完啊！那这个gpt三是什么什么样的问题呢？就是那个openai觉得gbt二还不满意，不满意是从前面这张图这里来的。就是说他对这条指标曲线感觉还不满意。希望能够这个把这个指指标曲线再提高一下。比如说我能不能够用？这个更多的参数或者用一些技巧性的一些方法把这个参数本来可以提高。

仇钧潼

03:03:22

然后呢。它里面呢，还要重点解决的是一个。呃，微调的问题。啊，那这个等一下再给大家说啊，下面我们先说GPT三吧。DVD三呢，有一个论文我也发在群里面，但是呢，它只是一个技术报告。它不是一个正规的论文。就是里面呢讲了很多这个gpd三的各种的优点啊，叭叭叭叭什么之类的。但是唯独就是他。怎么实现呢？这种细节的没有讲啊，有63页纸那么多，大家有空呢，可以看一下。

仇钧潼

03:04:00

那它主要的动机呢，是想解决GPD二？这个指标比较差的这么一个问题。然后呢，他也不再追求这个G OS那所谓的意思就是说，我不给你任何提示，你也能够完成一些特定领域的一些下游任务。那我是给你one song这个什么意思，等一下再给大家讲啊。好，然后呢，这个我们可以通过这个所谓上下文学习。来个来指导这些指标可以更高。那它里面的参数就更多了，一共有1750亿的可以学习的参数啊！那下面呢，我们看一看他的那个思路啊！那这里这张图呢？这个字比较小，那大家可以在你的手机或者你的电脑上面看。那我们对比一下GPT三的思路跟传统的这个fighting。这个思路的区别啊？那传统的话呢？我如果要调整这个GPT模型，那我就找新的语料过来，对吧，比如说我现在想GPT三也能开中药了。

仇钧潼

03:05:10

那我就找一堆医书过来。然后呢，我用这这堆医书来对他进行追加训练。那这个训练的过程呢，就是根据损失函数造成的这个一个损失值的计算。那我们对这个梯度进行计算，通过梯度呢，修改神经网络的全值。好，那最后呢，我们可以。

仇钧潼

03:05:32

这个得到一个新的神经网络，那这个新的神经网络呢？有望在开中药这个事情呢？上面呢，可以有更好的表现。啊，那这个这个是传统的思路。但是呢，在这个GPT三里面的思路呢是不一样的。他是这么来？是来做的啊！我们不再需要这个。对他进行追加训练。不再需要这个？然后呢，我们用一个上下文学习的方式。呃，是这个样子。我们先说one shop。就说我们在问。跟这个GPT进行交流的时候，我们给他一些提示。啊，举个例子，可能是这样的，我们现在要求把英语啊translate in english to french。把英语翻译成法语。那这个翻译应该是怎么样翻译呢？我给你一个提示一个例子。你按这个例子？来进行翻译就好了。好，那这个例子的左边呢是这个C？

仇钧潼

03:06:40

啊，那这个可能是某个名词吧，那这个是英文的名词，然后呢翻译成这个法文，就是右边这个样子，它中间这里呢是两个等于号，再加一个大于号。那是不是一定要写成这个样子的，也不一定？你写成个冒号或者写成什么一些别的符号，反正是不常见的都可以好，然后下面呢你就给出cheese这个来。的这个英文单词。又来等于等于大于我希望你回答右边这个是什么？然后这个这这个就是GPT三呢，就可以回答出一个正确的答案。那这个叫one so，我只给你一个例子。那这个跟相对呢，有我给你若干一个例子，就说我这里不止一个例子，可能是一个呃，可能是两个以上到几十个之间，但一般都不会太多。那这个我们发现呢，它就能够达到非常惊人的准确率。那这个准确的程度可以媲美有监督学习可以直接媲美有监督学习啊！好，还有一种呢，就GG就是我给你指令。但是我不告诉你例子。那这个呢，这个效果就比较差了。好，那这个GPT三在这里告诉我们的是什么呢？就是我们的微调。不需要了。我们再也不需要有监督学习了，甚至连无监督学习的微调都不需要了。

仇钧潼

03:08:09

那我直接给你一个例子。你就可以用更高的准确度帮我完成我的下游任务。他做到了这一点啊！那这里有一些这个？一些网络规模的这个一个表，一个参数表。然后呢，他也给了一些指标的对比。

仇钧潼

03:08:32

这里给了一些指标的对比啊，那大家可以看到了它的指标。在有了。这个。175B的参数以后就1750亿的参数以后呢？橙色这条线代表的是？这个蓝色这条线呢，代表的是接受那这个橙色这条线呢，明显要比蓝色这条线它的那个指标是要改善很多。那这个呢，就是GPT三所做的事情啊！那他已经从？这个20%几左右就提升到40%以上。甚至都已经。很接近于这个50已经达到50这个这个规模。

仇钧潼

03:09:16

这个任务。是什么？我忘了接下来了。这个任务是什么？我忘了接下来。完了这个就不追究了，那反正这个指标上呢，我有了一个比较大的突破啊，那GPT三就是这个样子。那大家看一看这个。有没有什么问题啊？那我们今天上午的内容呢？大致上就这样子了，那些关于这个模型微调啊，这些事情我们可以留到下一次课再讲啊。

仇钧潼

03:09:45

那个就比较复杂了，那个就那个是另外它有一个叫CLI P的模型。呃，这个CL CLI P它可以把这个图的这个语义跟这个文字的语义打通。啊，就利用这个图的一些注释的文字，嗯。就是就是他的训练的语料就是有图有文字的一些这么的一些语料啊，同时又有文字的语料啊！

仇钧潼

03:10:21

好，大家看有没有什么问题啊？

许涛

03:10:25

黄老师。

许涛

03:10:28

它这个这个这个东西是不是也可以理解，是一种有监督的学习啊？

许涛

03:10:35

我就说是不是可以这么理解一些，是不是可以这么？

仇钧潼

03:10:36

怎么说呢，我我也不知道这个这个应该不算是传统的，有监督学习吧，顶多就算举个例子。

许涛

03:10:43

但是可以往那个方向去理解一下还是可以的是吧？

仇钧潼

03:10:45

他没有学习什么，他根本就没有学习他没。它它没有改变参数，它没改梯度，没改参数，它根本就没有学习，它自称叫做上下文学习，那这个是一个比较有趣的东西啊。

许涛

03:10:51

对那那这个？就是就是那他是他在里边，他那他他是应该是起了个啥作用呢？他这个他这个翻。

仇钧潼

03:11:10

到了那个gpg四后面的时候呢，连这个连这个example都不用举了。就是你你直接？把它融合在你的文字叫做prom里面就行。那所以后面才提出了一个prom工程的这么一个概念，就是说你如果把你的这个问题问的特别好的时候。那你就可以得到更好的一个答案。

仇钧潼

03:11:31

那这个这个什么这些你也可以理解为是工程的这个雏形。就说你，你如果会问问题的时候。比如说你给他举一些例子，让他看一看啊，那他可能会得到更好的结果。那如果大家现在手头上有这个账号的话呢，你马上可以试一试，那我试过了，比如说我给了他几个shock那第一个中国啊，这个等于等于大号大于号北京啊，然后这个呃美国啊。等于等于大于华盛顿，然后我就给他一个国家什么乍得什么之类的。他都居然能回答出来，那这个这个不算神奇，这个还是语义比较明显了，我还举个举过一个例子，比如说。呃，这个。了啊，这个等于等于大于翅膀。然后鱼。这个等于等于大于这个棋啊，这个于棋。然后呢，我就问他这个乌贼等于等于大于是什么呢？他就回答我触手啊，那还是很厉害的，这个这个理解力还是蛮强。

许涛

03:12:34

那那就是说。嗯，不管问了些什么问题，那它这个GPT本身它这个模型本身它不会发生任何变化。

仇钧潼

03:12:41

对，这个我觉得可能有点类似于我们明天将要讲那个穿一穿11，就是它是一种平移关系，它能够捕捉到这种平移关系啊。

许涛

03:12:55

那那它这个因为按照咱们正常的理解的话，它这个这个模型它是不是应该是，是一直在不断的这个。呃，自我迭代，自我更新这么一个一个一个过程在里头。

仇钧潼

03:13:09

呃，不是这个意思，我猜想他是这么做的，我猜想啊，不一定啊，这个只是我猜想，就我举刚才那个例子吧，就中国北京啊，美国华盛顿是吧？那你。北京有一个向量，中国有一个向量，然后中国减北京这个向量呢，应该跟美国减华盛顿这个向量是平行的，对吧？应该是平行的，就好像一个平一样一样子。然后呢，我把这个平移的向量当你给我炸得的时候。我把乍得加上这个平移向量的平均值，我就大概能够知道那个目标向量应该是什么，然后把它解码，那就ok了。我们明天呢会讲这个叫穿11的一个算法。

仇钧潼

03:13:56

跟那个很相似，我到时候可以再提一下这个知识点啊！

许涛

03:14:00

好的好的行。

仇钧潼

03:14:02

看大家还有没有其他问题啊？

王蓓

03:14:12

目前线上没有同学在提问。

仇钧潼

03:14:14

好，那我们今天早上就先到这吧，就等下午何老师过来给大家一起实验啊！谢谢大家，明天见！