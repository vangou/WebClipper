# Day4a
关键词

大语言模型

神经网络

向量数据库

问答机器人

知识图谱

大圆模型

损失函数

垂直领域

相似度计算

实体对齐

仇钧潼

00:14

那个现场的录像记得开了观望。录像开了吗？昨天有看吗？

风铃之音

00:21

拆了的，昨天也录制了的。

仇钧潼

00:21

我听你有录制我的。哦，后来昨天那个录像放出来了吗？那个回放？

风铃之音

00:28

嗯，回放会在今天结束之后，明天统一明天一早，然后统一发给大家。

仇钧潼

00:48

帮我全屏智能？不能翻页这么奇怪？那他们从正对他？可以干什么？我的资料。所以他能写出一个？因为中文呢也是学。就他他打几个就只能。好，我们开始了啊，大家能听到吗？喂，能听到吗？

风铃之音

03:32

可以听见。

仇钧潼

03:34

啊行，那我们开始吧啊，那今天呢是我们最后一天的课程内容。呃，今天的话呢，主要是呃。讲一下这个知识图谱怎么扩展？以及这个之兔怎么跟大语言模型结合起来，产生一些令人令人这个印象深刻的应用啊，这个怎么做好这个问答？机器人大概是这一类的事情。呃，昨天呢何老师应该带着大家把那个。再加上向量数据库来完成一个呃问答机器人这个事情呢已经讲了。呃，这个也就是我们在一开始的时候。给大家的那个图吧啊，这个图我没有放在今天的这个PPT里面啊，那这个也是目前外界所谓比较主流的一种方法吧，你无论它的效果好不好，那它确实呢比较简单，比较容易实现。呃，这个很快就能做出成果，那这个好不好了，我们可以慢慢的去调它。呃，有一些同学呢，在听课的时候，他觉得这个跟上这个何老师的这个节奏有点难度啊，比如说好何老师讲一句话啊，你装一个结果你装了这个三天都没装上去，或者你装一个大圆模型，那也是挺耗时间的。那这个呃其实呢，我们最近呢也发布了一个产品，那大家如果觉得你的那个需求很急，比如说老板就催着你要完成一个问答垂直问答机器人，唉，你不是去学了黄老师那课了吗？那你学完以后你应该会做这个事情了，呃，这个你能不能赶紧？

仇钧潼

05:07

帮我做出来，我希望一个星期里面就能见到，那你这个写代码的话呢，这个肯定很花时间的。这个老师在堂上讲了一个idea你要实现的话，可能要几天几夜的这个调试代码才能搞完啊。那这里呢，我们也提供了一个资源给大家。那正好呢，我们最近呢。在做一个名字叫做开基地的产品啊，那这个产品最近就已经要上线了，那昨天呢，我问了这个开发人员，他说已经正在做最后的测试，那把一些这个不好的效果了，再调一调啊，那应该就可以上线。

仇钧潼

05:39

那这个胎基地大概是一个什么东西呢？我可以描述一下，那它背后的那个功能呢，就是让大家可以很快的做一个这个，呃，垂直领域的这个专家机器人，你不用买算力，不用买服务器，什么都不用部署。那你只要把这个文档，那这个文档包括PDF文档，TXT文档，呃，这个work文档excel文档，反正你能想得出来的文档你都可以传上去，那传上去以后呢，我们这个产品呢就会呃自动的对你这个呃文档进行分析。首先把它转成统一的这个txt格式。啊，那这个转文转文档格式呢，这个是简单的事情，这个就是一个IT功劳啊。然后呢，我们会有一个算法呢，自动的用最合适的力度来把这些文档把它切成一小块一小块的。就像昨天何老师告诉大家的样子，那我们其实也是根据。

仇钧潼

06:31

我们的经验呢，来设计这个课程的内容，因为我们本身也在做这种事情啊，那这个切成一小块，一小块以后呢，我们就用这个向量数据库，用一个合适的embedding算法呢，把它这个呃，每一个小块把它向量化。放在这个向量数据库里面，那一旦有人进来问问题的时候呢？呃，那我怎么去回答他的问题呢？那就是把这个问题也做一个。然后我就在这个销量数据库里面寻找相似相似的这些向量。那把这些向量？呃，对应的文本把它找出来，那可能不止一。不止一个向量，可能若干个向量，把这个对应的文本找出来，然后把这些文本呢打成一包，就是就是合并这一块送给这个大语言模型。那这个大语言模型呢？呃，你可以指定你使用什么大语言模型啊，比如说你指定使用啊。GPT四啊啊，或者说用这个清华的GLM啊，或者用那个羊驼啊，什么都可以，反正我们什么制作模型都有，然后呢，这个大圆模型呢，就会把它重新封装成一个按照这个回答框架底下的这种体验比较良好的形式。封装成一个答案，那后来回复这个用户的问题，那他能做这个事情。

仇钧潼

07:50

我们把后面的所有事情都已经做了，就是把这个服务器部署好了啊，上面还有若干个基座模型。那大家不要以为这个很简单，你光安装一个制作模型，可能都要花一天时间，那你还得安装十几个，那这个要很多服务器资源可能还要花很多，花上一个月半个月时间。那不过幸好呢，我们不是在钉钉里面就已经提供了各种这个模型的。服务嘛对吧，那这些服务呢正好是呃。这个给我们做了一些技术储备，那我们也可以把这个事情很快的做好，那大家做起来可能是不容易的，但是我们已经替大家做好了啊，然后呢，这个我们把这个把这种前端的程序啊，把线上的数据库啊，这些全部都部署好了。

仇钧潼

08:31

那刚才说了，大家只要把这个文文本语料一上传就搞定。呃，那这个产品上线以后呢，呃，我们可以提供这个web端手机端小程序端，那你可以在上面像那个。呃，你在这个T GPT一样进行这个问答，那这个问答跟T GPT的区别呢，就是它是按照你上传的文字材料来进行垂直领域的回答，就是它不是一个像T GPT的样子通用的，但是回答起来。比较水的专业能力几乎没有的这么一个机群，而是根据你上传那些文本的那个专业能力很强的机器人。你那个文本里面包含了什么信息，他就能回答你什么信息，就理论上是这样子，那当然要做到这个事情呢，也是不容易的，因为。

仇钧潼

09:17

这个在这个限量数据库比对的过程里面呢，第一个你选择什么样的啊？那这个很重要，有很多种啊。那如果你用what的话呢，那绝对你不能指望有很好的效果，那你很可能要用一些稍微有技巧性的。甚至可能要对这个算法要做一定的训练，用你的语料来做一定的这种。

仇钧潼

09:40

加强微调的训练啊，然后呢，还有这个相似度的匹配。那如果你光是算距离或者夹角余弦的话呢，就稍微粗糙一些，那可能有可能会用到一些更加复杂的方法，那比如说你可能要加上这种字符串。匹配再加上这种呃传统的这个向量相似度计算，这样的功能就多种多种的那种。呃，相似度衡量复合起来啊，变成一个比较复杂的东西，那你发现这么调理以后呢，才可能会有比较好的效果，那这个对于大家来说是不容易的，那我也不好说这个具体用什么方法，因为这个跟你的场景呢也有关系。就无论你选择还是选择相似度计算的方法都好，都是跟你的场景有一定的关系的，那我们能做的就是呃，提供若干种相似度计算的方法给大家，比如说ABCDE FG可能30种34相似度计算。

仇钧潼

10:35

然后呢，这个可能也有十种，那你可以勾选，那我也可以根据我的经验看一下你的文本材料，告诉你，你大概勾选哪一种可能会比较好，那这个实在不行了，我们自己来上就说，哎呀，我这搞不定这个效果怎么上传的不好，那我我们自己。来帮你搞，就是这个就有点人工的性质的，就是说我们另外给你特制一个这个相似度计算的方法，或者这个啊，那这个也是能支持的，那这个你这么些东西勾选好以后呢，那你这个经过一定的训练以后。就可以对外服务了，那对外服务的话呢？呃，第一个我们提供这个手机端呃网页端小程序。另外呢，我们还有API接口，那如果你不喜欢用我们的那个界面的话呢，你一定要自己开发一个界面，那也很简单，那你用你爱用什么来开发都可以啊，你用这个PHP啊，或者前端这个vivo写个网页啊，或者写一个。小程序啊啊，用这个unit app啊之类的写一个小程序啊，那这些都都没关系，那你最后调用我们的API就行。啊，那看起来就好像是你自己做的一样，这个老板也不会问你你后端是什么呢？这个他也不会管你这个。然后这个使用了顾客就更加不，不会管理了，你，你用了什么服务器，用了什么模型，这个根本跟他没关系啊，那这个你可以很快就把这个事情做好了，那我们目测一下的话呢，你做一个垂直领域的机器呢？这个大概可能一天时间应该就足够了。

仇钧潼

12:00

那如果你有这个线程的前端程序，或者有丰富的经验去写这个前端的话啊。然后呢，我们在这里，呃，这个收费形式是怎么样呢？那我们对商业用户的收费。呃，一般包含下面几个部分，那第一个是启动费啊，就是说你你，你把东西传到我这里来，那我要帮你放到库里面去做一些调整啊，什么之类的啊，那这可能会收一个费用，那如果你是纯自己上传，不用我们干预的话，那这个费用应该是不高的。

仇钧潼

12:29

那维护费还有这个？呃，点击费维护费就是每个月像那个ppt一样，每个月收理。一两百块钱的几百块钱的，这样子就可能是这么一个费用。然后呢，这个点击费的话呢，就是按照每一次点击多少钱来收费的，基本上是模仿这个GPP的那种商业的模式。

仇钧潼

12:50

那这里我允许这个我允允诺的给大家的一个福利。啊，昨天我看到群里面有同学问了，唉，黄老师给了大家什么福利了啊，有同学就说，唉呀，那个驱动云就是福利啊，那个驱动云算什么福利呢？那个不算福利，那个只能算是一个商业合作吧，就是他们跟卖给我们的价格比较便宜，那我们也是成本价，没有任何的钱就。转转转赠给大家了，就转送给大家转卖给大家。那大家这个呃可以用那个算力，那驱动云呢是那个高领投资的一家公司，他们做的那个猎户做X那个软件呢，可以把一张大的显卡切割成若干小的虚拟显卡。那这个功能呢，好像目前只有他家的软件才能做得到，就全世界里面只有他家才能做到。那关于他们这个事情呢，我们会在下一个课程，就是那个分布式。AI这个课程里面给大家讲啊，那这个公司呢，还是有一定特色的，尽，尽管他现在的这个服务能力啊。它的这个服务的那个内容啊，它还有点幼稚，那这个没办法了，因为它毕竟是一个初创公司，它不是一个。特别成熟的这个时间很长的公司，那慢慢会成熟起来的啊，那大家也不用这个。这个不耐烦啊，稍微给点耐性，那他们提供了算力呢还是可以的。那至少呢，我去跟他们砍价呢，还是有那个ar的能力，那大家如果需要这个很庞大的算力的时候呢，你可以找我，我去找他们，让他们提供一些比较便宜的算力出来。

仇钧潼

14:17

那算算一小时才几毛钱，你一天能够用多少小时呢？你难道一天能用24小时吗？那我就不信你一天顶多也就用个56小时，那才一两块钱，那你用一年才都不够1000块钱？你买了1000块钱的算你可以扣一年呢，你说实话，一年里面那个显卡的跌价跌了都不止1000多块钱。那这个显卡跌价是很快的，新的显卡出来就了显卡就跌价了，那所以你用这个租回来的算力的可能呃，还比你自己买显卡了还会更便宜啊。

仇钧潼

14:50

那我这里其实真正想说的资源呢，是这个，那我会提供给大家至少一年的免费服务，就是说你要做垂直领域的机器人，那只要你有场景，有idea有这个语料。啊，那这个无论什么题材我们都很欢迎。啊，那你也不用担心，我会道歉，你什么商业秘密之类的东西。那第一个这个东西很简单，这个没有，没有啥秘密，就是我在上课里面我都随便跟大家说，也没有觉得这个是什么秘密，对吧。那当然你的语料是一个秘密，就是你用什么东西来训练它，这个是一个秘密，那这个秘密呢，我们可以允诺，就是我们可以签个合同。那我们绝对不会把这些材料交给别人啊，然后呢，这个大家可以一年里面免费用我们的这个全套的服务。那当然这个首位免费的也是一个合理的范围之类的，那比如说你一天的点击，如果达到10万次以上，那这个算力呢，我可能就吃不消了，你不能点击太多。

仇钧潼

15:41

是吧？呃，另外呢，你你上传的资料，那如果是这个几十兆几百兆啊，那这个。这个100篇文章啊，这个1000篇文章还可以，你不能上传11亿篇文章，那这个我们可能没那么多资源来支撑你的服务，就反正在一个合理的范围之内吧，那大家也能明白什么意思啊，就不能够这个随便滥用我们这个资源。啊，那如果在。呃这个这个我不管了，这个转不转卖我不管<笑声>我也没有办法知道<笑声>这个大家都很有商业头脑啊，马上有现场的同学提出来能不能转卖啊，这个我不管这个事情就是你反正你有权利使用就行。然后呢，这个如果一年之后呢，也可以有条线的延续啊！

仇钧潼

16:24

这个有条件的延续继续，那这个什么条件呢？其实可能应该就是一些简单的条件，比如说啊，你又参加了我们的这个明年的特训营啊，或者说呃，你把这个其他一些顾客介绍给我们。啊，这个付费的顾客介绍给我们，那，那这个就可以免去你以后的费用。那总而言之来说，这个总的成本应该是很低的，那大家呢可以考虑一下这个事情啊，就说如果你学完这四天的这个特训营以后，那如果你能够自己马上能做出来能实现，那当然老师很高兴。那证明我的这个教学是得达到了我们的目的，达到我们的成果啊，那你觉得，唉呀，这个太吃力了，我连听理论听代码我都没听明白，这个我要学会的话可能得要花好几个月时间，那这个几个月都花都谢了，这个老板都等的不耐烦。

仇钧潼

17:09

那你想快一点，达到这个效果的话，就用我们这个产品，那我们一上线，你马上就可以上传语料来实现你们的机器人，那我们的这个技术人员呢，会，呃，这个全面的配合，你就我们在这个课程公号里面是不是承诺了这个每人？要拿着一个这个垂直领域的专业专家，机器人满载而归嘛，是吧？那至少在这个支撑的角度上，我应该是能做到，就说你如果用我这个tb的平台提供的功能，那你随时是可以实现这个目的，而且呢，我是。免费的就不收你费用啊，不收你费用。第一年不收，第一年不收。那第二年呢，应该也不至于说这个收了一个天价的费用呢，肯定也不会，那肯定也是基本上是以免费为主调了，但是这个可能有一些条件呢，有一些很容易满足的条件啊，那大概是这样子。那大家呢，可以看看这个呃，符不符合大家的要求啊！

仇钧潼

18:04

那这里我们这个台基地的话呢，还欢迎一些比较大的一些项目，比如说像这个建知图谱啊，这个因为这个很多企业都想见自己知知图谱，但是他没有这个能力，他他这个自然语言处理他搞不定了，呃，大家都听到何老师讲的那些了，有什么命名实体识别啊？等一下我还会讲一堆这种跟知识图谱有关的这种图，机器学习啊，这种技术啊，那这些技术呢，这都不是普通人能够看书，能自己理解了，这个太太复杂了，光理解它都很困难，就更别更别说写代码了，是吧？那这个一般来说呢，都是可能是有一些。这种。带有一些人工啊，按我们项目的话来说，可能就是比较消耗人天的。

仇钧潼

18:47

还有这个知识图谱，再加上这个大语言模型的形式。啊，就我们在堂上一直提供了这个主线索啊，来做这种垂直领域专家机器人这个事情。那这种东西呢，一般都是定制项目了，就很难做成一个产品，那我们也是想试一下能不能通过上传一些资料呢？我自动在里面提取这个知识图谱。然后呢，这个把这个东西做成一个产品，那目前呢还在努力中，那我也不敢说这个能做的一定100%完美能做的很好，但是呢，呃，至少说我能够实现一个简单的知图谱，然后在人工去增强一下。比如说这个顾客自己可以自己去增强一下啊，用一些我们给出的算法或者用人工挑选的方法来自己增强一下，然后呢，在已知图谱作为memory。他的记忆在结合上这个大语言模型的包装。那这个做出一个效果更好，更精准的这个垂直领域的专家集权呢，应该是有希望的，那这个是我们下一步的这个目标。啊，那这个呢可可能是要收费的，因为可能需要需要消耗我们的一些人天时间，那这个具体在面议了，就是大家如果有这样的需求啊，那可以告诉我，那我们呃，可以这个接口讨论一下。

仇钧潼

20:07

然后呢，我会给大家呃一个报价，那这个报价应该对于大家来说呢，也是呃，相对于我们正常的这个商业报价呢也会低很多啊。那大概我要介绍的我们这个台基地的服务呢，大概是这么一个情况，那希望这个除了课程以外呢，还可以通过这个产品啊，给大家带来额外的价值就减少大家在这个技术上在这种。比较细细微琐碎的工作上所消耗的时间啊！

仇钧潼

20:36

好，那这个大家也学完这么几天课堂了，那考行心里面都有一些想法，那我们下面呢，可以提早就略微进入一个短时间的一个讨论环节，那我觉得可以讨论一下这么几个问题吧。第一个呃，您听完以后呢，你觉得你想做一个垂直领域专？的专家机器人嘛啊，这个比如说金融领域的医疗领域的啊等等，那我们都欢迎。啊，然后第二个呢，需要什么样的支持呢？啊，比如说这个像这个TCD这个产品可能还不够，那我还可能需要其他更多的支持。那我觉得某一方面的话呢，这个没有涵盖在黄老师承诺提供的支持范围以内，我需要提供这些支持，那你也可以说。啊，然后呢？这个呃？那看看我们有没有合作机会的啊，好，那下面呢可以。趁着今天这个内容不是很紧很紧密。那我们可以接受一些提问啊，谢谢！

仇钧潼

21:31

啊贝贝可以打开提问。

风铃之音

21:38

嗯，好。

时间之外

21:39

喂，你好。

时间之外

21:42

诶，你好。

仇钧潼

21:42

诶诶你好诶！

时间之外

21:44

黄老师你说的那个？呃，前现在咱弄的那个菜基地。这是就是完全是基于向量库的是吧？

仇钧潼

21:50

呃，目前已经实现的就是就是昨天下午何老师给大家讲的那个算法。

仇钧潼

22:00

就是那个呃，基于再加上向量数据库来实现，那当然我们自己有自己的一些。

仇钧潼

22:08

呃，你你称为叫专利算法也行嘛，就是我们打算申请专利的算法，但目前还没有去申请啊，就是我们自己设计的一些这个embed的方法，还有一些计算相似度的方法啊，这个也包含了一些比较独特的方法。

时间之外

22:24

呃，就是说咱们可能自己有一套的模型是吧？

仇钧潼

22:28

对，没错，就跟普通的可能会有点不一样啊，这个略微做了一些改进，这个相似度计算也是我们做了一些改进。

时间之外

22:29

唉，就是说咱这种项链裤的话，就是说第一个就是可能咱课程初期说的那种，比如说它，呃，使用项链裤的话，它那种点那个搜索的数据可能是不准确的，这个咱是怎么解决的。

仇钧潼

22:52

对，就是呃，我们在找资料的时候呢，我只是笼统的说你可以扫描这个向量数据库。

仇钧潼

22:59

找出相似度匹配的若干条。

仇钧潼

23:02

那你这个相似度匹配呢，就有很大的疑问了，那匹配出来的东西是不是一定准呢？这个其实真的是不一定。

时间之外

23:10

这个呃。

仇钧潼

23:10

啊，真的不一定就是你的做的不好，然后你相似度计算的方法比较粗糙，那你可能找到了这个所谓相似度很高的东西，其实不一定靠谱。

仇钧潼

23:22

呃，然后呢，这个有一些时候可能，呃，它这个引入向量数据库的内容被分割成很多条了，就是说它可能是同同一个逻辑里面，不同的部分被分割了很多条了，被你被你错误的切割了。

仇钧潼

23:38

那这个可能会导致有一些资料呢，没有被你正确的选上，那就会影响这个回答的质量。

时间之外

23:46

对对对，嗯，像咱咱这种，比如呃我昨天也问过，就是说如果就是说数据特别多的话，达到上亿的话，其实是不推荐使用这个向量库的是吧？

仇钧潼

23:47

嗯。可能有这样的说法，就是销量库呢，因为它毕竟也是不是那么快的，就是它，它要比较那个相似度，尽管有他自己一些缩影啊，但是呢，这个数据。

仇钧潼

24:11

这个速度还是不是那么快？

时间之外

24:14

比如像比较嗯开源性比较高的那种。不行的话，你做了哪一些方面的优化呢？就可以简单介绍一下。

仇钧潼

24:25

呃，这个就11言难尽了，因为的方法太多了，就最简单单的就复杂一点的话呢，任何一种大语言模型都基本上都能做。

仇钧潼

24:36

就GPT啊，还有but啊，本身都都做了这些，那更更加厉害的方法就是用你的语料对它进行这个，因为它本身也是一个神经网络嘛，对吧！它本质上它也是一个神经网络，那无非把你输进去的东西。

仇钧潼

24:51

变成一个低维的向量而已啊！那你对这个神经网络呢，用你的语料再去进行一些训练，那使到能它能够符合你这个。呃，领域的那个需要精准度更高啊，那这个有很多玩法了，这个你你如果是专业完这个人，那就会知道这里面有很多玩法啊，谢谢！

时间之外

25:12

呃，最后一个问题咱们这边就支持哪一些向量库呀？咱们现在支持了。

仇钧潼

25:17

呃，支持什么来着，没听清楚，不好意思。

时间之外

25:19

下面裤下面裤支持哪一些项链裤？

仇钧潼

25:22

像库我们，我们这个如果你说TCT的话呢，我们就选定了，已经选定了是这个向量库了，这个他们应该好像安装了几种吧，我具体还没细问了，这个事情都都应该就是何老师向大家介绍的那些我们自己的其实也搞过向量的数据库。

仇钧潼

25:39

大概在几年前嘛，也搞过向量数据库，那我们也决定这个可能在稍后晚一点的时间等到我们都把这些工程完成以后呢，那我们再把我们的向量数据库可能呃，这个计划会重启。

仇钧潼

25:53

那我们的相应的特点呢？主要是他能够用上显卡。就是，就是其他向量数据库的基本上都不用显卡，它就是靠一些算法这样子来加快速度，那我们用上了显卡以后，那个速度真的是加快了很多很多。

时间之外

26:09

这一块的话应该是现在的话应该使用的一些一些开源的，比如说。

仇钧潼

26:14

啊，都是一些开源的，对，没错。

仇钧潼

26:17

其实现在太多了，因为现在不是国家搞那个信创，信创嘛，信创就是那个央企啊，国营企业啊等等啊，你要在三年之内全部。这个东西都要换成国产的。那这个基础设施都要换成国产的，可能包括硬件都要换，那其中有一项很重要的基础设施呢，就是数据库啊，那大家中间大部分人也知道了，我黄老师大概在呃，十几20年前曾经创建过一个数据库，社区叫I。那我我看到当年我IP发上那些朋友呢，都全部出来创业了，他们创业的一项很重要的内容是什么呢？就是研发国产数据库，就是我一下看到全国里面开的这个几百家国产数据库公司，然后其中还有。

仇钧潼

27:01

可能有几十家是IT报道网友开的啊，这个看起来都眼花缭乱了，就向量数据库是现在新创活动的一个呃很活跃的领域，所以你这个你根本就不用担心，没有产品可以用，有大把产品可以用，除了开源以外呢，就算是商业的。

仇钧潼

27:17

那还有很多厂家是可以支持的。

时间之外

27:20

呃，黄老师，我最后问一个问题，就是说昨天我问过何老师这么一个问题，就是说对于身份证号下电话之后进行一个检索嘛，咱这边就是说自己开源的模型的话，对于这种数据的话有做优化吗？

仇钧潼

27:35

呃，我我这个这个说的有点快，我没听清楚，前面半句话。

时间之外

27:39

就就呃，昨天的话就是我问何老师了，就是说比如说身份之后入到项链库之后再进行一个现代化搜索的时候。

时间之外

27:48

身份证号是，它是一堆数字嘛，无法进行一个精确的搜索，就是咱们这个呃自己做的这个模型的话是对这方面做优化的嘛，还是没有。

仇钧潼

27:48

啊对。呃，您您提这个idea很好，那我们现在应该还没有做优化，但是呢，我们将来也会针对一些常见的这种向量的内容，比如说很多时候我们可能是呃，针对一些号码或者一些什么这个一些字符串啊来搜索，那我们可以针对性的进行一些优化。

仇钧潼

28:15

身份证号码的话呢，就不用那个向量数据库。

仇钧潼

28:19

都可以找的很很快的，就是你你用那个B加速就可以了，就说你你知道整个身份证的那个那个号，那你用B加速就可以了，就数据库里面的B加速，那如果你是只是知道那个身份证号的一部分，他不知道全部。

仇钧潼

28:34

只知道一部分的话呢，我们也有方法，我们，我们刚才不是说我们曾经做过一个呃向量数据库的一个。

仇钧潼

28:43

呃，一个东西嘛，当年就是为了公安。

仇钧潼

28:46

为了给公安服务来做的，那公安有什么问题呢？就是它的那个数据库里面那个案件库里面呢，有很多材料啊，有身份证号，有姓名，还有这个案情资料。这个案情摘要呢，一般是一个一段短文本，就是可能100到几百字的一小段中文。

仇钧潼

29:06

呃，那这段中文呢，你如果要很快的检索这段中文里面的内容。

仇钧潼

29:12

啊，那这个你需要对这段中文先进行分词。然后你再用这个分完词以后，这个再见，全文检索缩影就有点像那个google啊，百度里面使用那种倒牌缩影差不多，那应该很多讲检索的书里面都会讲这个知识，然后你就可以把这个内容把它查出来了。

仇钧潼

29:31

那你的分词分的不对的话呢，那这个内容就查不出来了，那我这个很遗憾的，那就是这个公安这个这个案件库里面这个案件内容啊，有很多是属于地名人名。那这个地名是千奇百怪，什么都有，那这个分词呢，基本上都很难准确，那有很多大公司都进去做过，包括像这个华为啊，什么之类公司都进去做过，那最后都做的不是很满意，就是这个检索的按键，这个按键太多了。

仇钧潼

29:59

这个我们国家人口多，所以是一个犯罪大国，这个偷偷的说真的是一个犯罪大国，这个案件太多了，然后呢，这个检索了速度很慢，那特别是很多公安一起检索的时候，那它的速度基基本上这个慢了就无法忍受，不能使用。那后来呢，我们专门为它这个场景呢，做了一个东西就是。

时间之外

30:03

对对对。

仇钧潼

30:18

你不需要分词我，你只要输入你要搜索的内容，可能你可以甚至可以输入一个类似于正则表达式量的东西，就带通配服量的东西，我都可以在这个。

仇钧潼

30:30

最多是毫秒，这个级别可以把你给搜出来，就我们发明了一种比较独特的缩影，那这个缩影呢，主要是利用这个显卡来加快它的运算速度，就是说我不需要分词，不需要建全文检索缩影，我在单机的情况底下。

仇钧潼

30:46

我不装什么elasticsearch啊，这个ES啊，那种古古怪怪的，这个分布式的东西不装那个东西，我们就一台机。

仇钧潼

30:54

啊，就可以搞定这个搜索了啊，那这个不好意思，这个吹的时间有点长啊，一吹起来就激动了。

时间之外

30:56

我我大体了解他的，谢谢谢谢谢！

仇钧潼

31:02

好，谢谢谢谢啊！谢谢！

仇钧潼

31:05

好，看看有没有其他问题啊，诶，你好！

田志军

31:06

唉，就是我们现在用的那个呃知识图谱，呃面对的我们提供的那个saas的服务，每家的它可能会不一样。

田志军

31:18

所以这个知识图谱，它的这个权限的控制这一块有什么好的方案吗？

仇钧潼

31:24

呃，知识图谱呢就理论上应该是一个比较手工的活，就是，呃，一开始的时候提取一个粗糙的知识图谱呢，这个应该还是可以做的，就可以自动化了，但提取出来这个知识图谱呢肯定是缺了很多实体。缺了很多联系，那你后面呢要慢慢把它补全，那这个补全呢应该是一个呃，一个定期的或者不定期的一个维护性的工作，它是需要通过这个迭代来增强的，那后面这个很多时候呢都。

仇钧潼

31:54

需要人工干预？比如说用户反馈，就是这里面有一些，唉，这个这么简单的东西我都查不出来，比如说我查这个病人，他曾经开过什么处方，那你就查不出来。那那这个时候你根据这些别人的这种。投诉对，对你的这个弱点的投诉来对它进行一些增强啊，那这通通常都是要经过很多次迭代以后，它才会成为一个比较完善的支突破。那现在目前来说还没有一个产品呢。啊，包括那些很大的公司，它也没有能力能做到，我给你一堆文本，你就能自动提取这个知突破，那他们还做不到。

田志军

32:31

噢，是的，我的，我的意思就是说有可能我们给出的这种产品a的一家公司是是这个B家公司，它会有所不一样，然后我们想说能不能控制说，呃，像传统的关系数据会一样，然后来去做到那个字段了。

田志军

32:48

字段级别的，然后来去进行赋值或者关系级别来去进行负责，每家公司它可能不一样看东西。

仇钧潼

32:49

明白。明白，呃，那这个可能就分行业应用了，就是比如说我是针专门针对这个，呃，金融里面的这个基金行业的，或者说我专门针对医院，那这个医院呢还是卫生院级别的，那因为这种这种级别的这种企业的话呢，它有很多共性。

仇钧潼

33:10

啊，它的这个产品它的服务呢也比较有限啊，那所以呢，还是比较容易把这个知识图库把它迁移过去，嗯。

田志军

33:20

呃，那您的意思就是每一家都可以装到一个独立的知识图谱，然后来去给他们用，而不是通过权限的方式然后来去进行。

田志军

33:29

呃，控制，然后如果是通过权限的方式，这个现在有成熟的解决方案嘛，或者有有有解决办法嘛？

仇钧潼

33:35

对，没错，您说了对这个概念设计大家都是相同的，对吧，因为大家的那个结构差不多嘛，然后呢，我通过控制这个权限，就不同的企业访问这个图谱里面的某一个部分。然后呢，这样就可以达到这个快速的建立知识图谱这么一个目的了啊，那这个这个是很好的idea啊。

田志军

33:55

呃，有像这个有有这种权限的控制是？有相应的解决的这种方案嘛，或者工具嘛，如果说都用。

仇钧潼

34:02

呃，我想应该是有的，因为呃这个图数据库呢，也是在信创活动里面很活跃的一个领域。就是有很多家在出自己的图数据库，那作为商业的图数据库的话，他肯定会考虑到全线这一类的功能了，不大可能会不考虑。

田志军

34:23

诶，好的。

仇钧潼

34:24

好，谢谢。

仇钧潼

34:26

好，看看还有没有问题啊？

仇钧潼

34:29

诶，你好听到。

许涛

34:30

啊，然后我一开始可能有点地方可能没太听，就是说那个这个太基地是是咱们的一个产品是吧，不，不是一个产业园什么的东西是吧？

仇钧潼

34:38

啊，是我们的一个产品，就是我们提供的一个产品，像GBT差不多就是专门把那个垂直领域这块强化的拿出来提供的一个服务，它背后呢还是有基做模型的，比如说，呃，可能会用一些开源的模型啊，或者经过我们增强的一些模型啊。

仇钧潼

34:55

啊，或者就直接用这个GBT啊，那这些大家都可以自己来进行选择。

许涛

35:01

啊，我一开始以为他是一个产业园这个名字，然后那个，然后就是呃，刚才说的那些项链数据库什么的那些服务，包括现在这一页里边的那个知识图谱加LLL就是大语言模型这种。

仇钧潼

35:05

啊。

许涛

35:14

这一个是两两，它里边儿是都是它里边儿的东西是是两个它不同的一个。一个类似一个方向是吧？

仇钧潼

35:23

呃，我一下没听清楚，不好意思啊，行。

许涛

35:25

呃，就就是第一页和第二页这这两页PPT里边应该是描述的，这不是应该是一个是两种不同的太极地的。这个产品的不同的一个方向是吗？

仇钧潼

35:36

第不是，它都是同同一张纸，因为内容太多，我才分成了两页。呃，前前一个呢是标准的服务，就是说这个这个是你。不需要人工介入的，你只要这个在我们这个系统里面获得一个呃，使用的账号，有有权利来使用里面的资源，你就可以自己自助式的进行完成一个垂直领域的机器人。而后面这个呢啊，就是需要有一些人工介入的，那这个是项目式的。就说这个我们可能要进行一些交流，你把你的诉求告诉我，那我就可以给你这个呃，计算一个人天，那我们按这个这个人天来进行收费啊，是这样的，那后面这个因为要需要用到人。能力，所以这个就没有办法给大家免费了，但是也可以给大家一个很高的折扣啊，那这个是我能承诺的。

仇钧潼

36:24

啊，那大概就这样子。

许涛

36:26

行，好明白，明白这个台基地现在就是不是已经可以用了是吗？

仇钧潼

36:30

呃，应该过一过一些时间，他们现在在最做最后的测试，估计应该不超过一个月就可以上，可以上线使用啊，几个星期内吧，或者甚至可能一两个星期内的啊。

许涛

36:39

到时候可以试用试用一个，比如说在那个盯孩子在哪？

仇钧潼

36:45

呃可以不不光可以试用，你可以正式使用就行了，反正我也不收钱<笑声>好的，谢谢好，谢谢！

许涛

36:50

啊，好好行。夏老师。

仇钧潼

36:54

啊，大家看看有没有问题啊？

henryho

36:56

诶黄老师。

仇钧潼

36:58

诶，你好诶。

henryho

36:59

唉，你好，呃，其实我有两个问题啊，其实我们这就是呃，人工智能一直在说相应的数据库嘛，所以我接触上的数据库是从那个PPT出来，就是做那个搜索，然后才了解这个相当数据库，但但是我听下来其实。

henryho

37:16

呃，我们不是做那个关键词搜索，其实不是ES也能解决嘛，关系型数据库其实也能解决，其实三大数据库我们为什么要选相关数据库呢？因为它的数据库搜索也不一定准嘛，刚刚说的。

仇钧潼

37:22

对，就是向量数据库呢，它是肯定是先做了，对吧？那你不做，那你这个向量从哪里来的？那这个以后呢，就可以用这个语义，就是说这个语义越接近的东西，它那个向量的那个夹角余弦的话距离越短。

henryho

37:34

嗯嗯。

仇钧潼

37:45

那我们的主要的数学基础，它的数学依据呢？是这个。那当然你要计算两个东西的相似性，那除了算这个的夹角余弦以外，还有很多其他的方法了，比如说以这个最大这个字，这个呃，相同尺寸的长度啊。

仇钧潼

38:02

诸如此类的这个关键字匹配啊，还有很多其他方法的，那这个所以也不等于说你不能用那些方法。当然那些方法的话呢，这个找起来也是很慢的，你你想想逐行都匹配一下的话呢，这个也是很花时间啊，但是如果你的数据不多，比如说你只有几百行，再花时间，也就是零点零零几秒的事情啊，那这个还是算快的。

仇钧潼

38:25

啊，谢谢。

henryho

38:26

对，还有还有另外一个问题啊，其实我看到就是林先生在提供那个服务啊，所以我觉得挺好的，就是可以做一个原型给，呃，集团是用完，然后就是可以看这个效果，但是只有我们企业可能比较特殊，要稳定化部署，那后面如果要怎么办呢？这个事情。

仇钧潼

38:29

对，没错。

仇钧潼

38:43

就是说你你的那个数据很机密是吧，就不能报部署到我这来，要部署到您公司那里去是吧？

仇钧潼

38:51

啊，那就这样的话呢，那可以谈一个这个定制项目的，就是说我，我把服务器或者说我，我派人到您企业里面去驻场。然后呢，帮你们完成这个事情，那大概是这样子啊，那这个只能够收费了，因为产生差旅啊，这个人天的开支，那这个就不能免费了啊，谢谢啊！

henryho

39:00

是吧。啊，明白。明白明白，这个也是好的服务，因为企业比较特殊，有一些东西好，第三个问题就是我一直在说那个驱动语上面，我这在说驱动云，就我是我我刚找出那个问题就不能提供外部接口，他们说要签那个安全协议，那个能推进一下吗？

仇钧潼

39:11

啊，谢谢。是吧。

henryho

39:25

那个驱动员我觉得是满足的本身是，但是就是没有那个就没有web页面了，用了就。

仇钧潼

39:26

好的，因为我我跟他们的管理层面都很熟，我会尽快的促进这个事情，他们他们生意的模式上呢，确实有一些比较青涩的地方，因为他这个公司这刚都都是刚建的，也没没多长时间，所以这个在服务上呢，可能只是服务。比较少的那种大客户来买他们的算力，这种公开性的这个对于这个社会开放呢，应该也不是很大量啊，那这个所以这个也借这个机会把他们的商业模式，还有这个服务模式锤炼一下吧，我会促进这个事情。好的，谢谢！

henryho

39:50

啊，谢谢黄老师！

仇钧潼

40:02

好看，还有没有问题啊？还有问题吗？

王蓓

40:18

目前线上没有新的提问。

仇钧潼

40:20

行，那我们就讲课了啊！啊，那昨天呢，我们曾经讲过这个QKV那回去以后呢，我又好好琢磨了一下啊，终于明白了这个QKV怎么给大家解释了<笑声>那大家可以听一下，能听懂不啊？那昨天我们说到呢，在这个transformer里面呢，它有一个所谓自注意机制。那其实transform的核心就是自注意力机制，对吧？那这个卷积神经网络的核心呢，就是卷积的计算，我们有个卷积和巴巴拉，这个给每个来算一下卷机来提取一些特征。那这个自注意力的机制是什么回事呢？那这个是每一个学transform的人。

仇钧潼

40:58

到了这个地方，都是感觉到非常的困惑，那我昨天呢就用这个图给大家解释了。就是说，QKV的原意到底是什么呢？那从他的名字其实你就能看出来。他一开始的时候呢，是为那个问答数据库问答机器人服务的。啊，就是说我们有一些或者叫。啊，然后呢，我也准备好了一些回答的素材。那这个回答的素材呢？有两个，一个是key，一个是value，那你也可以把这个key呢理解为是一种标准化的问题。或者说我把这个呃，你问的这个问题里面一些比较关键的词汇把它提出来的形成这个key啊。然后value的话呢，大概大概率就是一段txt的文本，就是一大段的答案。

仇钧潼

41:50

那我现在要做的事情呢，就是根据用户提出来的query看看跟哪些key最匹配，最后再决定怎么用这个value去回答它。那所有的这一切全部都是。都不是具体的文字啊，那我们这里写了五512嘛是吧，这里五百一十二那就表示他已经被。那我的我现在的那个在底下呢，就变成我收到一个问题的。然后呢，我根据我的K。这个向量的数据库里面的材料来找出一堆value。适合回答这个问题的，那我再把这些value组合一下加起来。那就变成一个语音向量，那这个语音向量呢，就适合回答这个query啊，去回答这个问题。那其实我做的就这个事情啊！

仇钧潼

42:45

那这里的话呢？这个我们算这个？是什么意思呢？那其实不就是把这个Q里面的每一行。跟这个K里面的每一行计算一个类。那内积其实跟夹角余弦是成正比的，就是你。除去了那个呃，这个向量的长度以外，其实就是夹角余弦，那你可以把这个类级计算呢理解为某种相似度的计算。那所以呢，我们一下就理解了。

仇钧潼

43:18

这里的qkt呢，其实就是一个相似度计算。那就是求N个问题，跟a MKV对之间那个key之间的那个相似度。里面的每一个元素，比如说DI行D这一类的元素啊，就是它的这个相关度大概有多高啊，那其实说明的就是这个事情。然后呢，我得到这个相关度以后呢？我再用它啊，这个是刚才得到了相关度。在用它去乘上。这个value这个矩阵啊，那这个是答案矩阵。那相当于什么意思？那第一第一行这里这个相关度矩阵里面的第一行对应的，这是这边的第一条问题啊。第二行呢，对应的是这边的第二条问题。那我看第一条问题，我应该怎么回答呢？那我就用这一行。乘上这边所有的行。怎么乘呢？比如说它这里有很多。不同的这个位置，每个位置都上面都有一个数字。那你可以把这些数字在以后。它就变成一个全职一样的东西了，变成一个全职。那就相当于是啊，我这个乘上第一行。用这个全成第二行用这个全成第三行，用这个全成第四行，一直这个这个把这个事情做完，把这些。得到的结果全部加起来，就相当于对这个value做了一个加权的一个加法。

仇钧潼

44:47

那最后我们就得出一个综合性的答案。那你相关度越高的那个key那我对你这个value也就越重视，我采用你的成分就越多。那相关度越低的。key那我采用这个value呢，它的这个全值也就越低，那我采用的这个东西就越少，那最后全部加起来以后呢，我们就得到一个语义向量。那这个语义向量呢，就是我们的。这个答案21那这个是回答Q一的。那至于这个怎么把一个embedding变成一堆文字呢？那这个就是解码器的事情呢？啊，那我们有很多工具可以做到这个事情。那这个qkv它本来的这个意思是这样子。

仇钧潼

45:28

那我们再看一看这个自助力底下的QKV是什么意思呢？那大家注意一下呢，这里的QKV只是借用了刚才那个问答机器人。这个场景里面的QKV这个术语，但实际上呢，跟他这个意义已经差了10万8000里了，他已经不是做问答机器人这个事情了。

仇钧潼

45:49

那首先在刚才那里，我们是对句子做吧，问题是句子。这个key是句子或者至少是很多个词。然后这个V呢也是一段文字，它都不是单个词啊，都不是单个词。但是呢，在这个transforma里面呢，这个词线这个这个每一行这个X矩阵。我们不是讲有一个X局的矩阵吗？叫做I have a cat？每一行呢，都是一个词的。那所以这个在这个角度来看是不一样的。

仇钧潼

46:25

好，那我现在这个QKV想达到一个什么目的呢？那就是。下面这张图可以说明这个事情啊，比如说啊，现在我这句话是老王告诉张三他准备去北京。那我已经把它分好吃，每一行分别是一个的向量。那现在的问题是我这个？

仇钧潼

46:47

Embedding的向量呢，可能是用一个比较粗糙的的方法来算了。呃，因为它也没有办法不粗糙呢，因为你现在的信息很少了，没办法不粗糙，那你顶多可能用一个通用的工具。用一个什么之类的工具，就把这个词变成向量。那这个what肯定也不知道你这个句子里面这些词的。这个先后的次序是什么你肯定也不知道。

仇钧潼

47:11

啊，那所以它只能够相同的词它都会给你算同一个。

仇钧潼

47:16

那举个例子，比如说这里的。那这个他到底是老王还是张三呢？那你你用这个来做的话，他谁知道你是老王还是张三，他不可能知道的他他因为他根本是把每个词割裂出来来对待的。所以你无论什么句子里面的。你用来做的话呢，一定只是同一个向量。那这样的话，这个语义就比较差了，就得出来，这个语义就不准确了。那很多时候这个自然语言处理用这个，这种方法来做这个处理的结果不够好了，其实主要的原因就来自于这些干扰。那一个呢是昨天所说的。有两个词，它文字上是一模一样的，但一个是动词，一个是名词，完全不是一回事。那比如说那个fire FIRE是吧，那你既有着火啦，这个这个这个火啊，这个意思也有解雇的意思。那你你你说你让他的时候，你你让这个怎么决定？那这个必须要根据上下文，但是这个本身并没有这个能力啊。那所以所以的话呢，这个我们还需要做更多的事情，那这个transformer你实现的是什么功能呢？这个这个部分实现的功能就是。它使到这个更加精准了。就更加准确的能体现出它的意义。

仇钧潼

48:34

那这个更加准确，是体现在什么方法上呢？那就是自注意力，那所谓自注意的意思就是。当我做一个东西的，比如说做他的时候呢？我要计算一下我这一行。跟其他行的。这个相关程度是怎么样？就是我要根据其他的词，我前后出现的这些词的那个相关性来判断我这个词应该怎么去。那他做的是这么一个比较精密的事情。那所以这个叫做自注意力的原因啊，就在于这里。好，那这个事情怎么做呢？那一个最简单的方法就想到，唉，我自己跟自己做个类级的，不就完事了，那我要计算这个X。乘以那不就完事了？那不就完事了，那那那不就是每每个我我这个跟另外一个embedding做一个夹角鱼钱。啊，那我就能够判断出他们的相关性。但是你要记住呢，你这个相关性的基础是什么呢？是what的结果是这个一个比较粗糙的in的结果。你这个相似性如果是建立在。的结果之上的话呢？那这个结果肯定也是很粗糙的，因为你本来就是一个不是那么靠谱的一个的方法啊，那你你连他的一些位置啊，这些也没有怎么考虑到。

仇钧潼

50:05

那这个我举个例子吧，比如说那个。那它如果是着火的话，它应该是出现在。这个就如果说是名词的火的话呢，它应该出现在主语或者宾语这个位置上面，对吧。但是如果它是far，就是那个这个解雇啊，那个动词。那它应该是出现在这个谓语这个地方，那你出现的位置不一样的话，在句子里面位置不一样的话呢，那决定了你的是不一样的。那所以呢，我们现在第一个事情要解决的就是这个X本身就是不准确的，你在不准确的基础上，你怎么算这个相关性呢？那这个你算出来肯定也是不准确的。好，那这里有了，这里来了。既然不准确，我就对他进行一下调整。就我认为它可能会有一种更高明的方法，我把这个X呢，把它变成一个更高明的空间里面的。啊，在这里面这个点是更准确的。好，那这个怎么变呢？下面呢，可以看到这个魔术就来了啊！

仇钧潼

51:06

这个磨出来？我们把这个X。乘上一个调整矩阵wq。那这个调整矩阵相当于什么意思呢？那大家看一看这个结果是怎么生成啊？那不就是。这里面的行，这里面的这个结果里面的每一行是什么呢？那不就是哎。那不就是这里的第一行？呃，乘上这里的第一列。那这个是第一个元素是吧，然后也是这一行乘上这里的第二列。那这个是第二个元素？然后这里也是这一行乘上这里的第三列，那这个是第三个元素。那大家可以看到呢，这个结果里面这一行都只是跟这边这一行有关系。那你可以考虑为它是对这个这一行它是一个词的，对吧？

仇钧潼

52:06

对这个行的各个分量进行了一个调整。那这个这个的每个分量是代表一些什么意思呢？它代表的是隐语义，就说这这个东西我说不出来它是什么意思？那他可能这个第一个，第一个维度表示一种高兴不高兴的情绪啊。第二个维度呢，可能表达了一种是不是一个猛烈的动作啊。第三个维度呢，可能表达的是其他东西，那这个这个这个是一个引语义，那它实际上这个这两个矩阵相乘是做什么呢？就是对每一行的这个里面。这些隐语义做了一定的调整。做了一定的调整，做了一定的微调。那实际上就是把你原来用的那个。重新给你又又做了一遍，一个别的一个就是说把你的改成别的。

仇钧潼

52:54

那这里有一个问题来了，就是那你你这个WQ听起来好好猛啊，好厉害啊，但是问题是你怎么知道怎么调整呢？你你能告诉我你怎么调吗？你我我塞给你一个场景，你就知道这个Q是啥吗？那这里最高高明的办法就是我们不要定这个WQ是什么，我们让他自己去训练。

仇钧潼

53:15

我让他自己去训练。他自己训练出来的最好的就我给一个任务，比如说翻译啊，这个创始方法，做翻译的任务。那我这个WQ呢，就是个针对翻译这个任务，根据我翻译的语料，他自己训练出来的，那他就一定是。对这个X在翻译这个任务底下的一个最好的方法，那当然它要跟这个结合这一块它可以看成是对的一个调整。一个调整，那它这个调整呢，就是针对这个场景呢是最佳的。

仇钧潼

53:47

那所以为什么这个我们要把这个X？啊，要乘上一个W？最后呢变成这个Q矩阵，那也就是这个原因。好，同样的，那我现在要跟这个X。不是自自注意力吗？要自己来比较嘛，对吧？那我比较那一方，他也是用water的，那这个也是不准的。那我也要对它进行调整。那这个这个调整是不是跟这个这跟这个Q矩阵是一致的呢？那这个不一定啊。因为我们有时候前面这个是。呃，刚才在那个传统的那个问答机器人里面不是有一些时候是问题，有一些时候可能是这个P嘛，对吧。那这个问题跟P其实是有一点差别的，所以我们这里也不强求他。你这个比较的对象它也同样用W。那如果他需要用同样的都是wq的话，他自己训练的时候会训练出来了，我不用管这个事情。我就假设他们可能这个调整的方法有点不一样，就是我比较的那个对象，他所在的那个嵌入空间的那个结构跟我现在这个结构可能有点不一样。所以呢，我在这里就把它乘上另外一个矩阵WK，那它就变成了K。

仇钧潼

55:06

好，那最后呢，我们要算的就是。再除以根号再来做一个。最后要算了这个，那这一个呢，其实本质上的意思就是我把这个呃原先的每一个词都跟其他的词。啊，我把这里吧，把每一个词，比如说它跟其他的每一个词我们都算了一下这个相关性。但是呢，在算这个相关性的时候，我们使用的线路是不一样的。这两个词使用的线路都不一样。

仇钧潼

55:48

然后这个嵌入？是怎么具体的嵌入，这个是训练出来的，不是我管他训练出来的啊，好，那最后呢，你这个相关。相关程度训训练出来了啊，这个相关性这个做出来了。这个东西是我们算出来的。那我们现在就要做最后的总结了，那最后的总结是什么呢？那不就是把这个结果在这个V矩阵里面？把它那些向量重新组合起来嘛，就是说这自注意的意思就是说，我现在算出了一套全职了，你这一行怎么了？跟这里每一行。都有一个全值了啊，比如说第一行的全就是0.1第二行0.2啊，第三个0.3就0.4。那我就把这堆全部加全加起来，那就是我这一行需要的。embedding这个就是我这一行需要的。

仇钧潼

56:50

但这里又有一个问题，那在这个V这里呢，这个V本身是不是要用原先的那个X矩阵呢？我们觉得也不一定要用X矩阵。那我可以把这个X。乘上一个。这个X乘上一个WV以后呢？把它变成。这个微矩阵。那这个也是另外一个人，为什么我们要搞了三套这个人呢？那原因就是他们确实需要那么多，因为他们可能在不同的这个嵌入空间里面啊，去达到这个效果。好，那这个就是QKV的解释，那大家看看有没有听明白啊？我说实话呢，我找过为了讲明白这个QKV，因为我我讲的这个也不是第一次的在。在这个念经里面。之前的课程里面也讲过，然后在这个学校里面也讲过，我看过很多的参考书啊，就讲这个创的这个论文啊，参考书一大堆的。那一讲到自注意力的时候呢，就他很生硬的就搬出来。QKV一堆东西啊，你就这么算就行了，他也不解释是什么道理，那结果弄了你一头雾水的，就对这个事情理解的不深刻啊，那我们就花了一些时间呢，这个把这个东西深刻的剖析一下啊，那希望对于大家来说呢。是有帮助的啊！

仇钧潼

58:16

好，那大家看一看这个。有没有什么疑问啊，这个又又可以再提问。几分钟时间？这可以把这个题板打开啊！

田志军

58:43

哎，就是您最后算出来那个那个呃那个那个V的那个值。

仇钧潼

58:47

对，听到。

田志军

58:49

唉，那个威廉那个值它肯定会有一系列的这个。

田志军

58:54

呃，数值和权重，那他这个数值和权重。你要选择的时候是按什么策？有选择的策略吗？虽然是最大的那个值，然后来去进行选择，还是说在一在一个范围之内，我我随机选的一个都可以啊。

仇钧潼

59:11

呃，它不是，这里是一个soft message矩阵嘛，对吧？那它其实上把V里面每一行都选上了，它只是这个全不一样。他这个权是前面这个结果提供的。就每一行都会有这里QKT以后这个这个跟这个下面这两个相乘以后。就是一个N乘M矩阵嘛，对吧？就一个NN乘M矩阵，然后呢，它会根据这里面的全来选择这个V里面的这些值，就每一行都给它做一个加权，每一行都会选上，最后全部加起来。来得到这个最后的结果，就不是像向量数据库那样子，我就挑这个最相似的三行啊，或者这个挑这个最高全这个这个这个这个相似度的这个多多少行啊，它不是这个概念。它就是一个注意力。注意力就会加！加权计算这个总和。

田志军

01:00:02

哦，那我看我理解对不对，也就是说现在算这种这种蜘蛛意的算出来的方法，它不是一个呃，一个一个一个序列，它就是一个值了嘛。

仇钧潼

01:00:13

对，就是自注意力的结果呢，就是对这个X更加精细的一个。对这个X更加惊喜的明白。啊，那当然这个关键就是QKV这一块，那它后面还有一些什么残差啊，还有一些什么成规异化的这个什么多头啊，那后面还是还有的，还有一些结构的啊，但是关键就是QKQKV这块了啊。

田志军

01:00:38

嗯ok好。

仇钧潼

01:00:41

刚才什么大爷你好啊！

许涛

01:00:41

黄老师。就是这个。嗯，这这一页里边这个X应该是。经过那个未知编码后的那个矩阵是吧？

仇钧潼

01:00:50

X是我们输入的那个矩阵，就是它用比较简单的，比如说像这样做，然后再加上位置的以后所形成的一个矩阵。

许涛

01:01:01

就是经过X就经过位置编码以后的一个五幺二，五百一十二维的一个。

仇钧潼

01:01:04

对，没错，就是这个加号以后的这个地方。

仇钧潼

01:01:12

这个这个地方。

许涛

01:01:12

那把这种方式如果就咱不放在这个，就是除了在全方面里边，用在之前的一些模型里面，算法里边，是不是它也会效果也会更好一些？

仇钧潼

01:01:19

呃，对，这个QKV呢，肯定是在发明以前就已经被广为流传。呃，有一篇原始论文呢，我我这里一下没有把它摘下来，比较老了，好像是20。16年还是什么时候的啊？我稍后可以找一下发给大家。就关于这个自注意力的论文啊，这个早期有一篇。就其实这个讲解这个一个比较麻烦的东西，一个比较深刻的东西呢，呃，这个我觉得还是蛮有意思的，我记得很久很久以前，我那时候可能还是叫年轻人嘛，就别人叫我小黄的时候，现在都叫老黄了，<笑声>，那时候我是刚开始讲这个机器学习。那我我讲那个支持向量机，我最喜欢讲这个东西了。这支持向量机的这个有有有有有些人搞来搞去就没搞明白这个支持向量机到底是什么，就一开始那一段呢，还是很容易明白的，讲到后面讲到那个核方法，这个核空间这一块他就不好明白了。就有，有企业找我说黄老师我给你多少万块钱了，你来我这里讲一下就讲一样东西就行了，就支持下量级。

仇钧潼

01:02:27

呃，后来我就发现，哎呀，原来这个讲一个很难的东西呢，这个还是有点意思的，还是还是能够成为一个商品化能挣钱的东西啊，那所以呢，后来每一次讲课我都会特别留意这些，有一些比较难的点，就大家不好理解，那我就专门挑出来给大家讲。那当然不是说要收大家多少万多少万钱，收到多少万块钱的意思，我只是觉得这个这个事情呢，大家可能比较关心比较呃，这个呃比较注意到。好，谢谢你。

仇钧潼

01:03:00

啊，当然大家还有没有问题啊？

时间之外

01:03:02

哎哟，黄老师！

田志军

01:03:02

哎，韩老师。

仇钧潼

01:03:03

诶，你好诶。

时间之外

01:03:04

嗯嗯，我我我其实没大听懂啊，我想知道这KTV是不是对向量数据进行一个加强啊，这个意思是吧？

仇钧潼

01:03:13

对，就是说我，我们本来应该算那个X这个。

仇钧潼

01:03:18

呃，我找一刚才那张图啊！啊，这是这个，我们本来你要算这个自注意力，那不就是这个，呃你你这里已经做好了一个了嘛，对吧？

仇钧潼

01:03:31

啊，那我我要算的呢，诶，这个怎么又回不去了？我本来要算的不就是X跟X的类级嘛，那应该是X乘以X的转置才对嘛，对吧。那为什么我要搞出QKV那么多东西呢？那原因是你要算这个相似度相不相似，前提是你的应该是要准的才行，对吧？

仇钧潼

01:03:57

那如果你赢得不准的话，那你算这个相似度，其实意义也就不大了。

时间之外

01:03:57

对啊。

仇钧潼

01:04:02

那我们是很难认为这个你一开始做这么粗糙的，它是准的，我们不认为它是准的，我认为它需要经过一些微调，它才能够去做这个相似度计算这件事情。

仇钧潼

01:04:16

那所以这里后面这个X乘上WQ以后成为Q啊，那这个是第一个微调。

仇钧潼

01:04:22

那么我们也不认为这个向量。

仇钧潼

01:04:25

本身就只做一种微调就够了，我们认为要他要做好几种微调，根据这个场景来看的话呢，他要做好几种微调才行。

时间之外

01:04:32

现在有的好吧啊！

仇钧潼

01:04:34

啊，那然后呢，我们就于是就有了Q跟K以及V三个东西。

仇钧潼

01:04:40

啊，那是这么一个概念啊！

时间之外

01:04:42

啊，最后咱进行增强之后的话，这个呃就会耗费的资源，也就是说后期占的，比如说内存或者GPU啊，或者磁盘它会也也会相应的增加是吧？

仇钧潼

01:04:56

不好意思，你那边有点噪音，我一下没清楚，听清楚您的问题。

时间之外

01:05:00

就比如说。咱们呃，对于向量数据进行增强之后，他其实是会耗费更多的资源。

仇钧潼

01:05:04

哦，那他如果要做大量计算的话，那肯定是很消耗这个计算机。

仇钧潼

01:05:12

<笑声>这个向量数据库是个好东西，但是说实话，有一些相似性的这种算法呢，我都想不出它可以怎么才能才能加快，我觉得它本来就没有什么办法可以加快。

仇钧潼

01:05:24

啊，那所以他有时候可能也会硬扛。

时间之外

01:05:26

咱们。

仇钧潼

01:05:28

我已经见到有有那个。

仇钧潼

01:05:30

我我我因为经常看那个这种做机器人啊，这个这种。视频我一上油管的时候就有人推了一个广告给我，那个广告呢，就卖一种叫做向量数据库一体机。

仇钧潼

01:05:43

<笑声>它就是一台很强的电脑，那可能是什么16个CPU啊，什么之类的，然后里面呢，这个就装了一个向量库数据库的软件，它什么都把它装好了，这一台机那就说，唉呀，这个东西就可以拿来做现在这种。

仇钧潼

01:05:58

呃，这个垂直领域机器人这种事情啊，那这个大概可能要1万多块美金了，看了一下那个价格。

时间之外

01:06:05

那咱们就这种后期有相应的，比如说代码或者实现嘛，这种会。

仇钧潼

01:06:14

啊，那个穿的实现呢？这个是早就已经做好了这个。何老师在前？

仇钧潼

01:06:20

可能在第一次课的那个下午的那次课呢，不是给了大家讲了那个穿嘛，就后面有个S那个那个其实就是上面已经实现的。这个transform的代码，那它里面自然就包括了这个QKB了，因为它是transform的核心。

时间之外

01:06:37

啊，已经包含了是吧啊，其他的。

仇钧潼

01:06:38

好的，谢谢。啊，还有问题吗？哎，你好！

许涛

01:06:43

方老师。

田志军

01:06:44

哎，黄老师。

田志军

01:06:45

还有我我还有一个小问题，就是呃，之前呢这个呃做类似的这个系统的时候，它有分，有一个部分就是叫呃推理机嘛，这是这一部分是负责规则和逻辑推理的。

田志军

01:06:58

然后如果说我们现在用这种方式来去做那个呃向量向量库，它可是数据库嘛，那负责推理这一部分是放到哪个地方，然后来去进行完成。

仇钧潼

01:06:59

像数据库本身，它只能够起到一个查询的作用，它大概它自己是不能够提供推理的能力的，那推理的话无非就是说你根据这一条向量找另外一条向量，再找去另外一条向量，它应该没有这个能力。

仇钧潼

01:07:24

啊，所以的这个能力只能指望说你把这些语料。这些材料找出来以后呢，指望后面那个那个大圆模型能够帮你做这个事情。

田志军

01:07:34

呃，这个呃，大圆模型这一块就是呃，除了提供的一个开源这个能力，然后来去做的话，就是如果我们做作为一个独立的公司的话，有有有能力去开发这一块嘛，或者说它有暴露这这一块的能力是在哪一块，我们来去进行调嘛。

仇钧潼

01:07:53

就是说你想对这个语言大模型进行翻寸做，就是说呢，是那种改变权重的那种调整，就全局改变权重的那种调整。是不是一个意思？

田志军

01:08:04

呃，也不是完全这个意思，我的意思就是说推理这一块的这个能力，想说在大学模型它是哪一块是负责，它是怎么实现机构。

仇钧潼

01:08:10

这个都说不清楚，就是说我，我认为open AI的人现在都没想清楚为什么这个GPD四啊，这些它的推理能力能够达到这么强。

仇钧潼

01:08:22

呃，好像它所用的技术啊，这种语言模型本身也并不是有什么推理能力啊，对吧？啊，那我觉得这个以后可能有很多研究，为什么它这个GPT四能够推理？这个目前是一个解释不清楚的一个事情啊！

田志军

01:08:37

嗯，好的，黄老师！

许涛

01:08:41

22我那个。

仇钧潼

01:08:42

哎，你好。

许涛

01:08:43

呃，就是你刚才说的这个圈里边这个X矩阵，如果直接和它这个转置相乘的话，不是说这种方式，首先因为咱们分成三种，可能有一个原石，它比较粗糙嘛，这种方式是吧，直接和它相乘。嗯，然后我我想。呃，说一下就我个人对这个东西可能有另外的一些理解，我不知道是不是对的。

仇钧潼

01:09:02

啊，好的，请说。

许涛

01:09:03

就直接和他这个转制相乘就是。呃，你比如说他如果直接和他转相乘的话。那我觉得比如说这个老王告诉张三是，就就这这几个项，这这几个这几个行，那应该是他自己和自己是对相关的吧，是吧？所以说他肯定还是从这个角度上理解，也是比较粗糙的。

仇钧潼

01:09:17

嗯，对，就是说，呃你你自己跟自己相乘的话呢，它里面的每一个元素就每一个语义。

仇钧潼

01:09:28

其实都已经切割好了，对吧，但实际上呢，你你把这个语义跟另外一另外一个向量的这个对应的语义算相似度。呃，可能不一定合适，你可能把这些语义重新组合一下，以后我们再算相似度，那可能会更合适，那其实也也有点这个意思。哈哈啊。

许涛

01:09:46

因为你想就是我就说一下我对这个粗糙的理解，为什么粗糙嘛，他肯定是你想他算这个夹角鱼弦的时候，他肯定和自己那个是不是最相关的吧，是吧？

仇钧潼

01:09:55

我一下没有没有能够理解出你的说的意思。回家没有理解心。

许涛

01:10:06

就是就是它和它的转制相乘的话，那它其中有一个，比如说第一行嘛，第一行。

许涛

01:10:11

呃，他和他那个转世，他肯定是首先这个老王和老王他这这两个项链，他肯定是有一个算家教育权的过程，他自己和自己算了，肯定有这么一个过程吧。

许涛

01:10:22

然后他得到一个元素就是。就是。

仇钧潼

01:10:24

行，我们可以可以私下找另外一个时间在微信里面再讨论这个事情，我一向没没有理解上，不好意思，我一向没有理解上。

仇钧潼

01:10:35

大家看还有没有什么问题啊？

王文璞

01:10:37

呃，黄老师他这个就是QKQVWKWQ它这个是一个，就是它训练好之后是所有的像。

仇钧潼

01:10:38

哎，你好。

王文璞

01:10:47

就所有的这种语句对它都是同一个一个全职吗？

王文璞

01:10:54

就比如就我的意思是说。

仇钧潼

01:10:55

对呀，你在推断过程里面，它是不会改变这个WQWK的，就是你在训练的时候，当然每每一个语料都可能对它进行这个改变。你你训练好了，它就不改变了。

王文璞

01:11:08

但是它这样的话就意味着说，比如一个句子，它可能陈述句和疑问句，它本身语序和内容就是不一样的，这样它如何去把它变成了一个是统一的，都可以使得它的权重有显性增加的一个效果呢？

仇钧潼

01:11:25

啊，那这个就是WQWK的那个效果？就是他可以。呃，怎么说呢？就是它这个WQWKWV里面这些全值，它可以自己把这种规律把它记录下来。也就是说，包括你刚才说的这个句型不同啊，这些规律都可以记录在这些权重里面。那为什么这个权重这么多啊？原因就是这个规则确实也太复杂了。我不知道这个回答让你满意了不？

王文璞

01:12:00

因为我我我在在直观的理解中，它应该是不同的，就甚至是不同的句子，它可能就意思表达就会不一样嘛。然后他应该就有不一样的权重去去给他去修正。如果说他希望精细化的话。

仇钧潼

01:12:01

就是它是一种自学习的概念。它不同的句子。

王文璞

01:12:16

啊，现在你告诉我说是一个。

仇钧潼

01:12:16

对他。

王文璞

01:12:18

统一的全职就可以把它做到精细化。

王文璞

01:12:21

那其实我的理解，那那如果说大家统一的都这样乘一下就就精细化了。

王文璞

01:12:28

那跟他不成的时候其实也没有。本质的区别，因为如果说你后面做了转制的相乘，这些向量都被抵消掉了呀！

仇钧潼

01:12:31

他是这样的，就是怎么说呢，这个问题呃。它，它会把某一种句型映射到空间里面的某个位置，就不同的句型可能映射到空间里面不同的位置。呃，这个我觉得不用替他操心这个事情。它，它是自己学起来的，它是根据你这些词的这种不同的这个顺序，不同的位置啊，它自己会决定你这个是什么句型，应该嵌入到什么地方。那这个他自己会算这个事情。

王文璞

01:13:11

那现在也就只能硬性的这样理解了呗。

仇钧潼

01:13:13

对，没错啊，这个就这个，因为神经网络它本身就是一个。半黑的这个灰箱嘛，对吧，所以你要绝对解释的清楚它的细节呢？这个也是很难做到，但是它从效果来上呢，确实它达到了这个。把这个嵌入做得更加精细，这么一个目的啊！就是你，你会根据这个词，在这个句子里面的位置，以及它跟其他词之间的这种相对的位置来修改它的，来使它的这个更加。更加精精准一些啊！

仇钧潼

01:13:50

啊，那大家没有什么问题的话呢，我们先休息十分钟啊！

仇钧潼

01:14:29

作用就是因为翻译可能。中国人主要是说很多人他会反过来。但是我管你的时候。输出我们的词的时候。可能他不是对应的，这个我就我有时候啊放到后面，但是中文。说它中间要有一个。证据那个。这个就带了一个。他是这样解释的，当初怎么？关系到了。上去就这个想法。我就觉得他明白不了我，我理解。就是，本来就是像神经网络。

仇钧潼

01:24:08

喂喂，大家听到吗？好行，那我们继续吧啊，刚才呢，我们讲了这个。这个又不动了。好，那刚才呢，我们这个解释的QKV这个事情，那我们也消耗了比较多的时间，那在这个课间休息的时候呢，我又想到了一个。这个解释的方法。我一下没找到那一页，就说我们为什么不直接算那个？乘以X的转。而是要把它先变成QKV呢。那大家可以想象，如果你算X和N的转折，就是互相做这个类级的话，那一定就是同一个词，它的类级就肯定是最大的，对吧？就它的相似度最高的，因为它本来就是同一个词。然后呢，其他相似度更高了，那就应该是同义词。那所以这个注意力呢，都会集中在这个同义词身上。那这个显然不是我们的原意，那我们的原意呢？应该是我需要注意到前面是动词啊啊，后面是不是名词啊？我在句子里面哪个位置啊，我需要关心的这些信息。那所以呢，我们需要的并不是在这个意义底下的这个句，这个这个这个这个词的。不是这个，不，不是我们需要的，不是我们需要的，是另外一些东西，比如说我们更关心前面的位置是什么什么什么词性啊，我处于这个句子里面，是主语的位置还是宾语的位置，我可能更关心这些。那所以呢，我们这个。要对这个整个做一个大的调整，那这个也就是为什么它要乘上QKV。

仇钧潼

01:26:00

这个WQWKWV这三个矩阵变成QKV这三个奇葩的东西的一个主要的原因啊。那这也是一个解释了，那希望这个解释呢可以。啊，让大家更好了，明白一些啊！

仇钧潼

01:26:14

那下面呢，我们继续昨天的那个话题，昨天我们讲到大语言模型，那我们讲了这个还有讲了GPT11跟GPT二。那这个我们今天啊还有G？那我们今天呢会讲这个3.5来提提四，那3.5跟四呢就没啥好讲的。就因为他都没有一个公开的技术资料。他出来的全部都是这个技术说明书就表达自己啊，经过各种测试以后怎么牛逼啊？唉呀，这个各种各种流啊，就反正我就就催我怎么厉害，但是这个具体的这个实现细节呢，他一概都不讲，而且有一个很讨厌的地方呢。

仇钧潼

01:26:54

这个open ai工程师写的论文呢，大概就跟那个小学生作文差不多。这个这个水平就公认的在行业里面公认的open AI写的论文，他写的是很差的，这个很差，体现在哪里呢？他经常遗漏一些细节。都不知道是有意的还是无意的，但我猜想他有一些论文呢，其实好像也没必要这个保守什么技术秘密。那这个我我只能够说他的这个写作水平可能本来就那么糟糕。那这个你去跟这个google啊，这个其他这个微软啊什么这个微软研究院的这些。写了论文一比较的话呢，就他这个论文说，说实话，就跟那个小学生水平差不多，那我举几个例子啊。我举几个例子就是。

仇钧潼

01:27:39

比如说。我们在讲那个？呃，GPT的时候那我们说这个GPT呢，它是采用了这个。就是这个这个这个解码器这个部分对吧？呃，然后呢它解码器部分大家马上就有一个疑惑了，那不是这个在自助自利益这这里呢，我们应该是三叉戟嘛，是吧？就上去是一个三叉戟嘛，但其实它有有有一些呢是从。Encode这边过来了，那我们就有一个疑问，就是你如果单独用decode的话，你能成吗？你能摆脱这个encode的影响，你能做到吗？你有一些信息是从encode这里过来的呀。对吧，那他自己里面也没有解答，那他就说了，唉，我们用了类似于另外一篇文章所介绍的这个传词方法的decode的结构。那他说的文章是哪一篇呢？就是这一篇。那在给大家的论文里面的标号是320。那大家可以看一下这篇论文啊，那实际上呢，它把这个C这个过来这个。信息砍掉了，它还是用了原先那个三叉戟的形状。来形成这个？那这个是第一个很很明显的这个问题。好，然后呢，在论文里面呢，它是采用了左边。这种构造这个自注意力就是用了左边这个构造，本来这个V跟K是从这里这个C。这个这边的这个C这里过来的，对吧？那它现它现在把这个砍断了，就变变成下面这种结构。好，那你注意一下上面它这里有一句话就是什么什么。

仇钧潼

01:29:19

那大家都知道了，这个我们的这个dota本身。我们原先是你输入第一个字符，它出第二个字符，你再输入第二个字符，它又出第三个这个呃说错了，说出第一个词，它出第二词，然后你输入第二个词，它出第三个词，对吧？那这个。这个这里你用了max这个字眼的话呢，我就很容易怀疑你在训练这个大语言模型的时候，你是怎么给它训练的呢？

仇钧潼

01:29:50

那举个例子，比如说。我这里有一个语料，这个语料可能很长，有N个字符这么长。那理论上呢，这个穿穿的decode它不大可能接受这么长的一个字符串是吧？所以我在上面只能够取一个窗口，那这个窗口的宽度是K。我就把这个窗口里面这个语料截出来。好，然后我怎么预训练他呢？我怎么预训练这个模型呢？那我就把这个窗口的东西塞进来，组成一个X矩阵，我预测的是这个窗口底下的下一个词，这个词是什么？我预测的是这个。

仇钧潼

01:30:29

好，那如果是这种情况的话，有两个细节问题你必须要解答。第一个max用在什么地方？你还哪有max？你输进去的举证都是满满的。你还哪里有？那他这个不解释，他就写这个max那里。也不知道他怎么？那这个是第一个问题，那第二个问题呢，这个可能有点钻牛角尖啊，比如说你输进去。的是不是不是这个？我输进去了，可能是更前的，比如说。这个W一前面这里是空的。我我但是我你你总不能忽略了我这个WE的这个信息量吧，对吧。当然你可以可以从稍后一点的位置来开始截取一个完整的窗口，那我的意思就是说，如果我们要从这个位置，最左边这个位置，再往前面截的时候，那你这些空缺的部分，你填什么东西进去呢？那他也完全没有说明这些事情。

仇钧潼

01:31:27

就是说这个open AI写的论文往往会漏掉很多很多细节啊，那这个刚才我提到的这两个问题呢，大家都可以。这个仔细的细想一下。

仇钧潼

01:31:47

好，那还有呢，这个还有一些疑问呢，我们讲到它的分类任务对吧？那分类任务呢，大概就是这样子，就是说我们把要完成的分类任务的那些语料拼成一个长的序列。拼成一个很长的序列。比如说我们要对这个句子进行分类。那我们就开始给他一个star的标记啊，我说的是GPT一啊，然后呢，把这个文本夹在中间，最后给他一个X的标记。我把它送给这个。那送给他。最后出来是什么呢？就像右边这里显示的这样子，我要既要要要求它有分类。然后呢，也要求他能够预测这个下一个词汇。

仇钧潼

01:32:32

是什么。那两者都要求？那我在训练？是一个微调任务的时候。训练这个分类任务的时候，那我怎么输入这个序列呢？但也是开一个窗口。这样滑动的这个输入进去吗？这看起来比较别。就说我。我有有一有一种做法呢，就是啊，这个文本可能不是很长，这个文本一般都比较短。那我就把这整段话。那我我我就不需要这个什么窗口滑动这类的东西东西了，我们整段话插进去，那这里也谈不上什么max。那他这个又又又不知道怎么搞了啊，好，那如果这段文本很长呢，那你要开一个窗口。那开一个窗口的话呢，你送进去穿？然后你这个窗口又不断的滑动啊，往这边滑动。这个一直滑动到最后一个X的时候。一直滑动到最后一个的时候呢，我才能够得到它的那个分类的标签啊，最后得到它的分类的标签。那那这个那我这个滑动还有什么意义呢？那我还不如直接就给他最后一个就算了，那你说前面这些滑动对他来说有什么意义？好像没有什么戏。那究竟有没有滑动呢？那在这个论文里面也是说的不大清楚。啊，因为这个GPD本身。

仇钧潼

01:33:55

有公布源码GPT EG PR这些有公布源码，完了，大家有兴趣的话，你可以看一下源码，来研究一下它里面的这些细节的事情。那我把我的疑问呢都写在这里了，就是说你对于分类任务啊，那如果是短文本的话，那这个好办111口气塞进去就可以了，就没有所谓max的概念。那如果长文本的话呢？那你如果要滑动窗口，那但这个滑动窗口有，有没有意义呢？那你为什么不直接你输进去最后一个窗口就完事了？那我希望大家回去以后大家可以思考一下这些问题，那我很乐意。

仇钧潼

01:34:34

在这个微信里面呢，跟大家进行讨论啊，那你可以进一步读这个GPT的论文。好，那下面呢给大家说说这个后续的G。那有一种GPT呢叫CPT，那这个其实就是我们现在用了3.5版本的GPT。那就是用指令？来进行优化的这个gdp它是人类介入的。那这篇文章呢？很长，有这个大概好像60几页吧？那里面呢也有，大家看到有很多这个华裔的。一看这个名字。比如说这个跟这个啊，还有这个这个无啊，那另外还有下面这些。那大家可以看到有很多这个华裔的工程师在这个ai里面工作。那现在呢是很高薪的。高薪到什么程度呢？三级的工程师三级大概是什么水平？如果你从一所名校硕士毕业，那名校呢是指像这个CMUM IP啊，斯坦福啊啊这个这一类的。如果你从名校硕士毕业。进到这个公司里面，大概就是三级，那三级的年薪呢是60万美金。那这个什么概念呢？那我对比一下吧，就是美国总统拜登的年薪是40万美金。那他一进去就已经比拜登的工资还高了，<笑声>，有时候我也会鼓励一下这个毕业生这个去从事这个方向啊，争取了一份很高的薪水。

仇钧潼

01:35:58

然后到了56级，那56级的话呢，大概就是工作几年时间。就你已经有几年，就是刚才说的那种。呃，名校毕业的硕士你在一个公司里面待了大概有56年，那你平均？一到三年会升级一次，假如你不被炒掉的话，你会升级一次，那就工作了56年时间，那你可能会升到五级六级，那些都是过100万美元的年薪。就这个这个这个增长幅度很可观啊，那所以这里呢也吸引了很多有才华的工程师在里面工作，那也包括我们很多这个华裔的工程师。

仇钧潼

01:36:36

那这个我记得有一次接受采访的时候呢，有人问我一个问题，他说你觉不觉得中国的人工智能水平比美国落后了？我说好像至少我直观感觉没有，没有这种感觉，为什么呢？因为我看任何一篇人工智能的论文，而且是那种殿堂级的，就是那种很很牛逼的发发表在很牛逼的杂志上，对后面影响很大的论文。那最典型的，比如说像那个长沙神经网络，这是何凯明写的说。那我一看这个名字就有一大堆华人在里面。那你看这个汉语拼音呢？这个都是内地的汉语平原。因为这个香港跟台湾用的那种拼音，跟内地用的汉语拼音是不一样的。所以你从名字上就能看出来，这个绝对是一个内地出来的，可能是高中出来的话，大学出来的。然后呢，这个去了美国工作，或者在中国本地工作，或者在这个港台工作啊，这样的一些人写的论文，所以至少从华裔这个角度来看的话。

仇钧潼

01:37:37

话呢，我们是一个ai。这个上帝之选的民族，我觉得是这样子，就我们这个民族是很擅长于跟AI合作的，那也许这个AI时代呢，对于我们来说，反而可能是一个机会，是给我们一个弯道超车的机会，那我们错过了上一个时代的机会。像这什么蒸汽机啊，电气化这些，那我们都没有能够好好的追上这个世界的这个先进水平，要花了很多时间，因为我们那时候都在忙着打仗。这个各种的战争啊，没有时间去搞这些事情。那现在呢是一个和平时代，那正好又赶上这个AI发展的大潮。那以我们中国这个人口的数量，产生数据的那种，那种速度，还有我们这个中国那种聪明才智。啊，那种卷呐，就是那种勤奋的，就是日夜刻苦的学习各种知识，这种勤奋的程度，那我觉得应该在里面呢，是很有发挥空间。

仇钧潼

01:38:31

啊，我说远了，我又回到回到这篇论文这里面来啊。呃，这个GPT主要是要解决一个什么问题呢？那大家如果用过那个早期版本的GPT啊，甚至现在的GPT其实都有一个。所谓的这个一本正经的胡说，胡说八道的这么一个事情，就是你问这个GPT一个问题。他的回答看起来是像模像样的。比如说我问他的，我问他一个问题，这个我我就开玩笑了，我是IT P的创始人嘛，我看看他认不认识我啊，我问他IP他的，他的IT的创始人是谁呢？那他就告诉我一个名字，这个名字我从来都没听说过了，<笑声>就就就一。11本正经的回答了我一个名字，然后我过了几天又问他同样的问题，他他又回答了另外一个名字给我，那很明显的他他这个比较缺乏这个知识。就是尽管这个IT八创始人可能在网上能够有这个网页，可能有一些记载，甚至可能在这个什么百度百科里面都有记载，但很明显的他呃没有。没有用到这些，他找回来这些资料啊，或者他可能有自己一套理解的方法吧啊！

仇钧潼

01:39:42

那这个一本正经的胡说八道的话呢，是大家对这个GPT系列的第一印象。那我们怎么样改变它呢？那另外呢，还有一个就是它不能够避免这个有害信息，那所谓有害信息就是说，呃，是国家法律不允许的东西，那我们国家就不用说了啊，就是有很多这个国家法律是明令禁止。你不能讨论这些东西，你不能干这些事情啊，那就算你在所谓的这个标准，自己很言论自由的国家，那有很多话题也是被禁止的。那比如说你试一下，在美国说一下黑人的坏话公开场合。那您马上会被？会被做成一个肉饼？<笑声>等等，就是说你其实在任何一个地方都有它不自由的的那个有有限制的地方。那这个你作为一个公共产品的话呢，你肯定也要做一些避免有害信息这种事情。那你现在如果你不做一些人类的干预，不做人类的这种监管的话呢，那很明显这个GPT本身是不大可能获得这种能力的。那所以呢，由于这些驱动力呢，这个open AI就提出了一种叫做RL HF的做法。那大概的意思就是人类指导的。或者人类指定的。这个。

仇钧潼

01:41:00

人类指令的那个强化学习。啊他他他说的是这个事情。那open AI本身就是以强化学习，建厂为一家公司，那所以它在这个GPT技术里面呢，用到这个强化学习呢，一点都不奇怪啊。那这里有一张图，它描述了这个整个思路的过程，那我下面逐逐样来说吧啊。我这个图太小了，我到把它变大一点啊，它大概做了这么几个步骤。

仇钧潼

01:41:28

那第一步呢，它从一个所谓的P数据集的什么是pm呢？那你可以把它理解为就是问题。大家问的问题。那这个数据集是怎么来的呢？那第一个有可能是由open AI的员工用了某种机器学习的方法自动生成，或者这个由这个员工或者他的雇员来编写。那也有一种可能呢，是来自于用户，就是说你问这个这个GPT的问题全部都被他落下来了，被他记录下来了。那这个是一个？疑问啊，那大家要注意就是open a i可能会记住你问他的东西，尽管他声称自己不会。但是这个还真的不好说。好，那我们就从这个数据集里面呢，抽样问题出来。那这个openai公司它在？非洲在肯尼亚用两美元一小时的这个薪水。呃，大概是这么一个级别的薪水，在当地来看已经很高了，当然这个。从我们这个角度来看的话，从欧美的角度来看呢，这个是一个很低的一个薪水啊，他聘请了一堆这个回答人员给这些问题编写这个合适的这个答案。好，然后呢，我们就通过这些编写好的问题跟答案对。

仇钧潼

01:42:51

对这个gpt来进行。那这里的微调是指翻吞两个意思，就是会改变它的那个权重。他会对这个模型进行修改的。那通过这种人工编写的这种回应。来增加这个。呃，GPT对于这个知识的覆盖程度啊，那这个这个是第一个啊。然后第二个呢？呃，我采样一个。以及让GP。来作出回答。来做出回答。就是说这个我可能通过选择不同的参数或者选择不同的模型。那使到这个GPT回答出来的答案呢？可能有ABCD四种。那我就找一个人工。标注圆来进行标注！把这四个答案排一个序，从好到坏排一个序，就是说他他看了这四个答案以后，觉得哪个是最好的，哪个是不好的，把它排一个序啊，好，然后这些标注数据呢，我们就可以用它来训练一个。

仇钧潼

01:44:03

这个深度学习模型。那这个深度学习模型呢，就是专门给这个回答打分的，那大概它的用途就是你送给我一个答案，我就给你打一个分数。大概他能做这个事情啊？好，那我就用这个模型作为奖励模型，就相当于强化学习里面的奖赏函数。那我用它来进行强化学习来改变这个呃。这个gpt模型里面的一些参数。好，那这个是第二个步骤啊！第三个步骤刚才我讲了啊，第三个步骤。好，那这个做完以后呢，我们这个GPT就自己进化。这个GPT三呢就开始自己能够进化。呃，大概到了3.5就用了这个。RL HXF的技术以后呢，它的质量上呢，就已经有了很明显的这个提高啊。那据说这个GPT3.5呢，就是基于这个的啊！那当然这3.5了，大家也用过。这个在我们的这个钉钉群里面也有提供。他的回答质量呢，其实还是有所欠缺的，尽管他已经比以前的好很多。那这个在GPT三GPT22的时候，这个GPT基本上就是不能使用。都是经常性的满口胡言，就是说任何一句话基本上都是满口胡言。基本上到了90%以上这种惊人的程度。那到了这个GPT3.5的时候呢，就已经比较靠谱了，大部分的回答呢？还是可以的啊。

仇钧潼

01:45:44

好，那这个是GPT三？那至于这个细节，比如说他用了什么模型啊，用了什么参数啊，这些都。无可奉告因为这个gpt这个这个这个公司没说啊！好，然后呢，这个我们再说一下GPT四就目前。最厉害的这个。大语言模型。那GP四呢？有一篇论文，那这个论文呢，也是一个技术报告，它也不是一个正规的，详细的论文，那一概的细节都没有透露，现在我们都全部都不知道。然后这个论文里面基本上就各种吹，就是说这个GPT四怎么怎么怎么怎么厉害啊，各种吹啊，那连那个署名都省掉了，这个连，因为大家都觉得这个署名这种文章没什么意义，那不就是一个一个广告文嘛，一个软文嘛，对吧，那所以就干脆就。数了一个openai就算了，大家都不想署名了，没什么意思。

仇钧潼

01:46:36

那这个G四所做的改进我在这里列出来的，那我也不练。那因为大家都用过，大家都知道那大家可能。呃用的不多，那你你可能不知道的就是这个GDP T四确实特别贵，它大概是比3.5要贵十倍以上。那以我们的印象呢，还还不止贵十倍，可能贵了有好几十倍这么多。就是你回答3.5的一条问题，可能就是一分钱以内的成本，零点几分钱。但回答这个gp的一条问题呢，通常都要几毛钱这么多。所以这个贵的特别厉害，那我们不是现在免费供应给大家，这个钉钉群可以用那个。GPT3.5嘛对吧。3.5是全免费的，但GPT四呢，我们就限制了，你必须要带着别人进来，我们的群，你才可以去申请用那个GPT四，那为什么要这么做呢？那主要是把一些这个无聊的这个问题，一些无聊的人。把它这个用这个门槛把它隔掉，因为这个实实在是太费钱了。那这个每个月都费我们很多钱，那费了多少钱就不说了，<笑声>，就每每个月我们为了这个钉钉群都付出了很多钱。

仇钧潼

01:47:46

好，那这里是他说他自己吹的一些指。啊，比如说应付各种考试，什么历史考试，这个数学考试，语文考试28的有一堆啊，然后呢，这个蓝色的是3.5能够达得到达到的高度。绿色这个呢，你你看一下呢，这里还有深浅呢！就是有深深的一节，还有浅的一节，那浅的一节呢，就是GPT四，但是没有那个视觉功能。他不能看图啊，那所能达到的高度到这里这个地方，但是当他能看图的时候呢，他就到到了这个地方。那这个很简单呢，因为这个试题里面肯定包括图的呀，那你不会看图呢，你还能跟他干嘛呢？那你就只能直接举手投降了，对吧？那所以呢，这个能看图就能够拿到比较好的分数。

仇钧潼

01:48:30

那大家可以看到了，在大部分的考试里面呢，它都是能够拿到80%左右或者更高的这个分数。那这个也很厉害呢，你让我现在去考这些科目，我能够考个平均分，四五十分就不错了，我还记得那么多东西。啊，那所以对于考试来说，他是很厉害的啊！

仇钧潼

01:48:52

然后呢，这里是一些文本类任务的一些指标了，这个就不细说了。还有他已经有了这个多模态的能力。那比如说啊，这个有人给了他一张图片。问这个ppt啊，你觉得这张图片里面有什么特别的地方吗？那这个DVD试一看啊，就马上就说出来了，这个图有一个很幽默的地方是什么呢？就是说啊，我们把一个。很老的一个很大的这个。一个古老的计算机插头，这个一个VGA显示器的插头居然插在了一台。手机上面那这个事情很搞笑。啊，那这个他看出来这个。这个亮点啊，那这个说明他还是比较聪明的。

仇钧潼

01:49:36

那这里比较了一下不同的版本。啊，他的那个回答问题的能力，那大家可以看到了，这个GPT四绿色的这条线都是遥遥的领先。那这个大概是目前人类能够达到的这个最高的高度的模型。那这个GPT三跟GPT3.5了，它的结构是一样的，都是1750亿。

仇钧潼

01:49:59

的参数那GPT四有多少个参数呢？这个有人猜大概是它一共有八个模型。每每个模型是用来管一个领域的知识。回答一个领域的问题。然后呢，每个模型的参数量呢，大概是2200亿左右。那八个模型呢，大概是一共是1万7600亿的参数，那大概就是GPT3.5或者GPT三的十倍。这么多。那所以它的收费也大概贵十倍，那这个是有道理。因为它的计算量确实大了很多。

仇钧潼

01:50:39

那这里说的是回答那个敌对性问题的时候的表现，那敌对性问题通出来就是找茬的。那比如说这个有一条很典型的问题，就是梁山破108好汉是谁？这个到目前为止好像还没看到哪个大语言模型能够很很高水平的回答。

仇钧潼

01:50:59

啊，还有一些敌对性的问题，比如说这个鲁迅为什么打周淑文？<笑声>那这个这个很明显不是正常人能问问得出来的问题是吧？这个梁山坡108好汉，这个还是一个比较正常的问题，但是你说鲁迅为什么打周四一听就是来找茬了，对吧？那现在GPT是呢，已经能够很好的回答这个问题了，他说这个鲁迅跟周树人其实是一个人，他们不可能打架。大概是这么回答的啊，那这个其他有一些这个大大圆模型还不行。就是一些比较低版本的模型，他就找出各种理由，就是说，唉呀，鲁迅跟周树人什么价值观不一样啊，他们写作的文风也有区别啊，所以这个鲁迅一生气就把朱主任给倒了，这一听起来就是一本正经的胡说八道啊，那大家以后要考验一下你的这个大语言模型。靠不靠谱。那你可以用这些问题来回答一下他。我肚子里面可能有几千条敌对性的问题，我经常用它来考验一些机器人，然后呢看到他们的回答了，我就会哈哈大笑，那这个是一个很好的一个娱乐活动。啊，那这个是对敏感内容的一些指标了啊。

仇钧潼

01:52:09

那关于这个大语言模型呢，这个GPT四就是一个顶峰了，很遗憾的就是。并没有他的技术细节。那我也给了一些论文给大家，这些论文呢，可能里面也介绍了其他的一些语言模型，稍微在原理上要复杂一些的，但是参数量呢不一定很多。这些语言模型呢，也许对于大家选择这个基座模型。会有所帮助。啊，其中比较有点有代表性的，一个是T五。那T五呢是google的一个大语言模型。那有很多它的那个产品呢，比如说像这个but什么之类的，都可能是以这个T五来作为制作模型。那我觉得这个是一篇很值得研读的论文，那大家有时间呢，可以去读一读它啊。

仇钧潼

01:52:56

然后呢？还有这个清华GO？就是呃，这个何老师昨天。让大家这个安装了这个模型，那这个因为清华大学是我们国家自己。的这个团队，然后这个模型本身就是开源的。他的水平也不差，就在中文的那种解答的水平上呢。可以跟至少可以跟GPT3.5相媲美，只是比四稍微差一点点。那这个大概代表了我们目前我们国家的最高水平的一个开源的模型。那其他甚至商业模型都赶不上他？那比如说我们也用百度的这个文新一言，哎呀，这个回答的质量就是一塌糊涂，还收钱呢？那这个我还不如用清华了，算了，完了大家可以对比一下，我们在丁丁区里面都开放了，有文心言的，也有这个清华的，那你就找我刚才所说的一些问题，那些敌对性的问题啊，比如说这个曹操为什么跟慈禧太后离婚啊，这周周世人为什么打鲁迅呢？打鲁迅呢，这个梁山或108好汉，这是哪些人呢？找这些问题问问他，那你就能够感觉到它的差距所在了。那由于这些。论文里面介绍的原理的细节都比较繁复。那我觉得就不大适合在课堂上讲了，那大家回去呢，可以有时间的时候自己去研读一下，那可以帮助你更好的了解这些系统能不能够满足你的这个场景的这个需求啊。

仇钧潼

01:54:26

好，那关于这个语言模型的那个？呃的一些选型啊，还有哪些种类啊？我们就说到这下面呢，我们说一说大语言模型的微调，那大家注意了，这里的这个微调呢不是指。那翻脸呢，一般是指我们做追加训练，就说我们这个模型训练好以后，我再用一些新的语料对它追加训练，使得它也能够理解我这些新的语料，但翻脸呢，通常会意味着整个模型的权重会进行修改。它会产生梯度，它会对整个模型的权重产生影响，对修改的影响。那我说的这里的微调呢，其实是指那种微调层的微调。就说我整个模型里面大部分的权重是不动的，但是我给它里面一个小的一个构建，比如说一个层或者一个小的部分。那我就只改这个部分的权重就好了，其他地方我是不改的。那是这个意思？

仇钧潼

01:55:24

那我第一周的课的时候，第一次课的时候就给大家解释嘛，是吧，我说那个这个GBT允许大家训练语料，那我举个例子，我们胎基地那也允许大家训练语料是吧？那那是不是每个同学都给你一个GROM？就是说黄老师可能要拿出100台服务器出来，分别每台机都装一个GOM，为什么每台机只能够装一个GM的原因很简单，因为他把那个显卡的资源都耗光了，他要。十几个车的显存。那我假设我用一张3090，那它就只有24G显存。那所以它基本上一个GOM就已经把一个显卡的显存耗光了，那除非我插几张显卡。那那这个显卡的价格已经跟一台机差不多一样贵了，可能比那一台机还贵，那比如说黄老师这个给大家免费啊，我们就提供了很多这个服务器，然后呢，大家免费用吧啊，那这个黄老师。这个每个月掏的电费都不得了，就不要说买服务器的钱，这肯定不能干这个事情。

仇钧潼

01:56:28

那这个所谓的这个呃胎基地的提供了服务呢，肯定在这个GM上面呢，我们有一定的微调整。就这里有一些微调层啊！每一个使用者，他改变的只是其中一部分，这个微调成了这个权重。卡龙这个微调成呢，只有几百几十兆到几百兆这么多，那你就算100个人用，那我顶多也就是几个G。这个十个车这个规模，那我这个。内存啊，还有存储啊，各种的资源还是能顶得住的，那我用一台服务器呢，就可以为大家提供服务了。

仇钧潼

01:57:04

那这个也是为什么我们能够使到这个台基地，向大家提供廉价或者免费服务的主要原因，就说，如果你一家企业自己要去买一台服务器回来，你要去管理它，那你不光要钱，要人还要有电费。其实你也很费时间就很费资源，那其实没有啥必要，那你就用黄老师给你的台基地就好了，那你只需要付很少的钱就能搞定，那这个相当于一种更高级别的托管，那以前我们托管呢，往往就是塞一台服务器。进到这个IDC里面去，或者说我在IDC里面租一个虚拟机是吧？那现在呢，我们就更高层面的那种托管，我们可以帮你托管模型。然后呢，甚至还可以帮你托管你自己训练的这个微调。那那我们就干这个事情。

仇钧潼

01:57:53

好，那下面呢给大家讲讲几种不同的这个微调方式啊！第一种呢叫做那这个论文也已经。放在群里面给大家编号忘记了，但是那个名字就叫做。那他的意思是什么呢？大家看一下这个不是一个？里面的层嘛，对吧，这是里面的一个层。他对这个穿这个层做的一些结构上的改动。本来这个。还有下面这里是没有的，这两个东西是没有的。我现在呢，加进去一个。那无非就是一些简单的BP神经网络，可能有一到三成这个样子。而且呢，很可能也是没有激活函数的，就是纯线性的。我把这些层把它加进去。那这些成可以给你做什么事情呢？

仇钧潼

01:58:54

当你要对这个模型进行这种追加训练的时候。这个穿这个我已经把它训练好了。我就把预训练好的那些参数全部把它冻结。你进行微调的时候呢，能改变的只是这个层里面的参数。只能改变这些参数，其他你都不能改变。那通过改变这些参数呢，就可以影响这个创始方法的行为，可以达到理想。

仇钧潼

01:59:26

这个诉求的效果就说它能够按照你要求的回答来回答你的问题，或者按照你要求的这个语种来进行翻译等等。那你的所有的这个诉求都是来帮你体现的。那所以呢，每一个人都有各自的诉求，a有a的诉求，B有B的诉求，C有C的诉求。那我针对a啊a我把你这两个层拿出来。保存起来啊，这个是a的，然后B的话呢，我也把它拿出来，这个是B的。当这个ABAC上线了，使用它的应用的时候呢，我再把这个a的这个表层把它漏到内存里面去，来进行这个推断的过程。那B就使用它的时候呢，我要不又把这个打成？放到内存里面去。来满足B的需求。那这个a跟B的诉求也可以同时满足，那只是这个数据走不同的回路就行了啊，那这个通过这个方法呢，就可以实现微调。那这个是一种比较古老的这个微调的方式。然后它的效果呢过了。

仇钧潼

02:00:31

好，那这个是第一种。第二种叫。那这个论文呢？也放在这个？呃，群里面的那大家一下就看到我有我们。华裔人士啊，这个一看这些都是我们中国人比较典型的性。这个名那是两位都是斯坦福大学了，可能是留学生。或者博士啊，这样子？那这个也是一个比较？有意思的工作了，后面那个ping呢？就是以它为基础的，那是什么意思呢？那大家看一看它的结构。那这个穿它本来是这个样子。那如果我们要进行修改的话，那这个就大工程啊！比如说我要这个方法完成不同的任务。当他要完成翻译的任务的时候呢，我用翻译的语料来。进行对他进行这个追加训练。然后呢，形成这个这个翻译的这个transforma啊。然后呢，当它要用来做写这个文本摘要的时候，我们又要用它来训练一个transforma，这两个transform法呢是彼此不一样，完全独立的transform。他都很大。

仇钧潼

02:01:44

然后呢，这个第三个呢这个是？这个这个把一些数据表格把它变成文本。那总而言之，就是你为了不同的任务，你可能要训练不同的传输方法。那假设我如果把这个模型不变。我给他加上一些微调的构建，能不能够使到他能够完成不同的任务呢？那这个想法比我的厉害呢？我的想法只是满足不同的语料而已，就是能够回答不同领域的问题。但是呢，我的任务还是问答机器人这么一个任务。他现在就直接就奔。不完成不同任务去的。其实我对它的效果是很有怀疑的，因为不同的任务呢，会导致这个方法的结构，它里面的这些权重很不一样，那是不是我们增加了少量的部件，就能够使到它们在功能上产生如此大的区别呢？其实我自己也没有什么底啊！好，那他的做法呢就是这样的。

仇钧潼

02:02:48

那穿是放马的话呢？我们在这个。输入的东西的时候呢？我们加上了这个。这个所谓。那我我把这个事情呢，已经在这里说了啊！就针对下的新的下游任务跟领域呢进行微调，那我们冻结这个预训练模型的参数。但是呢。这个我们只需要在。呃，在这个transforma的这个维度上做一些增加，增加了部分呢，我们称为叫。就这样就可以了。就说你原来可能是。A乘以512的输入。A N乘110就是你的单词是512就是你的那个数目是吧？那我把512扩大一些，扩大一个，比如说扩大到。这个768那增加256给他那前面那256呢，就是让你去做这个。那大概是增加了这么一个构建，就能达成这么一个目的啊！那这个是。

仇钧潼

02:03:54

然后呢？还有？那这个批领呢？昨天下午何老师应该给大家讲过这个清华的GOM的那个微调。清华的那个的微调的方法呢就是。那他这里有第一版有第二版的区别？那这里清华用的是第二版，那第一版呢？好像不是清华清华人发明，只是他使用了。但是后来可能觉得那个版本不大好，那又又改成了用第二版。

仇钧潼

02:04:23

那第一版是什么意思呢？第一版呢是这样的。我们本来要输入。到那个。呃，穿是双马的时候。那我们首先把一句话做对吧，比如说我们用来做。然后再输入到方法里面。那我们的过程呢？是这样的，我们。先编码，一个矩阵。好，我们再把它输入到这里去。我们本来本来是这么做的。好，那现在呢，我们输入这个矩阵X的时候呢，在中间。加一个结构啊！这个是X。好，然后呢，我在中间这里加一个小结构，大概是四层的LSTM。那这个X呢？经过这个结构的变化以后呢，最后再输入到这个in code里面去。那其实增加的结构呢，就是在这个部分。那当我们进行微调的时候呢，也是跟以前一样，这个encode这里的参数完全是被我冻结，完全不动。我们只是动这LSTM这个层。这个层里面的这些参数。那所以呢，它也能产生微潮的效果，那这个是V这个第一版所用的方法。然后第二版用的方法呢？基本上就跟那个差不多了。它也是在前面加上这个一些前缀的部分，不过呢，它就不叫。它改成叫。就就是在那个你输入的那个词的前面加上一些东西啊。那这个批这个批的名字？一开始的时候可能是来自于。那后来就变成是？那这个是。那大家如果想了解详情的话呢，可以看一下。这个我给大家的那篇论文啊！那还有一个词呢，也会经常出现的。

仇钧潼

02:06:40

那罗拉是什么意思呢？就是低质的。这个。大语言模型的这个自适应。这个什么意思呢？啊，这个就比较有趣了。好，左边这个。是你已经。训练好的。那这个我们输入一个东西进去。输入X出去他就出来一个C。这个X跟C的维度是一样的，在这里显示为就是黄色，这个是X，上面这个黄色的是C。那我们怎么对他进行这种微调呢？那它会旁入一个神经网络，这个神经网络。那这个神经网络呢，有点像我们在这个预习材料里面讲的那个。自适应的啊，不是那个那个叫自编码器一样。它是一个中间有一个很明显的瓶颈结构的，两边是一个倒锥形的这么一种结构，就好像你进来一个向量啊，经过每一层的变化呢，我把它略越捏越短，越裂越短，这中间好像压榨它一些什么东西？然后上面呢，在上面再放松，把它还原出原先的向量。那这个在微调的时候呢，我给的这一块东西是完全没有训练过的，你随便给他一些什么全。

仇钧潼

02:08:02

好，然后我怎么微调呢？我把这个向量。送进去以后，那本来我要改变，如果是翻领的话，我要改变左边这个部分的那个全值嘛，对吧，那现在呢就变成说我在右边这里也输入同一个东西，把这个东西要。撸到右边这里？然后经过他一系列的变化以后，把这个输出也跟左左边方法的输出相加，这个才是最后的输出。那我们的训练呢，就只是对红色这个部分来进行，蓝色这个部分是被我冻结，你是不能动的。

仇钧潼

02:08:41

那所以呢，所有的微调的效果呢，都体现在右边这个部分的权重里面。那你你要这个训练的这个什么程度呢？就就是你你是那种略微的微调一下呢？还是深度的微调一下呢？就看你这个网络的那个复杂程度了。那如果这个网络比较简单的话呢，你大致上就只能做简单的微调，那你要复杂的微调的话呢？那你可以把这个网络改成复杂一些啊！那我们。在我们的台基地里面呢，呃，稍微透露一下，有可能会采用这种手段。那我会根据你给我的语料的复杂程度，就我认为我判定的复杂程度来选定这个罗拉层它的这个复杂程度。然后呢使。不至于被过度训练。就是到这个火候刚刚好啊，我说的我火用了这个词来比喻，就说你不要把它烧焦烧焦了，又不要夹生，就刚刚好，那你怎么去选择这个罗老城，那这个就很重要了。

仇钧潼

02:09:44

那这个呢是很？很主流的，非常常用的一种微调的构造跟微调的方法。那原因很简单，因为它根本就不需要改动，原先的这个代码就是你原先的创始方法是怎么写的，还还是那样那么去写。没有问题，我我对你这个原先的东西完全不感兴趣，你你你这个代码封装好了，我不都不需要管它，因为我根本就不改它。那你只要有输入能输出的那就行了。那我是根据这个输入输出的这种产生的误差的效果呢？修改我能够改的这个罗拉的结构，我是改右边这个结构。那所以你对于理论上你对于任何的神经网络预训练好的神经网络任何的。你都可以使用这个方法。而且呢，对这个结构上没有多大的变化。也很容易实现。

仇钧潼

02:10:36

那所以这个是一种。啊，不错的方法，我个人认为是一种不错的方法啊！那现在用的比较多，这个落的地方呢是那个？那个画图。生成那个图的那个软件？那用这个用的比较多，那在清华里面呢，它微调方法除了会选择的以外，也可以选择。

仇钧潼

02:11:04

好，那这个是关于微调的内容啊！大家看看有没有什么问题啊，关于这个部分。啊，不也可以进入时间？

王蓓

02:11:25

王波同学可以发言了，刚刚举手的。

仇钧潼

02:11:28

诶，你好。

王波

02:11:30

呃，黄老师你好！

仇钧潼

02:11:33

哎，你好，你好！

王波

02:11:34

啊，我就是想问一下，刚才讲这个微调。

王波

02:11:38

这这个就是一个？但是增加了词的维度呢？还是增加数的长度啊？就是前缀这个？

仇钧潼

02:11:49

怎么说呢，这个就是就是说说实话，我我也没有怎么细读这个论文，就看起来好像是加入了这个，加多了这个长度。但是呢，按照你说的这个这个这个好像也能也能这么理解，我回去再仔细看一下那篇论文，以后我再跟大家说这个事情。

王波

02:12:12

呃，还有个问题，刚才您说那个G四可能是有八个模型是吧？

仇钧潼

02:12:17

呃，这个是传闻的这个微卫星政策。

王波

02:12:18

啊，就就就就就假如有八个模型，假如有八个模型的话，就我们平时可能也会用到，就是我我有八个专业领域，我可能把它协调的话，我是用冷呢，还是有什么别的办法吗？

仇钧潼

02:12:20

这个只是传统。我我可我觉得他可能比如说像分派啊，比如说我我这个接受你这个提问以后呢，我觉得哪个模型我有一个分类器，我觉得哪个模型会回答你这个问题更好，我就把你这个问题分派给这个模型。

仇钧潼

02:12:45

<笑声>我感觉这样子我觉得不大，可能是集体学习，这个成本太高了，就说如果把你这个问题同时给八个模型去回答，然后我再综合一下这个回答，那这个手段的话呢，可能就是成本很高。

王波

02:12:58

哦，这个这个如果用了这个能能实现吗？

仇钧潼

02:13:04

啊什么。

王波

02:13:05

就是那个昨天讲的那个？

王波

02:13:10

那个放假能能实现这种东西吗？

仇钧潼

02:13:11

啊。

仇钧潼

02:13:13

呃，能解决什么问题？

王波

02:13:15

就是八个模型协调。

仇钧潼

02:13:19

他好像不能解决这样的问题吧，我我认为啊，他不能解决这样的问题啊。

王波

02:13:21

啊，我我还有就昨天也问过，就是何老师，就是今天就我想问一下，这就是我我我训练了一个专业的，就是那个网上的。

王波

02:13:34

微调的案例嘛，就是它遗忘很严重，我就想从根本上它的一就是它就把那个通用的回答都忘了，就是原理根本是什么原因呢？根本原因是什么？

仇钧潼

02:13:39

这个就说不清楚啊，就是说这个微调完以后就变成一个**，这个也是很常见的情况。

王波

02:13:49

他他只能回答我训练语料里面的东西。

仇钧潼

02:13:52

就好像有人去高考了，这个拼命让他学了一段时间以后，他回家就变成一个**一样，<笑声>，就其他的生活都不能自理了。

仇钧潼

02:14:01

我觉得这个这个这个。微调了还不如用那个？向量数据库的这个手段这么好，就记忆就是记忆是更更牢靠了，更可靠一些的一个方法啊！

王波

02:14:14

啊向量向量数据库啊，那个就是说。呃，这算不算一种过，你和呢就是这种遗忘？

仇钧潼

02:14:22

呃，有可能是就是你你的追加的这种微调的训练呢，导致他原先的一些回答问题的逻辑跟框架都搞错了啊，那这个是不大好的事情。

王波

02:14:32

啊，就我的可能太多了，或者是怎么样的是吧，就是训练的。

仇钧潼

02:14:36

对的对的是是是。

王波

02:14:38

我还有一个问题，就是就是这么多大模型嘛，就是。

王波

02:14:45

呃，我看昨天给了一个树啊，树的结构，然后基于各种模型他们。

仇钧潼

02:14:47

是吧。

王波

02:14:52

它性能差别的根源是什么呢？就是说我它的模型算法可能是。就是模型的这种呃。

王波

02:15:00

结构不一样了，还是数据不一样了，还是主要是？

仇钧潼

02:15:03

都有，就有模型本身的因素就是主要是它的那个。呃，参数的规模，这个因素。啊，然后还有他数据的因素，因为你规模上去了，不等于你就有很多知识。那你要有相适应的数据来训练它才行，这两种因素都会有。

王波

02:15:21

啊，我我主要关心就是说你像清华的GG LM还有什么呃，国外的一些对国外啊这么多模型，它模型本身这种模型的结构差别有多大呢？是？

仇钧潼

02:15:27

羊驼，羊驼。

王波

02:15:36

不是很大嘛，我觉得我也没。

仇钧潼

02:15:37

呃，差别就是原理差不多在细节上可能会有比较多的差别，因为大圆模型也是有很多种的，就是你看完那个T五啊G IM以后呢，你会发现它好像跟其他的大圆模型的套路。

仇钧潼

02:15:52

又稍微有一点不一样。哈哈哈。

王波

02:15:59

好，我我看还有什么问题？啊，大概现在没别的问题啊，谢谢！

仇钧潼

02:16:06

啊，谢谢好的！

许涛

02:16:08

黄老师。

仇钧潼

02:16:08

大家还有问题吗？诶，你好诶！

许涛

02:16:10

就昨天咱们说的那个就是那十就是说的那个下游任务不是有11种还是多少种下游任务吗？

仇钧潼

02:16:17

啊，那个是关于关于？

许涛

02:16:20

啊，就反正不管是不爱乖什么就是说。呃，那不那那个当时说的不也是一种微微调嘛，对吧，那那那个。

许涛

02:16:28

那就是。

仇钧潼

02:16:29

那个那个微调跟这个微调就不是一个意思了，那个微调就翻了，就是它直接是改权重。那个微调是改成用的？

许涛

02:16:36

就就咱们说的这个下游任务的这种微调跟跟现在说这个不是一回事儿是吧？

仇钧潼

02:16:40

啊，不是一回事，不是一回事，那个那个跟这个不是一回事，对，当你你要把它做成一回事也行，就是说我微调的时候，就是我要做下游任务要进行。

许涛

02:16:45

这这这个。

仇钧潼

02:16:53

翻吞点的时候，我冻结住这个大模型的参数不变，我只是把这个我增加了构建的参数进行改变，其实你也可以这么做，那当然效果就会略为。呃，顺势一些就差一些。

许涛

02:17:07

但是说那种方式就是咱们说的是下游任务，是说在这个大圆模型的后边儿。

仇钧潼

02:17:14

对，就接了一个构建，那接了一个ML P之类的这个神经网络，你可以单独训练那个ML P不改那个大模型，那也有一个这样的流派的。

仇钧潼

02:17:24

也也会有这样。

许涛

02:17:24

那咱们这。我就今天说的这个叫perfect，就是前前置的，就是把这个东西放在这前边去了是吧是？

仇钧潼

02:17:26

对对，没错是。

许涛

02:17:33

那那这种是主要是干是是一，它是主要是干嘛，而且也是也是为了对大大学研磨器去改进是吧？不是为了下游任务去用的是吗？

仇钧潼

02:17:43

呃，就是微调它它这个也是针对所谓的下。比如说我们。

许涛

02:17:48

啊也是。

仇钧潼

02:17:49

我们要这个用新的语料来训练它，这个要调整，要调整它啊，那这个可以用这个方法来调整。

许涛

02:17:59

嗯，然后就是刚才说那个lora那个模型，它右边那个神经网络就是一个普通的一个多层的一个神经网。

仇钧潼

02:18:05

对对，一个普通的一个ML P就行。那当然，你如果不喜欢用这个普通的，用特别一点的也可以，就是额外增加一个稍微简单一点的神经网络给它。然后呢？用这个增加的部分呢来负责。微调的参数的这个调整，那这个这个这种思路我们111概叫做lola，但是它也没有规定你右边这个部分非要用什么不可。

许涛

02:18:31

甚至比如说右边我甚至又接一个另外一个单独的一个方法也是可以的是吧？

仇钧潼

02:18:36

啊对对，没错是。就是它这个L了，右边这个部分呢，你只要跟左边的部分有同样的输入，输出的格式就行，使得它们能够加起来。

许涛

02:18:48

行，谢谢老师！

仇钧潼

02:18:50

大家看看还有没有什么问题啊？啊，不是没错，它就是单单独的一个神经网络，单独的一个神经网络，就另外的一个神经网络。

仇钧潼

02:19:04

那这个呢诶，你好诶！

lu

02:19:05

呃，老师你好，我想问一下那个transformer的问题。

仇钧潼

02:19:10

哎，好嘞。

lu

02:19:11

就是transform transformer做生成式任务的话，它是怎么从这个？编码翻译成那种文本什么的，就是那种。

仇钧潼

02:19:20

现在用它来做生成式任务的话。就是跟GPT的这两个比较主流的这个方法了，这个在前面好像都讲过了。啊，都已经讲过了。你你想了解的是什么什么什么是什么什么问题呢？

lu

02:19:38

哦，我就是想大概了解一下，就是它怎么生成这个从100生成文本的。

仇钧潼

02:19:45

呃，它左边就生成了，对吧，左边的这个编码器部分就生成了一个一个。然后这个呢，送给这个解码器。你只要输入一个begin的一个token，它就会生成第一个单词。然后你把这个单词又喂给他跟？的这个也喂给他？那它又能生成第二个单词，就反复循环，一直到最后。N那这个生成的过程就结束了。那在这个中间呢，你要不断的把这个给输入给它。那大致的过程就是这样子。

lu

02:20:21

呃，老师，我是不是可以理解为他是在反复不断的做一个分类任务，就是说我预测一个单词，然后放进去，然后再预测下一个。

仇钧潼

02:20:27

对，你也可以这么理解，就说每个单词就是一个类的话，那你也可以把它理解为反复的在做一个分类任务。这个理解是对的。

lu

02:20:39

然后老师那我没有什么别的。

仇钧潼

02:20:42

好，谢谢。那大家没什么问题呢，我们再休息一小会。今天就得要拖堂了。

仇钧潼

02:21:51

这个这个问题，所以我我也没有理解政策。

仇钧潼

02:23:32

我就实际上我只需要，就像你上次去参观。就直接上传一些资料。也是这个。当然会不会影响成交了？

仇钧潼

02:24:18

说这个破坏了，他非常广泛发现。我都搜不到。就就是拆迁地的，反正。上面有个酒店。昨天那个地方？楼下有个。35块钱。不都不是什么。警察所以我我没听啊！呃，在那个。他说那个最？其实这个事情是挺有趣的，只是他太我看到有一些文章研究是第一种的生物的神经系统，它里面好像只有七八个小时。呃，就把这个画出来。我把它每个..这个文章..。啊对，就反正什么人是不少，对他就已经发到了。所以如果你把一个人的系统研究清楚。那个就不是发到这个就是整整。这个中间。就什么呢。你就像那个机器一样。你知道。那你体会到？你要能够说我？就是说他怎么从微观现象，微观现象我们就能够观察。做测量的，但是你怎么从微观变成？这个确实要思考一下。但是在这个。上汉的研究。但是他是这个模型大家就去。在我们学习。就你你去。但是你怎么去答题，今天让你去做题？其实这个东西我是觉得。他那个ppt他也可以模拟各种。不是确实这个结构。那个那个你的那个？但是他有些人有好多个七八个。这个人不同，不同表现，所以刚才我们说GDP四的时候要分八个。如果你想统一。好，我们继续了啊，好听到吗？

王蓓

02:31:03

可以的，声音很清晰。

仇钧潼

02:31:05

啊行好。呃，那下面呢，我们又转到另外一个问题了。稍等一下，我这里先。

仇钧潼

02:31:27

呃，刚才在前面一次课程里面呢，给大家讲了这个。大圆型还有大圆型的微？那我们进入我们整个课程的最后一个话题。就关于这个知识图谱的扩展，以及怎么样用这个知识图谱。不好意思，跟这个大语言模型结合起来，完成一个呃问答机器人就是这个事情。那这个至于最后一个话题呢，应该今天下午由何老师。来回答大家啊，因为他会带着大家用这个知图谱跟大语言模型一起呢，来完成一个这个。机器人的设计的工作啊，那大家可以通过做就明白了。

仇钧潼

02:32:07

那我自己的这个重点讲一讲这个支出啊，其实怎么扩展？那我们前面给大家讲的呢，就好像大家都觉得这就是一个小玩意。就是啊，这个我根据一个场景啊这个。拉拉一些概念出来，然后呢，把它实例化。啊，然后呢，我们就建设出一个初具雏形的这个。那我们现在所设计的知图谱呢都是很小型的，那里面大概这个实体呢，可能应该就不超过两位数，不超过三位数，就两位数字，然后这个关系呢也是稀稀落落的。但实际的知识图谱呢，比如说你一个维基百科的知识图谱，它里面涉及的磁材有可能就达到10万以上这个级别，然后涉及的这种关系啊，联系的边的话，那就更多了。这个死不死不生死了那一个即使你能够完成最简单任务的这么一个。呃，一个医疗的知识图谱啊，那实际上的话呢，这个也是一件。非常麻烦的事情啊，那这个那我就把后续这些工作呢，给大家讲讲啊，就是说我现在先用一些简单直观的方法，比如说我在书里面，后面找这个index白系列的实体，把那个名词把它抽出来了。然后呢，我也可能用这个呃编程序的方法，用一些模板匹配的方法，比如说我判断一下两个实体之间。有没有出现某个动词啊？那我就判断这两个实体之间具有什么联系啊，等等，那我可以建立一个初步的一个知识图谱。

仇钧潼

02:33:35

那我怎么把这个知识物把它完善呢？这本身呢是需要迭代的。那里面。这个完善的过程里面呢，值得在课堂上面讲的。适合使用这个机器学习技术，特别是图机器学习技术的大概有。下面这几块？第一个呢是那个实体挖掘？

仇钧潼

02:33:57

实体挖掘其实就是命名实体识别，那这个何老师在堂上呢，应该已经给大家讲过，有一些工具，还有这个自然源处理的办法，那我今天呢，尽管在。这个PPT里面也贴了这个这个幻灯片上去，但是我就不打算详细的去讲了啊，然后呢还有这个关系的抽取。或者补全或者纠错。

仇钧潼

02:34:21

就说两个实体之间他有没有关系呢？这个。那我们这个图里面的关系越丰富。那我这个知识图谱的能力也就越强，它的这个推理的那个。呃，正确性也就越高，表现出来就是更聪明的样子，就好像我脑袋里面的这个神经元互相的连接，这个越来越丰富一样的啊！

仇钧潼

02:34:45

然后呃，这个就是里面的一项很重要的功能，这个关系补全，关系纠错等一下都会给大家讲。然后还有就是属性。那属性补全的意思就是说你这个节点现在有这些属性，但是我认为它应该还有更多的属性。那你把它补全以后，那也可以使到你这个知识图谱的内容更加丰富，那因为在。在网上的这个？前一节课第二次课里面呢，给大家讲过了。这个根据知识图谱做问答机器人。这个提问人的这个基本诉求，我们可以归为三类，就意图识别上可以归为三类，那第一类呢，就是你询问一个实体的属性是什么？那你如果你一个实体的属性只有很少，那你有什么好问的，他也答不出来是吧？但是如果他的属性很多的话呢？那他就会可以做出这种比较丰富的这个知识程度比较高的一个回答啊。那另外还有这个实体对齐，还有实体消极的问题，那这个等一下一起给大家解释，以及把它的方法来给大家讲讲啊。

仇钧潼

02:35:51

那关于这个命名实体识别呢，就是这个实体识别这个事情呢，因为之前已经讲过了，我不知道为什么没有把这几页VT贴过来了，那我就不讲。那我们下面呢，讲一讲这个关系抽取。股权以及纠错啊！那这个呢，主要是判断两个节点之间是不是有边？就是我我我可能做这么一个补全的事情，我发明一套机器学习的算法。我随便在这个知识图谱里面拿出两个实体出来。你告诉我你，你是一个分类器，你就告诉我它们两者之间有没有变。

仇钧潼

02:36:29

如果有边的话，它是属于什么类型？它是属于什么类型的啊？那我们就要回答这些问题。然后呢，还有一个呢，就是关系的这种纠错。两个实体之间已经有一个关系了，那这个关系是对的还是错的呢？那我们要发现这个事情。那关于这个呢，有一个非常漂亮跟成熟的算法。那我现在呢，就可以给大家讲讲这个事情，那这个大概也是今天重点讲的一个事情。这个算法或者叫工具呢，它的名字呢叫。在学这个图机器学习或者看图机器学习的书的时候呢，你应该看到过这个名词。那有关的论文呢？是这一篇啊，他的名字来自于这个translating。就是说对这个嵌入的一个翻译。啊，大概是这个意思吧啊？那这个算法是怎么样的呢？我画一张图给大家看一看，大家就明白了。那我假设左边这个是我的graph？在这个图里面呢，我们这个H代表头结点啊！这个是头。就是我们通常说那个subject这个东西啊！然后这个T呢代表的是尾实体。就我们通常说那个那个实体啊，然后这个L呢代表的是它们之间的一个。联系。好，那下面呢，我做一个事情，我把这个节点。包括T，包括H以及边这个L。都做一个嵌入都做一个。

仇钧潼

02:38:16

那这个线路是干什么事情呢？那就是把这些节点映射到一个低维空间，比如说这个空间的维度是K。我要求这个映射他能达到的目的是什么呢？这个。我要达到了目的就是。这个是头节点？应该是等。说错了。应该是加上L啊！就是等于T。要求达到这样的效果。就是说我这个节这个节点要嵌入边也要嵌入，都它们都变成了向量。我唯一的要求就是，假如这两个节点之间连了一条边。他就必须要满足这个事。所有的。你这个知识都没有里面所有的节点。只要他们之间有联系，就必定要满足这个式子。那理论上呢，只要你这个空间维度足够，就主要这个K足够大呢，一般来说呢，这个是能够做到的。那下面呢，我们看一看怎么做这个事情啊？我们下面看看怎么做这个事情。大家也大家可以看看它的算法。这个算法有点搞，我不知道有没有同学们看的明白。大家可以先简单看一看啊！第一个首先第一个问题。这个问题我觉得是一个灵魂拷问。其实有很多人。有一些很根深蒂固的认识。到了这个地方是转不过弯来了，我们一讲嵌入，我们就马上想到应该有一个计算嵌入的函数。或者一个神经网络对吧，你输进去某个东西？你输出某个东西，那这个叫我输进去一个节点，你就输出它在K维空间里面的这个坐标，或者说那个矢量，那个向量。

仇钧潼

02:40:30

那这个不就是一个是吧？但是你在这个算法里面，你看到了这个的函数是什么？你看到了这个？这个这个这个他用了什么神经网络了吗？都没有看到。而且最奇怪的一句话是，我给这个L一个初值。这个初值呢，应该是满足负根号K分之六到正根号K分之六之间的一个均匀分布。就说我从这个区间里面抽一个值出来付给他。我付给他干什么？那这个L这个L不是一个向量吗？不是代表联系的向量，那我为什么从这里抽支付给他呢？那理论上你应该有一个神经网络，我我是随便付个值给这个神经网络的，全职的，这个是对的。那这个L是我们要的向量啊，对吧，我我有什么理由我要对我的向量付个出值给他的那个不应该是结果吗？

仇钧潼

02:41:31

那大家看看怎么回答这个问题？他想想这个问题应该怎么回答？那这个问题呢是这样的，其实这里的嵌入呢很简单。不需要神经网络。那我假设你嵌入到三维空间嘛？啊三维空间？那我们现在要要切东西，不是有三个吗？

仇钧潼

02:42:01

还有这个T啊，另外还有一个。这这三个三个向量啊！这个画的有点偏，不好意思。三个向量。这个嵌入是什么呢？其实不就是他们的坐标吗？这句话有点费解。嵌入的参数是什么呢？其实就是这三个东西的坐标是吧？你告诉我这个坐标呢，我就知道怎么嵌入了。那不就是这个事情吗？那大家还记不记得在讲的时候我们说那个是一个矩阵。是一个矩阵。它的那个行跟那个词典一样多，这个词典就是从第一个单词WE开始W21直到WN一共有N个单词。然后每一行就是它的线路了。888什么之类的？那这个是在里面涉及到的嵌入矩阵，其实我们只要把这个矩阵求出来了啊，我就知道这个了。是怎么做的？他是怎么嵌入的？那大家想想。那这个不就是把每个单词它映射的坐标列出来？对吧，那因为你这个东西不就是它的那个矢量的坐标吗？就是你把只要把这些坐标全部列出来了，那这个就是代表一个嵌入。你是不是神经网络这个根本都无所谓，我根本就都不要紧，我觉得。所以在这里的话呢，你只要给出。H的坐标，七的坐标，还有这个R的坐标。

仇钧潼

02:43:49

那假设都是KV空间，那每一每一个向量都是有KV的向量，这里是K线，这里是KV向。还有这里也是，那我们就一共有三K个参数。这三K一个参数呢是待定。是代理，那我们的要求呢就是。如果他们在原先的buff里面是成为一个所谓黄金三角形。就他们之间是能连边的话呢？那他们就应该在这边也是黄金三角形啊！在这个这个kv空间里面也应该是黄金三角形。那这个就是我们的要求好，那你这么一看，那就豁然开朗了啊！那你这句话这个LO给他付这个什么意思呢？其实就是给它的每一个坐标在这个中间选一个值出来付给他。就这个意思大家明白了啊，然后呢，这个把它变成一个单位向量。这个其他的也差不多吧，这这些意思都差不多。

仇钧潼

02:44:50

这个对实体。就对对，每一个实体我们都要付一个这个。这个初值给他，其实也是对他的这个坐标付出付出值嘛，就是因为我们现在讨论的是整个知识图谱的嵌入，而不是讨论某几个节点的嵌入。所以我们对整个知识图谱的所有实体。

仇钧潼

02:45:09

都必须要给他付出值，就是给他的坐标付出值。它如果有K有K的坐标的话呢，每个坐标都是选自于这个负根号K分之六到增根号K分之六中间的一个数字，它是通过均匀分布抽取出来的这么一个数字。这些就是他们的初。好，有了出资以后呢，那我们就想到了，我们肯定有一个损失函数。我们对这个损失函数使用这个随机梯度下降法。那下面我们就差一个损失函数。我们就差一个损失函数。那我们看一看这个损失函数是怎么写的呢？它在这里写。就是这个。你对损失函数求梯度。那就是说你在这个对这些坐标给的初值的情况底下，我怎么迭代的意思是吧？我们怎么理解这个损失函数呢？

仇钧潼

02:46:06

这个是一个很典型的三元组损失。这个大家如果对神经网络很熟的话，你就知道什么是三元组损失。他这里的要求大家首先看前面这个部分。这里这个部分啊，首先看这个部分。我们刚才说了H加L应该跟这个T是相等的是吧？那所以这里的话呢，这个D表示距离。如果满足了这个要求的时候，这个前面这个部分就是零了。我现在要最小化这个函数，那这个D肯定是一个正数。那这个第一是距离，它没有说具体怎么去做，那这个我们也可以不用管它，那大家可以想象距离是什么样子。那它最最小就是零啊，一般来说它是一个正数。那最小的时候就是正数，那我们现在要求把这个损失函数求极小。那这个。它是零就是极小的。那我们希望对于大多数的这种有联系的节点呢，它应该是满足这个黄金三角形的要求的啊，那这里说了。

仇钧潼

02:47:18

这个HLT呢？它是一个三元组。对于三元组呢，应该他们能组成黄金三角形，或者尽量组成黄金三角形。

仇钧潼

02:47:30

好，后面捡的这东西是什么呢？那这个就是反面教材复样本？就是那些不是不能够配对的。后面这个部分这些是不能配对。那不能配对的是什么呢？很简单呢，我随便出两个节点。然后给一个关系给他，那这个他能够配对的可能性太低了。就我随便任意抽两个出来。然后我就就指定一个关系，把这个AO也指定给他，那他们他一般是不能配对的，对吧。好，那这个时候呢，我就减去后面这个部分。剪这个部分那我要使到这个。这个十个嘛？这个东西。越小越好。那我要减，这里是减的，大家注意啊，这里是减了，不加是减。所以后面这个东西就越大越好。

仇钧潼

02:48:23

那就是H撇，加上L跟这个T撇的距离。越远越好。那就是这个意思。那我们的embedding应该就能够应该要做到这样的效果，那这个是典型的三元组损失。那这个伽马呢是一个超参数？啊，这个伽马是一个超三。这个用来使到他这个？这个三元组损失就是怎么写的，我这里一下也不知道怎么解释比较好，就伽马呢是一个我们预先确定的一个数字啊。好，那这个就是呃，整个这个算法的这个轮廓了，那现在我们有了损失函数以后呢，对它进行梯度下降法或者随机梯度下降法。那我们就最终能够求解出。这个问题出来。那在理想的状态底下呢，就是对于我这个知识图谱里面的每一个三元组。都会在这个嵌入空间里面呢，有一个黄金三角形来跟它对应。那对于不是三元组的东西，那它们之间就不会有黄金三角形。那这个传一这个嵌入算法呢，就是这样子。他有什么用呢？这个算法有什么用呢？那这个算法的话呢，用处太多了，用处太多。

仇钧潼

02:49:48

那第一个比如说我可以用来做关系。关系补全。那我随便量拿两个实体。我第一个呢，我首先每一个关系它嵌入到什么向量，我是知道的，对吧。我现在已经知道每一个关系嵌入什么向量关系是有限的，比如说我现在我这个知识图谱里面一共有20种关系。那我就有20种向量。摆在那里。好，那现在呢，我做一个排列组合，我把任何两个这个实体对应的向量减一下。相减一下。那这个你如果有1000个节点，那这个组合数顶多也就100万。那你就算100万次他们相减？每剪一次以后呢，我看一看剪出来那个结果跟那20个固定的关系对应的向量。是跟哪个一样，那我就知道这两个节点之间是不是落了一个关系了，我就可以拉一条线过去了。那这个就是可以做关系补全的利器。那这个是非常厉害。

仇钧潼

02:50:53

这一招是非常厉害！那这个是重举啊，那直接就把这个这个很暴力的把整个这个知识都。变过过一遍，然后呢，它就能够把这个边把它给补上去了啊，那这个是第一个用途。那第二个用途呢，可以做实体补全。就说我有没有？漏掉一些实体呢？那这个道理也是一样的，就是说我，我现在有一个实体，再加上一条边以后看看另外一边那个实体，那是不是也在我这个实体的嵌入的清单里面，那我可以把这个实体把它给补上去。

仇钧潼

02:51:30

那这个过程呢，跟这个关系补全差不多。就是通过这种加加减减，那我们就能够发现很多的秘密。啊，就我们原先被漏掉了一些秘密啊，那这个是穿11。川习啊，一个非常精巧！

仇钧潼

02:51:47

呃，这个。很美妙的一个算法，就是我很少会赞叹一个算法会这么聪明。那这个我觉得这个算法是值得称赞。那这个穿11是不是已经完美了？那这个当然不是了，他其实还是有一些。不大好的地方。那我举个例子。比如说。我现在有a跟B两个人。A跟B两个人？那他们之间？这个。既是同学。又是夫妻。那这个多关系的情况呢？这个是有可能存在的，对吧？那在这个情况底下呢，你可能就会发现呢，这个传奇一没有办法表达了。就是。减去B就是联系嘛，就是a的向量减去B的向量。或者B的向量减去a的向量就是他们之间的联系嘛。

仇钧潼

02:52:47

那这个时候呢，就有一个问题呢，就是同学这个关系跟夫妻这个关系呢？那他们这个向量的表达这个应该是一样的。那这个显然不大符合我们的这个想象的要求，对吧？啊，那这个这个就有一点问题了啊，还有这个a这个人啊，这个是父亲，然后他生了很多孩子。就一对多啊！那这里有B？Cd啊，这个是爸爸？这个是孩子？那这个关系呢，就错了关系。那理论上呢，这三个都是叉的关系。

仇钧潼

02:53:28

那我们说这个伪实体应该是投实体，再加上关系，那你这个T有关系呢，它只能够有一种向量表达。那所以呢，就会产生一个很奇怪的问题，就是BCD这三个孩子他的向量表达这个都是一样的，就BCD是同一个人。那这个显然也是不对的。那所以这个穿11呢，尽管它的构思很巧妙，但是对于稍微复杂一点点呢，其实都不算很复杂，这个情况都是很常见的，那它还是不能使用。那有没有能使用的方法呢？那大家看下面这篇。那这篇论文呢？也给大家了，那这个就厉害了，那这篇论文就更厉害了。那他的思想是什么呢？

仇钧潼

02:54:13

我要主要找一个白的页面来画才行。我先把这个刷掉啊！

仇钧潼

02:54:33

他这个思想呢是这样的，他认为关系跟。这个。实体的映射的维度应该是不一样的啊，假设这个是我们映射的空间啊！这个怎么突然间他又帮我变了啊？啊，这个是我们的三维空间。这个是我们的三维空间。不好意思，我转那种颜色。好，那我们把这个实体。映射到里面去变成一个向。

仇钧潼

02:55:53

好，那我们把关系映射进去映射成什么呢？

仇钧潼

02:55:57

那这里有一个非常独特的思考。我们把它映射进去成为一个平面。而不是一个向量。我们要把它映射进去成为一个平面，就我凭什么这个关系要跟实体是同一个维度的东西？

仇钧潼

02:56:13

那在一个师徒里面，关系的量是很少的。那你映射成向量，那这个跟实体的这个维度一样，那这个没有必要，我要把它映射成一个更加宽广的东西。它映射成什么呢？这个关系呢？会映射成两个东西。一个呢叫做？叫做呃。这个关系映射成两个相映射一个平面。那一个平面呢，有两个向量决定，那其中第一个向量是DR，那这个关系所在的这个平面。应该经过这个地。就是就就他会通过这个点啊！然后呢，还有另外一个参，另外一个参数要决定一个平面，还需要另外一个向量，那这个向量呢叫做WR。那这个WR呢是这个平面的这个法向量就是垂直于这个平面，就是一个关系。R被映射为两个向量，一个是DR我要求这个平面。要通过这个点？还有一个向量呢，就是是这个平面的这个法向量。这两个向量里面的那些坐标值，那些东西都是参数是待定的，是我们要求的。那于是呢，如果你是这里是三维空间的话，H跟T这些实体都是三维的向量。

仇钧潼

02:57:42

但是这个关系呢，就被映射为DR跟WR两个向量，那一共就是有六个维度了。那这个维度是不一样的。好，那做完这个事情以后，我们怎么做那个损失函数呢？那这个损失函数呢是这样的？我把H投影到。这个平面上梯也投影到这个平面上，我们叫做。叫做H垂直，这个叫T垂直。然后呢，我再对H垂直。

仇钧潼

02:58:18

T垂直还有DR这三者他们都在同一个平面上面。在运行这个传。对，在对他们做传。那这个这个这个这个这个这个这个损失还是有点难写啊，我这里就不写出来了。那大家如果对他这个想法有兴趣的话呢，你可以看那个。论文啊，就是刚才付给大家的那篇论文。那这个问题呢，就可以很好的解决了。这个刚才所说的一对a啊，或者说两个人之间啊，多种关系啊，这样的问题。那这个算法呢？叫做？那它也一样，可以用来做这个关系补全。实体补全啊等等这些工作。好，那这个是关于这个知图谱的一个比较有意思的算法啊。那其他的工作的话呢，我就一笔带过了，因为这个时间实在也很难。

仇钧潼

02:59:25

呃，把他们全部都讲的很清楚。呃，我可以说一说其他的一些这个工作分别是怎么做的？这个就不说了，那么细了。还有这个关系补全呢，除了能够除了用那个。呃，传一这类的算法以外呢，还可以用推理。

仇钧潼

02:59:49

那这个推理的话呢？有两种，那第一种呢，你可以用这个。呃，OWL啊之类的这种像知识图谱的描述语言，比如说你在这个OWL里面定义啊。这个。是爸爸或者是父母的这个关系，他有一个反向关系是叫做四孩子。或者呢这个呃？这个是爸爸？王一是王二的爸爸，王二是王三的爸爸，那王一就是王三的爷爷。你可以把这些推理规则都写在这个语言里面。然后在导入的时候呢，它就自动的会进行关系补权，就会把这些关系全部建立。好，比如说他导入的时候呢，发现有两个人啊，这个a是B的爸爸，B是C的爸爸，他就会在a跟C之间加条边啊，就SC的爷爷，他就会做这个，按照规则，他就会做这个事情。

仇钧潼

03:00:42

完了，这个是第一种。

仇钧潼

03:00:43

那第二种的话呢，如果你在导入了知识图谱的时候，没有这个做这个事情，或者说你根本就没有导入，一直都是手工建的，那你就直接用这个C语言或者像sparkle这类的语言。那你先match什么？如果满足什么规则，你就给它补一个什么边啊，那不就这样就完事了吗？那这个就可以做这个？呃，推理补全的这么一个工作了啊？还有呢这个？呃，用GNA呢也可以做类似的事情，比如说我们现在有两个节点。有两个节点啊，就比如说这个是王一啊，这个是王二。我想判断一下王一跟王二之间有没有什么联系？啊，那这个很简单的，那你就做图神经网络。那你把这个边专门给它建一个神经网络，那旁边的那些相邻的节点跟边当然也要有相应的神经网络了，对吧？

仇钧潼

03:01:40

那他们这些神经网络之间呢，是要进行这种信息的交换，要做一个磁化的事情呢？要汇总，要汇聚到这里来，影响他的这个推断。那我最后推断的结果就是这条边的分类啊，比如说它有没有关系，它属于哪一种关系等等。那这个GNN本身就是给这个知图谱提供了一个解决问题的框架，那基本上这个知图谱里面。所有的问题都可以用gnn来解决啊！还有这个所谓实体对齐跟肖奇的问题。那所谓实体对齐是什么意思呢？就说我现在可能有两个实体，他们可能位于同一个知识图谱，或者位于不同的知识图谱。他们可能具有不同的名字。但是呢，他们其实代表的是同一个意思。那如果他们在同一张知识图谱里面的话呢，这个是一个错误。那这个需要进行这个？呃，修正的就说你要把另外一个实体。这个名字不同的实体其实跟这个是一样，要把他们两者合并啊。

仇钧潼

03:02:54

那如果是对齐的话，是两张不同知识图谱的话呢，那我就可以参照。另外一张知识图谱，比如说我把它的属性拿过来。把他的一些联系把他拿过来，可以进行补全的工作。啊，那这个实体对齐的对齐其实也是表达这个意思了。那这个实体怎么对齐呢？那这个方法太多了。那因为这个我在学校里面呢，也经常辅导这个学生写一些毕业论文呢，这样子。

仇钧潼

03:03:23

那这个大概是？读这种计算机系啊，数学系啊，统计系啊啊，还有一些。研究生可能是搞自然语言处理啊，搞这个有关的这种神经网络科学啊等等是最好的一个写论文的题材的太多了，就是呃，我怎么用一个神经网络来做一个实体对齐，那其实技巧的也是跟刚才一样。你可以把。这个节点上面建一个图，神经网络，也把它周边的一些节点把它拉上。

仇钧潼

03:03:55

或者呢，你不建神经网络也可以，你可以做一个图的一个节点嵌入。那我们之前不是讲过这个很多嵌入的方法吗？比如说你基于什么deepwalk基于low。啊，还有这个基于这个GNN的一些嵌入，那你就算一下他们的嵌入是不是这个相似度很高？啊，或者这个距离很近。或者直接你可以用这个端端到端的方法，那你把这个嵌入输到一个神经网络，然后通过这个神经网络，最后来判断他们是同一个东西，还是不是同一个东西。

仇钧潼

03:04:30

那用用这些不同的结构都可以进行判断，大概都能做出一些效果出来。然后呢，基本上都能满足一篇毕业论文的需求。那这个是最好的毕业论文题材的，那大概也可以千变万化啊，大家去去写这东西的话呢，也比较简单啊。那这个世界概念，所谓实体需要对齐，那这个细节我就不讲了。那我今天打算重点讲的算法的就是穿跟穿。那其他的话呢，这个大家自己。有了这个图，神经网络的基础呢，都可以想象是怎么做，但是大概率你也不会这么做，因为你手头上并没有一个很大型的知识图谱，那如果你没有很大型的知识图谱的话呢？你其实做这些事情都没有什么意义，你一眼都已经看完了。

仇钧潼

03:05:15

你这个肉眼都可以进行实体对齐，或者实体这个实体补权或者关系补全，你肉眼都可以做了，那你用这些东西干什么呢？那所以你想要用到这些技术的话，第一个前提就是你手头上先要拥有一个。呃，大型的这个知识图谱再说啊，到那个时候我们在讨论这个事情都不迟。好，还有实体消息，这个是什么意思呢？就说有这个实体。他们是相同的名称。呃，有时候我们在建立实体的时候，如果不检查，那可能会出现这种情况。但是呢，他们却是代表不同的意思。呃，那这个在抽取实体的时候比较常见的，那比如说有时候北京这个词。

仇钧潼

03:06:01

它代表了可能是北京这个城市。那这个很容易理解是吧？但是呢，在有一些文章里面。他可能这个北京，比如说他代表的是北京国安足球队啊，或者这个北京首钢男篮。那它简称就叫北京了啊，今天晚上北京对广州。啊，这个北京赢了，广州赢了啊什么之类的，那你看起来好像这个北京是一个城市，但是你如果把它抽取出来的时候呢，它代表的是一个球队。

仇钧潼

03:06:29

那像这种。这种这这种问题的话呢，我们叫实体消息，就是说我们提取出来同一个名词，看起来它们字面上是一模一样的。但它代表的却是不同的意思。那这种事情怎么做呢？呃，大概也是用我刚才所说的那些方法啊，像这个。CNN啊，什么之类的啊，不像这个GNN啊什么之类的，还有这个图嵌入啊，那基本上都是用类似的办法来解决啊。那我在这里也找了一篇文章，那里面呢就讲到了这个实体对齐以及消息，那大家有空的时候呢，可以参考一下。好，那关于这个知识图谱的一些计算呢，大概就简要的讲到这里啊，那我觉得这个穿11跟穿X啊，这些比较有意思，大家可以认真看一看。

仇钧潼

03:07:19

那这个系列里面呢，还有什么穿是啊，一堆穿是什么什么一系列的？不不止34个，可能有几十个这么多，那大家都可以去呃，看一看有关的文章，那每一种传识呢，都代表了一种非常有启发性的算法，那大概就是知识图谱里面使用频率。很高的一些算法了啊，那下午呢？何老师呢？会给大家演示这个传11的代码。那大家呢，可以试来跑一跑啊！那当然，你现在手头上呢，可能比较缺乏这么大型的知识图谱。啊，最后呢讲一讲这个知识图谱。加上这个大语言模型，怎么去共同组成一个专家问答系统呢？

仇钧潼

03:08:09

呃，这个话题呢，我应该只是给下午何老师开个头。那这里我们找了一个文章啊，那这篇文章呢在？这个。之前在这个群里面呢，也有同学发过。那这个文章呢，我发现他讲的很详细，我一开始还说，唉呀，我这个知识图谱再加LOM这个东西呢，是我自己想出来是独成一派的，后来看完这个文章以后呢，觉得我自己并不孤独，<笑声>，因为有很多人也有类似的想法。而且他们也总结了很多呃这种工作，这个他的那个引文足足有九页纸这么长，它引用的所有论文呢，都是把那个呃这个LOL OM跟这个知识图谱结合了，当然这个论文本身。只是讲结合。他们的结合就是啊unify他没有说是用来做问答机器人。那也许做问答机器人这个事情呢，是我首先想出来。然后呢，我可能也做了，我动手做了，那别人可能只是想一想，那没有动手做啊。那这个大家看一看这这篇文章呢，基本上就知道目前在这个领域里面一些比较流行的做法。那这个文章比较搞笑的是他在发表日期上呢，说说的是21021年8月，但是这个时间是非常让人疑惑的。因为在这个文章的左侧这里，你打开那个论文看就知道了，它标明了它的发表时间是2022年。

仇钧潼

03:09:43

然后它的内容里面提到的一些大语言模型。像这个羊驼啊，像这个拉马。这种东西那这个很明显不可能是2021年都是2022年。那所以我觉得它标这个时间呢，应该是，要不就是错的，要不可能是另有含义代表的，并不是这个论文的发表时间，而只是说这个论文。

仇钧潼

03:10:04

本身可能发表在2021年开始的一个论文集里面，是它里面其中的一篇论文啊，那那可能是这个意思啊，那这个我们就不深究了。

仇钧潼

03:10:16

那这里面我截了一些图啊，比如说他把这个大语言模型的这个家族啊，比如说只使用的。那这个gpt是最最主要的一条路线。那这里这个整个GPT系列都在这里啊，还有这个P。还有还有这个but这些我们都听说过。那另外呢？还有这条路线，那这个是都有的。

仇钧潼

03:10:40

那包括这个公狗的？然后呢，还有这个清华的GOM，它走的这都是这条路线。

仇钧潼

03:10:48

那还有呢，只有只用那最典型的就是。啊，还有这个？一些的变种啊等等，那这里对这个大语言模型呢，做了一个很好的总结。然后呢，他也对这个。这个知识图谱跟？啊大语言模型的结合使用给出了一些范式。那比如说我们可以用知识图谱去增强这个大语言模型。也可以用大语言模型来增强这个知识谱，或者两者呢，共同一起来使用。

仇钧潼

03:11:26

那我这里给了一些解说，那举个例子，比如说像这个知识图谱，它怎么强化这个大语言模型呢？那我们能想到的，比如说好像维基百科的知识图谱，那不是有什么这个呃DB PD啊，就是诸如此类的，什么鸭狗啊，这之类的。那我们可以用什么方法来强化这个大圆模型呢？比如说我把所有的三元组都变成文本。然后我就用这个文本就可以去训练这个大语言模型，就所有的三元组呢，我都可以变成。演变成一系列的问答。对按照一定的模板变成问答。对，简单的问答，对，我可以用这些问答。对呢来训练这个LOM那可以增加这个LOM的这个训练的语料跟素材，那这个LOL OM单就更长更强大了，因为我这个。

仇钧潼

03:12:13

这个基于维基百科的知图谱呢，本身就覆盖面就很大是吧？那第二个呢，你把这个知识图谱呢做成一个插件或者端口调用了形式。就说我有一个知识图谱服务器。你可以通过这个一个CG PT的插件来访问我的端口，你来问我问题我就来回答你。那现在这个GPT本身已经能够支持插件了，那虽然大部分插件都是很无聊。但是呢，如果你有一个领域性的知识图谱的这个插件提供给他呢，我相信这个绝对不无聊，非常有用啊。那从这个角度来看呢，这个大语言模型通过调用这个插件或者其他端口的方式。

仇钧潼

03:12:54

那这个也可以起到强化自己的作用，因为它获得了更加广阔的支持，对吧，但是很遗憾呢，现在能够支持这个插件或者这个外网访问呢，大概也就是GPT了。那其他的模型呢？还没到这个地步，就还没进展到这个地步啊。还有呢。通过这个展示这个资讯的来源，就说你大语言。模型回答别人问题的时候，谁知道你是不是一本正经的胡说八道？对吧，那这个我心里没底啊，你告诉我这个IT吧的创始人。是这个胡锡进？那那这个就胡扯了，一看就胡扯了，对吧，就是我该不该相信你呢？那你你应该把一些理论根据告诉我，就是你怎么根据什么东西来回答我这个问题，那你总总得要告诉我一下是吧？

仇钧潼

03:13:41

那一种方法呢，就是，呃，我把我根据的那些文本全部展示给你看，全部把它找出来是吧？然后呢，或者说我好像知识图谱一样，把我的那个寻找到的三元组以及他的附近的一些知识罗列出来给你看。

仇钧潼

03:13:58

那你就可以看看啊，这个回答原来是根据这个。那看起来还是有道理的啊，那你就可以决定自己要不要采信它。这些现在其实这个车GPT也好，方问也好，一个最大的问题就是使用者都不知道该不该相信你。那特别是医生，那你说这个方问开了一个方，那我是相信你还是不相信你？那你如果不能说出根据？那我为什么要相信你？那这个出了问题不是你方问负责，是我负责，对吧？那那我付出的代价好大，那我可能出了医疗事故，我被吊销这个医生的牌照，这个是一个很大的问题，相反的我用了方问以后把一个人治好了，好好的活蹦乱跳的就出去了。我又没有什么奖金。就负责任，要负很大的责任，吃好的人也没有奖金，那我很很很明显该选择什么，我就是我就反正没根据的我都不信，有根据的我才信，所以列出根据呢，也是一件很重要的事情啊！

仇钧潼

03:14:57

好，那第五个呢就是啊，第四个呢，就是它比较容易实现这个多模态。因为知识图谱本身它的属性是可以多模态的。比如说你可以把这个图像，声音，视频。作为这个知识图谱里边熟悉的一部分。那当我去查询这些属性的时候，我自然的就可以把这些东西送给你。而去观看。那相反呢，你要通过语义去找一些图片，视频声音的话呢？这个就太困难了。这个要做得准确呢，是不容易做到的啊！那大概是一个？知识图谱强化大圆模型呢？大概就是有这么一些。地方啊。还有反过来的？这个。大语言模型，强化知识。那这个怎么强化呢？第一个？

仇钧潼

03:15:44

大语言模型可以丰富这个知识图谱的功能啊，那我这个举个例子吧，比如说我方问啊。这个方问。那我在访问里面也在嵌入这个知图谱。我比如说我嵌入一个疾病的这个知识图谱。就是各种各样的疾病啊，每一种疾病呢？有什么表现？有什么症状，我通过什么检查可以查到。

仇钧潼

03:16:08

这个这查出你是不是确诊这种这这种疾病有什么药可以治疗？好，那这里有一个问题来了。我我不是医生，我找谁来干这个事情？如果我去请一个医生回来给我写的东西啊，这个医生。累死了，黄老师的钱包也是要变憋了，就说我要请11堆医生团队坐在这个办公室里面坐满，每天就给我编写这些文档，但是有必要我这么需要人力来编写的。那这不很简单吗？我就首先找一本书，把所有的疾病的这个名称列一张表出来。我就做一个循环语句，每一次我就问GPT四，唉呀，这种病有什么症状啊？这个GPT四老老实实回答我了，我就把它说出来，放到这个图里面，然后我就说啊，治疗这种疾病用什么药啊？那他又告诉我了，我又记下来。

仇钧潼

03:16:58

然后这个治疗这个这个这个检查这个疾病要用什么这个检查的方法他又告诉我，那我就不是全部搞好了，那还要医生干什么，我不需要医生。当然也可能需要一两个意思。不用很多，那这医生呢就是给我这个。校验一下这个CP四的回答对不对，就怕他一本正经的胡说八道。好，那这个事情就搞完。那这个就是所谓lol丰富知识图谱的意思啊！然后第二个？就像我之前说的这个LLOM呢，对用户的这种提问的意图，他的理解是很好的。我们一般用这个知识图谱来做问答机器人的时候，所谓的。QA就基于知识图谱的问答，机器人的时候呢，我们通过是用模板匹配或者正则表达式匹配的方法来理解提问人的意图，那这个是很不准确，而且是很费劲的，要略尽所有的模板。

仇钧潼

03:17:57

来重进人类能够问出的一切问题，那这个就是天方夜谭。所以呢，其实有很大一部分问题，我根本就不能理解他问什么。所以也回答不了他的问题。但是有了以后呢就好办了。那我只需要把这个答案送给LAM，把问题也送给他，哎呀，这个GPT你帮我回答一下这条问题，我把资料给你了，他自己就不知道它的整合，就去回答了。那这个它的回答框架呢也是非常理想，这输出框架非常理想。可以符合人的体验，总比我自己在这个知识图谱里面，按照一定的规则把这个回答的内容组合起来，送给用户要好很多，因为那种方法呢，实在是太生硬了，这个用户看起来就像个机器人讲话一样。

仇钧潼

03:18:43

根本就看不清楚啊！好，第三点。那这个从LAOM里面呢，也可以获得提炼。这个三元组的一些素材。那刚才说过了，你的素材其实是。不多的，但是这个。基本上已经用了天底下。很大一部分你能找到的这个文档来进行训练，所以这些知识都在LL里面。你可以源源不断的有点像取之不竭一样。让他向你提供三元组。你可以问他各种各样的问题啊，比如说我想知道这个疾病跟药物之间能不能治疗，那我就问这个这个GPT啊，这个疾病能用这种药物来治疗吗？那他无非回答你能跟不能，那我就马上能够完成或者。

仇钧潼

03:19:33

否定一个伤感。啊，那这个就是LOL能起到的作用。那最后一个呢，这个LM可以直接？赤膊上阵直接下场！就是我问这个何老师跟我都向大家演示过了，我给一篇文章给这个。LAM啊，我说唉，你帮我抽取出里面全部的三元组，他抽出来了。你抽抽取出里面全部的实体命名实体啊，它抽出来了，尽管这个抽取并不是。尽善尽美。他有遗漏的，但是呢，也已经很不错了，就算他遗漏了一半了。这么严重，都能够大大减轻我的工作，那使得我做这个事情呢，更加容易了一些啊。

仇钧潼

03:20:17

好，那这个是去强化知识图谱。这么一个事情。

仇钧潼

03:20:22

那还有一种用法呢，是协作的。就这个知识图谱跟LAM协作了，那在那篇论文里面还提供了一个所谓的协作的分层模型，它分成了好几块啊，这个我就不练了。那大家可以看呢，包括用了这个应用层。那可以用来做哪些应用啊？比如说可以用来做搜索引擎，用来做推荐系统，用来做对话系统就QA了，用来做AI助手。然后呢？用了哪些技术呢？那看到了有这个P工程，昨天何老师给大家讲了，还有图神经网络，那我们一上课的时候就给大家讲了，还有这个上下文学习，这个是昨天讲的。

仇钧潼

03:21:02

还有这个表征学习，那如果是图表征的话，应该是我们这个课程开头的时候讲的。那还有这个？神经符号。推断那这个我们没讲啊，这个东西太深奥了，这这个以后有机会再讲啊，还有这个few short learning昨天也讲。那大家有些同学觉得，哎呀，我们这门课为什么那个知识点这么多，好像这个很杂一样呢？那这个没办法，因为它本身需要的知识就很多啊，那只能够什么东一枪西一炮的，这啥都讲一下了啊，那这些知识我们基本上都讲过了。

仇钧潼

03:21:35

然后下面这里说的就是这个其他的一些事情呢，就比如说需要什么数据啊？这个。啊神经。这个这个大语言模型跟知怎么协作等等。那这里呢，我打算举一个例子来说明一下这个知识图谱跟这个LAM协作是怎么进行？那就以我们那个方问来作为例子吧啊！在前面啊。

仇钧潼

03:22:14

呃，在这个方问里面呢，我现在在设计一个功能。这个功能啊，这个读医的这个朋友呢都会知道，因为在听众里面呢，有很多医生就叫做中西医结合辩病辩证施治。这个名字好长，但是其实很简单。那第一步呢，首先我要。辨认出这个病人，他是西医里边的什么疾病？

仇钧潼

03:22:40

那这个是第一步？那第二步呢，你辨认出这个西医疾病以后呢，按照中医，它会给这个西医疾病进行一个分型辨证分型。一般来说，大概会有四到八种分型，就一种疾病对应有四到八种分型。然后呢？这四到八种分型呢，每一种底下呢？会对应于一种治疗的这种药物，就经方就所谓的经典方。那所以你只要选定了。这个。病症那后面这个流程呢，就是一个纯IT流程，其实呢，并没有什么技术含量。没有什么技术含量。就说我把这个功能加到我这个软件里面，这个医院都觉得你这个功能也没啥稀奇的，好像很简单啊。那这里面呢，我计划怎么做呢？计划怎么做这个事情呢？我打算把它做成一个知识图谱。那这个视视图里面包括的内容呢？就是西医的疾病。然后还有这个症状，还有中医的辩证，还有这个检查项目，还有你要开的中药。中药啊，中成药？另外还有这个西药处方等等。那包括这些内容，那大家可以想象啊，就在这些概念底下呢，他们之间有一些相互的联系啊，比如说啊，你开你得了某种病啊，你就应该去做某种检查，或者呢你可以去呃用某种处方啊，你应该会具有什么症状？

仇钧潼

03:24:14

那这些是可以想象的。另外有一些大家可能想象不出来的，比如说某两种疾病，它的症状会很相似，那你要注意区分好这两种不同的症状。比如说这个胃痛跟心绞痛，有时候人就分不清楚。啊，就表现出来就是胸口痛，哎呀，这个地方好痛。那这个是胃痛还是这个心绞痛呢？那这个差别就大了，胃痛的话，你大概就吃颗胃药就没什么事了。但是心绞痛的话呢，这个就很严重了，你要马上到医院里面去检查，要及时的吃一些针对这个心脏的药物，不然会有死亡的危险。

仇钧潼

03:24:49

那所以这个这个初学医的人呢，可能分不清楚这些东西，进一步的这个诊治检查方案也是没有的，那我这个图谱呢，可以给他提供相应的帮助。

仇钧潼

03:25:01

好，那我在这里可以想象出有一些什么功能呢？那大概我能够呃做这么一些人工智能化或者知识图谱结合的功能。那大家可以看一看啊！那第一个呢？我输入症状，这个病人有什么症状？那我马上能够通过这个语义匹配。跟那个疾病的这个这个进行一些匹配。能够给出他可能是什么西医的疾病？然后每一种后面呢，我都可以把相似度把它给附上。就另一张表出来，他可能是这个病，那个病，这个病这个病。

仇钧潼

03:25:39

那当然，这里面呢，也要考虑到这个病是常见病还是罕见病。那罕见病的话呢，要把它的全值降低，常见病呢，要把它的全值升高啊等等啊，这个我们有一些内部的一些调整的算法啊。那第二个呢，对于这个西医的这个疾病。就说我显示出来这种西医的疾病，因为它只有一个名字，有很多刚当医生的人对这种疾病是不是很熟悉的，那我要显示出。他的这个解释这个疾病是什么呢？他有什么症状呢？他应该做什么检查呢？他容易跟什么疾病混淆了。

仇钧潼

03:26:16

那这里有两个方法来展示，那第一个方法呢，就是你从知识图谱里面把这些东西查出来，你在界面上11的展示出来，以表格的形式展出来。那第二个形式呢？是直接展示知识图谱？

仇钧潼

03:26:31

我直接把这个并定位出来，就像那个这些画的那张图那样子。画图漂亮是他的一个优点嘛，大概是。那当然现在有很多这个图数据库。都把画图漂亮作为他主要的卖点。那都很漂亮啊，那这个越漂亮越好，那我们在这个图数据库选型的时候呢，也要注意选一款这个画图比较漂亮的这个图数据库，那这个对于增加你卖的这个产品的那个售价是很有帮助的，因为这个用户都是很爱美的。那如果你的这个输出的这个结果很美观的话呢，它对你的第一印象会有很好的这个改观。那就像大家。这个可能有一些这个听众会写论文呐，比如说在校生或者老师或者这个。这个其他一些生殖的这个医生啊什么要求？那大家可能不知道写论文你要写得好，要被那些很好的杂志像nature啊，这种或者这个国家的这个核心期刊收入的话呢，其实有一个很重要的前提条件是什么呢？你想不到的就是你的画图一定要很。

仇钧潼

03:27:41

你不要最好不要自己画，你要找专业的美工来画，你这个图画的好，特别是那个核心思想那张图画的好。对于你这个论文被收被录取，被录用是很有帮助的，那如果你画出来就像涂鸦一样。那你这个结论再算是最好。写的做的很好，别人都是不会录用你。他，而且他是很会怀疑你那个结论，因为以貌取人是人的基本的本性，就说你图画的不好的话，他对你的这个第一印象就会很糟糕，就像好像那个人长得丑，就不适合去当销售一样。

仇钧潼

03:28:18

<笑声>这个道理是一样的，那大家如果没发表过论文，你准备发表的话，你可以记住我这句话，你一定要找专业的美工帮你把这个图拿画好啊。好，那这个话又差远了，我们回来我们这个地方啊！呃，那这里就是。显示这一系列的资讯。然后呢，对于这个？呃，特定的西医疾病，比如说这个医生已经判断这个病人经过检查，经过这个X光啊，经过CT检查以后，他得的是这种病已经很确定了。好，那这个时候呢，我又会有一个AI的功能，我根据这个病人的症状可以判断出他的中医辩证是什么。那也是把这个中医辩证按照自信度从高到低列一个列表出来，医生可以选择一个辩证。然后呢，这个辩证的后面。就紧跟着他的那个经方，就说你要用什么经方来治疗。

仇钧潼

03:29:17

啊，就是这个功劳。那医院呢？也有这个一些辅助，一些其他的功能，比如说。这个医生是谁都不服谁，比如说我是广东省中医院，我是全国排名第一的中医院，我凭什么相信你那个二流学校的老师写的那本中西医结合类科学里面的内容呢？我为什么要相信你那个我不相信。所以我第一个要求就是我要人工定义。我的治疗路径，比如说我认为这种西医疾病呢，它的症状其实不止这么一些，还可能有其他一些，然后它的中医辩证呢，也跟你这个书上说的是不一样的，可能有其他的辩证，然后这个用了经方可能也是不一样的。那这个时候呢，他可能需要有一种直观的修改的方法。

仇钧潼

03:30:03

就说我这条治疗路径按照一条路径那样去修改。那这个图数据库正好可以满足他这个愿望。那比这个说关系型数据库那种？基于那种很呆板的这个抓瓦的那种框架开发的那种程序的那个修改要漂亮很多，我直接可以在一个图界面上面就能够进行修改啊，那这个是它的第一个要求，它要这个自定义这个。治疗的那个路径啊，然后第二个要求呢，医生刚才讲过的医生。很需要的就是你的根据是什么？你给我推荐这个，你告诉我这个，比如说你告诉这个金。你的根据是什么东西？比如说你这个经方来自于某本书，来自于一个名医大家的那个开过的药方，那你要告诉他最好呢，还要用直观的方式能够浏览。

仇钧潼

03:30:54

这些相关的证据，比如说以一个知识突破的方式来浏览。好，那大家可以看到了这个知识图谱，不光可以起到作为我们的问答机器人核心的作用，它本身还可以提到一个起到一个直观的提供证据，这么一个作用啊，好，那这个是一个很重要的功能。

仇钧潼

03:31:13

而且呢，我还有。更重要的功能！就是我可以用这个知识图谱，刚才那个中西医结合的知识图谱。加上大语言模型构成一个专家问答系统。这个医生可以随意的问我任何的中西医结合的问题，只要在我这个系统里面存在这个知识就够了。那我想这个功能是非常强大。比他去搜索。什么东西？什么资料库要强大很多，就说它意味着医生具有任意的抽取信息的能力。那这个功能呢是我认为呃非常。有用的一个功能啊！还有呢，你可以用知识图谱来建立一个智能搜索，刚才不是讲过吗？知识图谱跟LAM结合呢，可以起到一个智能搜索的作用。那就是说你搜索，比如说搜索这个黄连这个药材，我不光给你黄连这个药材，我可以把黄连能治什么病，经常黄连经常跟什么药材配套，我都可以通过知识图谱来获得，从而呢把这种。

仇钧潼

03:32:20

关联的知识的有关的文章都推荐给你。然后这个知识图谱里面呢，还有一些这个边上有连接强度。那我可以根据这个连接强度呢，来决定推荐给你的那些文章的次序，那这个就解决了一个推荐次序的问题。那这些都是搜索引擎里面呢，不容易解决的问题啊，那我们也打算把这个功能呢，加入到刚才所说的这个方，问的这个中西医结合。辩病辩证施治这个模块里面。那我们目前正在做这个事情，那这个方问呢，大概也是一个月后吧。就可以把这个功能把它做好，那可以给大家尝试一下，加了知识图谱的这种系统呢是什么感觉？

仇钧潼

03:33:04

好，那这个例子呢，大概就说明了这个知识图谱跟LLAM是怎么用它来共建这个专家系统。啊，那希望大家能够理解这些。啊，我所讲的东西。那我今天上午呢就到这了哈，那大家等着下午何老师过来带着大家去实操啊！那现在还有一点时间，这个再再接受一些问题吧啊！大家在睡觉了吗？啊，各位可以可以打开问题啊！

王蓓

03:33:37

好的，唐新波同学可以提问了。

王蓓

03:33:44

诶，他好像取消了提问。

谭霁

03:33:48

哦，好的，黄老师你好，能听见我说话吗？

仇钧潼

03:33:50

啊，听到很很很洪亮，听到很！

谭霁

03:33:52

啊，好的，那个呃，其实刚才您介绍方问的后面一部分的时候。

谭霁

03:33:59

就是说感觉已经部分的回答了，就是我想提的问题，但是我还是想把这个问题就是呃提出来一下呃。

仇钧潼

03:33:59

是吧。好的。

谭霁

03:34:07

啊，就是呃，在今天今天介绍了这个制图符合那个大学模型的这个就是专家系统这个呃领域里面。

谭霁

03:34:17

就是因为我们在实践活动当中，可能经常会基基于那个局部场景，呃，有可能要呃对那个就是呃所用的知识或者是经验库，比如进行取舍。

谭霁

03:34:31

那那就是这有点类似于拍照时候的那个对焦的那个缩放的那种那种感觉啊，但是但不但不，不一定是这个。

谭霁

03:34:39

那就是随着就是呃知识图谱解决具体场景问题的时候，特别是使用知识图谱的。或者专家系统的人来说的话，他随着某些具体的信事件信息确定下来。

仇钧潼

03:34:51

嗯。

谭霁

03:34:54

呃，知识图谱的那个就是。

仇钧潼

03:34:55

嗯。

谭霁

03:34:57

就是范围就。就就会越来越就是或者说越来越小也好，或者说越来越具体也好。那原有的知识图谱当中可能部分的那种呃关系不再需要。

仇钧潼

03:35:05

嗯。

谭霁

03:35:09

那就是说我只是感觉到是不是有可能会需要采，就是有这种。

仇钧潼

03:35:10

嗯。

谭霁

03:35:15

类似于产生。

仇钧潼

03:35:17

减减掉他不要他了。

谭霁

03:35:19

啊，可能类似于这个这个概念啊，简直这个概念就是相当于啊，您您说啊。

仇钧潼

03:35:23

对这个。这个会有的。这个会有的，这个会有的，比如说像医院里面这个知图谱，那我可能具体实地化了，病人是吧，但这个病人已经出院十年了，我还要他的信息干什么？<笑声>那我就把它删掉了，那这个是常有的嫂子。

谭霁

03:35:29

呃嗯，明白，但是感觉这是一个静态的，简直就是说这个信息已经固定下来了，那我如果是。

谭霁

03:35:46

就是比如说在当前的一个呃，相对比较短的时间里面。

谭霁

03:35:50

就是产生局部场景的这种动态知识图谱。

谭霁

03:35:54

的这种，因为有的时候就说可能一个事情的原因，如果大概确定的情况下，那有可能应用的技术的这个方向会产生，就是剧烈的变化，就是完全用两两个不同的那个技术方向的这种。

谭霁

03:36:10

这种这种情况，那那这这一类这个技术就说，当然您刚才介绍方问的那个，那个时候，其实我感觉隐约隐约觉得已经。已经已经就是已经大部分在解解决了这个问题了，但我还是想问一下，就是说。呃，就是这种场景会不会就是有什么专门的研究或者技术啊，什么之类的可以去，比如有个什么关键词可以去找一找什么之类的。

仇钧潼

03:36:26

对，呃，知识图谱的维护呢，有很大一部分呢是通过手工来进行的，比如说刚才您说的这个有一些知识已经过时了，可能要删除它，那这个你能通过机器学习的方法判断一个知识是不是过时已经没用了，我觉得这个挺困难。

仇钧潼

03:36:52

我觉得还是手工维护比较多。哈哈哈哈。

谭霁

03:36:56

明白。

仇钧潼

03:36:59

好的。

谭霁

03:36:59

明白行那那谢谢您的介绍！

仇钧潼

03:37:02

啊，谢谢谢谢嗯，好！

谭霁

03:37:02

谢谢谢谢嗯！

仇钧潼

03:37:04

看有没有下一个问题啊？

王波

03:37:06

喂，王老师。

仇钧潼

03:37:07

诶，你好诶。

王波

03:37:08

啊，我还接着我刚才问的一个问题就是说，我假如有一个训练好的一个专业领域的一个模型，但它可能遗忘的比较多。

仇钧潼

03:37:11

可以啊。

王波

03:37:18

这时候呢，我就想用一个这个模型加一个通用的。这是呃呃LM这就是GLM这种模型加在它们两个一起问，然后就像刚才我问那个八个模型协调似的。

王波

03:37:32

这样来弥补一下可不可以？

仇钧潼

03:37:34

这样应该也是可以的，就说你有几个模型，有专业级别的模型，有有这种通用级别的模型，你一起问，然后再用用一个打包。

仇钧潼

03:37:44

封装把它们封装在一起，那这个是可以的。我我认为是可行的啊！

王波

03:37:49

哦哦，好，谢谢，还有就是刚才您讲那个知识图谱啊，这个知识图谱这个对话这个东西。

王波

03:37:56

嗯，就他。应用场景啊，就是有什么特征啊，比较适合这种知识图谱的对话呀，我这个。

仇钧潼

03:38:05

呃，就是要强行强强行的让他记忆住一些知识。就可控的强行记住记住一些东西，那这个时候知识图谱就比较好了，大原模型呢就是它的行为，你是不可控的。他回答什么，这个是不可控。就是你训练完他以后他能回答什么，你是不可预知的，那后面测试才知道，那你也不可能面面俱到的做到什么？100%能测试所有的这种问问题点嘛，对吧。那所以这个把握没那么高，那你需要把握成。

仇钧潼

03:38:38

很高，掌控能力很高的时候呢，那你可以用知识。知识图谱呢，其实也就是向量数据库的一个加强版。

仇钧潼

03:38:47

就向量数据库的样本之间是互相独立的知识图谱了，就互相之间不独立。其实也就那个意思。你可以把它理解为是一个更加复杂一点的这个限量数据库啊！

王波

03:39:00

哦，明白我嗯，我就是也就是问一下什么样的特征，什么样的场景，有就是什么样的特征，更加适合这种这种。

仇钧潼

03:39:06

就是不缺钱的，不缺钱的客户。

仇钧潼

03:39:11

第一个不缺钱，第二个他对这个精准度要求很高啊，那这种客户呢，就需要知识突破来满足他的要求。不然他他有个几100万的预算，你只给他花了3000块，他怎么对得住他？

王波

03:39:24

好好，谢谢黄老师！

仇钧潼

03:39:27

谢谢啊，谢谢！

田志军

03:39:29

哎，黄老师。

仇钧潼

03:39:30

诶，你好。

田志军

03:39:32

呃，就是你在这个知识图谱上面，然后呃判断了七的病症，从病症从症状到病症，然后它因为它是一个多的多的，你这个上面写的是设置自信度，你这个自信度是怎么来去进行？

田志军

03:39:47

好，算出来的。

仇钧潼

03:39:48

呃，自信度就是一个AI算出来的东西就是我的那个，你你这个病人描述的症状，它可以成一个向量嘛。比如说我用but的话GPT都可以把它变成一个成一个向量，那我也把这个。这个病典型的症状，一个向量，我们就计算它们之间的相似度啊，那大概就是这样子。那当然你也可以用原始的方法，就直接用模板匹配啊，字符串匹配的方法，那这个也可以的。

田志军

03:40:20

呃，那它这个自信度是完全是呃机器，然后来去算出来的，而不是经过人工确认的。毕竟比如说有些疾病嘛，它有35种那个症状，但是有一两种是啊，可能占据了80%到91旦出现这个病症，就能够判断这个。

仇钧潼

03:40:36

啊，这个是机器？

田志军

03:40:37

但是有些。

仇钧潼

03:40:39

这个是机器算的啊，机器算的对。

田志军

03:40:42

啊，那还有一个就是说从这个病出来了之后，你要给他一些经方经方肯定就是呃，经典的这方子，然后他这都是已经什么病。

田志军

03:40:53

种什么方案已经确定好的，但是一般的。

田志军

03:40:57

啊，它也有有为一个疾病，它也有好多个经方，那你怎么从这三个经方里面或者几个经方里面选一个呢？

仇钧潼

03:41:05

呃，这个医生这个是属于医生的责任呢，就是医生会选择一个经方，那我们当然也可以注明这个优先次序。

仇钧潼

03:41:14

啊，就是说啊，你优先用这个方子效果比较好，这个第二可以用这个方，第三用这个方子，那我们也可以采用这个办法，但是我们不打算背这个责任，<笑声>，这还是留给医院跟医生去干吧，这个事。

田志军

03:41:26

您的意思就是我这个是给医生的，我把全量的都给你出来，你选哪一个，你自己来去控制，是这意思吧？

仇钧潼

03:41:32

对，没错误，但是我们现在打算呢，就是只推荐一个经方，就首选方就说如果有1233个经方，我都明显知道第一个比第二个好了，我为什么还要把第二个放上去呢？对吧，那我就放第一个就够了。我就免得这个医生把给给弄糊涂了，就就选一个首选。那这个首选经方呢，一般是要经过他们医院的这个审核，就是可能他们有一个专家组来审核这个治疗路径是不是合适啊，才能确定下来？

田志军

03:42:04

对，我刚才的问题就是你选择这个策略。呃，专家专家那个策略完了之后你肯定做一个规则或者做一个什么，然后辅助你来去选择这个东西出来嘛。

田志军

03:42:16

就这个东西啊啊？

仇钧潼

03:42:16

对对。这个我们没没打算用人工智能的功能，因为警方本来就不多。哈哈，通常就一个。就就是你看那个中西医结合内科学里面呢，基本上每一个西医的疾病，你只要把它的中医的辩证变出来了，一般就是一个经方啊。

田志军

03:42:35

嗯。ok，那我还有一个问题啊，就是说我们在，呃，用大元模型肯定是与人来去进行交互啊，就是问你一些问题，然后不断来去进行回答，就是呃，大语言它这个怎么能够知道用一般的望闻问切就是你的。

田志军

03:42:52

疾病症状体征嘛，你肯定是一个一个，然后来去进行问他引导，那那他怎么知道说我问了这一个问题之后，下面要问另外一个问题，然后根据他的。

田志军

03:43:04

他的症状要另问另外一个问题，这种一步步的人的这个流程，这个是啊，有我们有能控制吗？还是大圆模型自己控制？

仇钧潼

03:43:09

呃，这个是这样的，就是我觉得这个不应该是方问的功能，就方问里面不包括这个功能，这个是属于医生的基础知识，就是说他在医学院学习的时候，应该就知道这些东西，或者说在他的实习阶段，他重点学习的就这些东西，这个不是方问帮他解决的问题。如果医生连怎么问病人？怎么记录都不知道的话呢？这个医生就不大合格了，就不，不应该坐在那个位置上。我们方问做的这个呢，就是你问完病人了以后，你总结出他的这个主要的主诉的症状，再输入到方问里面，这个前面这个环节就交给医生去做了。

田志军

03:43:52

那就相当变成一个知识库了。

仇钧潼

03:43:54

啊，可以这么理解，对可以这么理解，就是现在这个机器人看病，这全AI看病呢，目前国家还是不允许的。

仇钧潼

03:44:03

就是在政策上是不允许的，所以我们没有打算开发这么自动化的功能。

田志军

03:44:10

嗯呃，那最后一个问题啊，就是说。

田志军

03:44:13

呃，用户医生这边来去帮我们来去输入他问诊的这些东西之后，那么我们要知道我们的疾病那个制图谱里面去去查嘛，那个去查查的这种范围是查一层，查两层还是说查多少？

田志军

03:44:30

这个我们有控制嘛，还说全量查完给给出来。

仇钧潼

03:44:31

呃，这个我们是要控制的，因为你全量查的时候呢太多了，那这个L LOL没法办，帮你总结那么多东西，那我们可能就查临近的一两圈这个样子。

田志军

03:44:46

嗯，这个是可以，就是我们去调用那个。

仇钧潼

03:44:48

控制的可以算法和控制的啊，可以算法控制，对可以控制。

田志军

03:44:49

自己控制是吧？好好那，那我没什么问题，看其他同学。

仇钧潼

03:44:54

好，谢谢谢谢！

王波

03:44:57

喂，黄老师。

仇钧潼

03:44:58

诶，你好诶，听到。

王波

03:44:59

我还有个问题啊！

王波

03:45:00

就是那个刚才这个方问系统啊，我想你们这个有没有可能加上这个呃，GPT a的RL HF这种功能啊，就是给让医生给这个女方问打分来，逐渐提高你们的东西。

仇钧潼

03:45:14

对，我觉得这个这个挺好的，就我们是呃，在因为因为我们这个课是AI课嘛，所以我在这个课里面呢，也不提这个方，问本身的IT功能，那其实IT功能也是挺丰富的，比如说我们呃可以允许这个医生收藏这个处方。可以对这个处方进行分享，也可以对这个自己开出的处方，跟别人分享的处方进行一定的评价啊，这些功能都是有的，那我们也会根据这些评价呢，来对我们系统的功能，还有这些推荐的处方来进行一些改进。啊，这些已经有了这个功能。

王波

03:45:49

哦，好好，谢谢！

仇钧潼

03:45:50

嗯，好，谢谢。

仇钧潼

03:45:51

大家还有问题吗啊？

许涛

03:45:54

王老师那个我想问一下这个知识图谱里边咱们讲的是三元组嘛，然后就是那个头和尾，然后加你关系是吧？但是我考虑是不是呃，还有这种场景，就是他这个关系是不是不会要带一些权重什么的，是不是有这种？

仇钧潼

03:45:54

诶，你好啊。

仇钧潼

03:46:10

啊，关系是能传递是吧？

许涛

03:46:14

对，就权重。

仇钧潼

03:46:15

关系有权重会有的，就是有一些关就关系本身也是可以带有属性的，比如说这个关系呢，可能是夫妻关系，那你我可以把一个一个结婚时间把它记录下来，对吧？它可以带有属性的。

许涛

03:46:18

就不用。

仇钧潼

03:46:29

那个有一种叫法，我看到那个阿里写的那本书就什么企业级知图谱。那本书里面呢，他提到有一种说法叫做边实体，他把边也看成是一个实体，也可以给他定各种各样的属性。

仇钧潼

03:46:43

所以呢，这个有很多不同的这个知识图谱的玩家跟流派啊，不同的这个玩家的玩的方法都不一样，用的基础软件也不一样，这个呃确实有很大的差别的啊。

许涛

03:46:54

他这种带权证的这种编就用咱们现在说的这这这这些个方式能能处理吗？或者有没有什么其他的方式，论文什么的。

仇钧潼

03:47:02

呃，什么来着没听清楚？

许涛

03:47:05

就是带有这种权重处理的这种知识突破。

仇钧潼

03:47:09

我们就可以可以放全中了。的边是可以带属性的。

许涛

03:47:16

呃，那那从那个就是从学习的角度，就是从那个就是图神经网络，它那个学习的角度它也是可以去处理的是吧？

许涛

03:47:27

就是他不是。

许涛

03:47:29

就是。呃，刚才就在那个不是嵌入到一个什么K的一个空间里边，就是那个。

仇钧潼

03:47:36

啊，穿一穿一也是可以处理啊，这个就是有各种的算法，有一些呢，可能在六的算法库里面有一些呢，可能有一些python的库，但是你可以连接这个六来做这件事情啊，那这些都是可以方便实现。刚才讲了两种，一个穿衣，一个穿穿穿11跟X都有，这个算法都有，而且代码都有。

许涛

03:47:59

嗯，比如说我在。

仇钧潼

03:47:59

啊，下午何老师会给大家。嗯。

许涛

03:48:02

呃，然后就是你，你把那个这个这个PPT再转到那个穿E的那个那个那个学习的那，那那个页上去就有一个小的地方，我不知道是不是。那么理解。

仇钧潼

03:48:15

我这里没有讲。

许涛

03:48:17

应该是在。不不是的，那个学习的那那那个？

许涛

03:48:22

一堆数学公式那个里边？哎，对对对，过了，过了过了。

仇钧潼

03:48:26

这个吗过了？这个。

许涛

03:48:27

过了过了不再往下，再往下再往下再往下。

仇钧潼

03:48:28

对了。

许涛

03:48:31

再往下来这这这。那就刚才上上也上也对，这里就是那个右下角那个公式，那个公式，那不是就是一个D，然后一个L后边也是一个减去了一个括号的一个一个L嘛，这俩L都是它都是不带撇的，是因为它这个关系是可枚举的，是不是这意思？

仇钧潼

03:48:33

这个。A L的话就是关系了，这个就是关系了，这在这里就是已经嵌入到这个K V空间里面，变成一个向量。就是说这个。

许涛

03:48:55

因为我是看到。

仇钧潼

03:48:56

投实体加上这个关系的向量以后呢，应该是跟后面这个伪实体应该是相同的，它这个损失函数就是这个意思，它们要尽量的接近。你你不能相同，也不能离得太远，他们要尽量的接近，越近越好啊，它是这个意思。

许涛

03:49:12

因因为我是看到它后边这个是带H撇还是撇，那这个这个L它没没加撇儿这个。就后后后边这种？

仇钧潼

03:49:18

这个LL的话刚才解释过了，我这里可能没写L吗？我这里写了哦，我写成R了。

仇钧潼

03:49:26

他其实在在这个论文里面，他叫我这里就写成L这个地方写成L。这个L就是R是一个意思啊！

许涛

03:49:32

我知道，我就说它后边这个L不需要加撇什么这么。

仇钧潼

03:49:38

就后面这一节里面的？

许涛

03:49:41

啊我我发。

仇钧潼

03:49:41

跟前面这个L是一样的意思？但是这个实体是H撇跟T撇的，就跟前面这个HT是不一样的，H撇跟T撇是随意抽出来的。

仇钧潼

03:49:51

是随意抽。

许涛

03:49:51

哦，我就想了解它为什么后边这个L它没有加撇儿，这个不知道为什么。

仇钧潼

03:49:55

<笑声>那那这个如果指定一种关系，我随意抽两个实体，他们是这一种关系的，可能性就很低了，对吧。但是如果这个AO也与你允许撇的话呢，这个我就不好不好说这个事情。就是它主要还还是为了跟前面这个比较吧。

许涛

03:50:10

是不是因为？

仇钧潼

03:50:14

就是说因为前面这个包含有L后面这个要有AO，那你才可以组成一个统一的一个省次函数啊，对吧？

许涛

03:50:23

就在同一个关系的这种前提下是。

仇钧潼

03:50:26

对，没错，同一个关系，这个前提底下啊！就是相当于这个三元组损失里面一个锚点。

仇钧潼

03:50:35

一个锚点。

许涛

03:50:39

没有问题啊，谢老师！

仇钧潼

03:50:41

好的，谢谢。那大家没有什么问题的话呢，我们今天的课也到这了，就今天上午的课到这里，因为很快就要何老师来上课了，大家也要吃饭了。好，再见啊，我们下一次特训营再见，谢谢！啊，再见。我们来现场点屏蔽可以拍照，这个是怎么关的，关这个？稍等，稍等。