# Day1b
关键词

神经网络

向量数据库

关系型数据库

知识图谱

最短路径

推荐系统

图数据库

查询语句

点击记录

图机器学习

翠仪

07:19

没听得到吗？那个下午的课程开始了，呃，这个不好意思，刚刚有一些问题调试了一下设备，然后这个我们现在开始下午的一些课程吧，呃，就是上午黄老师讲的这个呃，各种的一个。讲的向量数据库啊，图数据还没有讲，那可能有些同学呢还没有明白，就是说这个向量数据库跟我们传统的这个关系型数据库有什么不一样。呃，其实的话我们可以大体上可以把这个数据库分为这个关系型数据库和。

翠仪

07:55

关系型数据库那关系型数据库的话我就不多了解了，大家都知道那都是口的一个数据库的话呢，呃，其实就是呃考虑到有一些特殊的一些情况，它这个关系型数据库它虽然能实现。但是你用起来就是非常的一个别扭的一个情况，比如说像向量数据库啊，你大家也可以把所有的这个，呃，<外文>的这个向量存到这个数据库里面，但是你每次查询的时候，你就需要把这个，呃。这个相当啊，你从这个数据库里面都查找出来，然后再去实现一下这个啊，余弦夹角都要算一下它们彼此之间的距离就是显得非常麻烦，不直观。所以后续呢就有了这个销量数据库的一个存在。

翠仪

08:41

那这个非关系型数据库的话，其实主流的有这几类，呃，啊，当然还有其他的那个非关系型数据库，像这个蒙o DB这样子的，那我们在这里主要是讲这个像那个数据库和这个图。数据库啊，就是跟我们后续课程比较相关的。那向量数据库它的一个特点呢，就是说我们主要存储的是一个向量，它就是在数据库层面呢，就可以实现这个向量之间的一个相似度搜索啊。还可以做一些这个聚类呀，降维等等的一些算法操作它内置在里面的，所以就是我们不需要在这个呃，算法层面去实现，我们直接可以在数据库层面去实现这些算法，然后它非常方便的去处理这个高维度。

翠仪

09:29

高相似度高并发量的数据，它在这个比如说像这个相似度搜索的时候，我们用像是相当数据库去查找某一个这个相似的一个向量的话，它的一个效率呢，是肯定是远高于这个关系型数据库的。因为传统的关键性数据库的话，你要从这个所有的这个数据里面去导入啊，这个去再算一遍啊，这个计算量其实还是蛮大的，那这个向量数据库的话，会对他们的这个相似度呢，去做了一定的优化。其实不是这个全部比较的，它是有这个相应的一个搜索算法的。然后的话呢，但是呢，销售数据库这个概念呢，提出来的这个时间呢还是比较短的，所以它这个目前各个产品的话，它的一个基础层入度呢相对比较低，然后呃，待会我们也可以看到一些。

翠仪

10:22

呃，现在发展的比较好，比较知名的一个向量数据库，那图数据库的话主要是这个知识图谱兴起了之后的话呢，呃，因为知识图谱本身它是一个图的一些情况啊，那你也可以用关系型数据库存储。因为一开始图书呃知识图谱提出来的话，它这都是一个IDF的一个三元组，那你可以存到这个呃关系人数据库里面，但是有一些这个图的算法，你在关系型数据库里面实现也很别扭，所以说的话呢？呃，这个图数据库它天然就呃，有这种这个在图上去进行这个推理的一个呃优化的一个算法，所以说的话呢，呃，它对于这种知识图谱的存取的效果是比较好。的，而且是比较高效的，特别是你构建了知识图谱之后，你需要在知识图谱上面去进行这个知识推理啊，这个呃，这个像图网络啊等等这些。但它也有缺点，就是说，如果你本身这个关系比较简单啊，这个复杂性非常低的话，你可能用图书就会有点这个杀鸡用牛刀的感觉，所以它比较适合那些，稍微就是关系比较。

翠仪

11:40

复杂一些，呃，这个节点数起码多一点，不然你直接用个mysql或者去存这个呃它的一个知识图谱就非常呃就可以了。那除此之外的话，还有一种叫做文档储存数据库啊，跟我们的课程呢相关度不是很大，所以我就这个。不多做介绍了。

翠仪

11:58

那向量数据的话呢，其实呃，我们所有的这个相当数据库里面存的都是一个，呃，通过技术将一些图像啊，声音的文本表达为一个高维度的这个向量，那这个维呃，向的维度的话，其实要看你的这个<外文>的这个模型。啊，你可以设定它是512位的。呃，B的话一般都是756位，然后还有这个可能高一点的这个1024维。然后如果像这个charge LM的话，它的一个的话是1500。呃，26位好像是这个数字？

翠仪

12:36

然后的话呢，相应数据库它最开始并不是在这个呃LLP里面去用的。最开始的一个需求呢，是在这个图像上去用的，比如说，呃，你要去做人脸识别人脸对比，你不可能说把你每个这个人脸的图片存储到这个。呃，数据库里面，然后每次要去找人脸的时候都把这张图图找出来，读出来以后两个图去对比一下，这样非常的低效，而且这个有很多这个重复的工作，所以说的话呢，当初就是呃这个设计这个项目数据库呢就是为了。

翠仪

13:12

去储存这种图像的一个向量，然后当我们的这个像这个人脸图片，呃，它上传了之后，它可以通过一个<外文>的一个模型，把它转化成一个高维的一个向量，然后存储起来，那下次你去做人脸比对的时候。呃，你新传的这个人脸的图片也可以通，呃，通过这个转化成一个向量，那就是两个向量之间的一个计算了，向量之间的计算的话，就两个向量计算，就很快就是人脸对比，然后人脸查找的话你。无非就是一个向量和呃这个一组向量之间的一个距离的一个计算，那这也呃会比你啊，这个把图像先转换成一个向量，或者是说把两个图像存到啊，放到这个输入到一个神经网络去算它们相似度。话也很多，所以就是向量数据库最开始的一个用法，就是用来去把这种图像做成一个向量化。那后面的话，这个包括最近这个T GLM啊和这个GG PT啊PT。

翠仪

14:20

这个技术火起来之后呢，大家就是把这个向量数据库也这个考虑到这个文本方向了，那事实上文本的这个向量化的话，在很久以前也已经有，但是可能之前的一个需求度不是很高。因为一般情况下来说，我们并没有那么多预先储存好的一个语料，需要去做这样的一个向量计算的。那就这个可能以前的技术更多的是在于说，呃，我可能有两个句子，它都是一个文本形式的，然后我通过这个标的技术去计算它们句子之间的一个相似度啊，可能是这样的一个用法。

翠仪

15:00

然后呃，就是它的一个，呃，基本的一个介绍，大家也可以详细的去看一下它的一个核心技术的话呢，是这个第一个是<外文>的一技术，然后早上也看到很多同学问这个<外文>的一个模型到底是怎么设的？其实<外文>的一个模型的话，有很多就具体要看你针对的这个，呃，储存的一个数据类型啊，像文本的话就，呃，本身<外文>系列的一个模型，它本身就是一个编码类的一个模型，它所以它天然的就可以把。这个呃文本都转成一个这个高维空间中的一个影像量，所有的一个<外文>系列的一个模型都可以这么做的。那反观这个GPT的话，其实它是一个decode的一个呃维度，所以它去做的话可能还需要去做一个转换，然后还有像这个呃encode和decode都去做的像呃比较早期一点的T五。还有这个GLM它其实都是，呃，把这个，呃，<外文>和<外文>的一个部分都包括了，那它们也可以去做这个<外文>的一个模型，那我们去做<外文>的话，就文本的<外文>的话也是非常简单。

翠仪

16:14

啊，待会会介绍一个这个线程的一个框架，那你不用这个<外文>的一个框架，你可以直接用这个<外文>的那个<外文>啊，就直接这个有模型权，就我就可以去把它去啊，做下面的一个<外文>。当这个<外文>的一个效果的话，就见仁见智了，因为不同的模型它适用于不同的一个场景，然后呃，很多这个大语言的一个模型，或者是很多预训练的一个<外文>模型，它针对的都是这个通用领域。所以就是说，呃，你一个子领域在一个大的一个通用领域里面，可能每一个句子都非常相似啊，所以就是在这种程度上的话，可能后期还需要去进行一个模型的一个，呃，追加训练啊，或者是微调去让它了。

翠仪

17:03

把这个呃相似度的一个计算<外文>的一个呃，向量呢，可以有更大的一个区分度。然后呃，就是呃，这是可以是向量数据库去做的，也可以是我们前期在自己的一个用自己的一个算法去实现的，然后向阳座椅和这个架构的话，其实都是这个目前向量数据。

翠仪

17:27

我所需要考虑的一个技术，那因为它的一个维度呢，往往是比较高的，所以它直接去进行扫描啊，这个全量扫描或者基于数结构的这种传统方法的一个扫描呢，会导致效率非常低下，而且这个。内存呢会非常多，所以它一般都会用近视搜索算法来加速它的一个向量的一个检索。那呃通常的话呢，我们现在这创呃去查这个向当数据库可以呃非常通呃都支持的一个算法就是。哼哼。找出根据他们的相似度角是最近的K一个向量啊，那你可以根据这个欧式距离啊，鱼线夹角啊。等等的，其他的一些计算。然后现在那个相当数据库的话，基本上因为它提出的这个时间比较新，所以它基本上都是采用这个分布式的一个架构的，呃，可以满足各种用这个你可以是一个图的相当数据库，可以是文本的相当数据库，可以是音频的相当的数据。都可以去。哼哼。然后它也会用到一些这个硬件加速的一个方法，因为这个向量在CPU里面算起来肯定是没有GPU里面去算的，所以它们也会有一些这个硬件加速的一个解决方案，那就是它的一个，呃，核心的一个技术部分。那最重要的主要是这个，它的一个缩影和这个分布式的一个架构。好，这里是讲了这个imbalance的一个步骤，然后早上这个黄老师也说过了啊，那我这里就啊，不再详说了。

翠仪

19:12

然后它的一个优势和缺点其实也是相对比较明显的，因为它可以处理非常大量的一个数据，它都是这个向量它，呃，对于向量之间的一个计算的话，它肯定会比关系型数据库。更快，然后这个呃也支持一些这个向量之间的一个算法啊，比如说像这个聚类啊，这个呃分类啊等等的一些啊，它可以去支持一些呃复杂一点的一个呃查询。但是的话，因为它技术还是比较新，然后市面上的这个产品的话，成熟度了，也就你跟这个oracle啊，或者买些口腔的一个数据库相比起来呢，可能这个真的比不上它的稳定性上面，然后。

翠仪

20:00

还需要去学习一个新的一个数据库，因为它的一个查询语言也不像这个C口这样子啊，比较统一的一个查询语言，它有自己的一个查询方案。啊，所以而且它的一个适用场景其实是比较局限，它只适合，呃，用这种啊，有这个向量化需求的一个数据，那普通的一些数据你放到项目数据库都是没有什么意义的。

翠仪

20:27

然后就是这个区别啊，这这简单列了一下，大家就进去可以细看一下。然后它的一个发展趋势和应用场景的话，就是说，呃，其实现在这个GPD火了起来之后，它的一个应用场景还是，呃，挺多的。像已经有的应用场景，像这个人脸识别推荐系统啊，这个图片，指纹呢等等这些文件搜索都可以去做。

翠仪

21:02

然后它的一个产业链的话，目前的一个产业链分布的话呢，呃，大概是有这些产品啊，国内的话像这个达摩院啊，蚂蚁金服啊，阿里巴巴他们都有自己的一个这个呃说是自研的一个向上数据库。然后它呃京东也有，然后这个。<外文>也有开源的一个相当数据库，也有这个商业的数呃相当数据库，然后给大家介绍一下这个像相对比较成熟的呃，向当数据库，然后这个是一个开源的，它是一个分布式的。那目前来说的话呢，这个呃。这款呃，这个它的这个成熟度相对会比较高一些，然后在上面的一个呃讨论度呢，也相对于来说是这几款项要素里面最高的可能它是做的比较早，然后它整个功能呢？还是比较完善的。哼哼。

翠仪

22:11

好，下面这个呃是呃，它也是一个呃，云原生的一个向量数据库，但是它是一个商业版本的，所以它只会提供这个API的方式，它整个数据库都是架在这个云上面的，你如果就不想自己。

翠仪

22:27

搭一个数据库，你可以直接去购买它的一个服务，它会提供给你一个非常简易通用的一个API有所有的一个操作，比如说像这个我要创建一个呃，像那种数据库啊，我要把数据库插入进去。啊，我要删除，我要查询都可以通过这个API去完善啊，它是呃，但是它不是一个开源，它是一个闭源的一个商业化的一个产品。

翠仪

22:56

然后还有就是<外文>呃，这个的话呢也呃，这个也是相对来说比较新的一个相应数据库，那它的一个优点是什么呢？就是它有比较多的一个呃相关的一个案例，呃，可以去供大家去参考。它也提供了很多这个，呃，向量数据去查询相关的一个呃，方法位置在这个数据库里面，所以就是大家去用的时候呢，呃。可以非常方便，比如说像这个<外文>，它也有内置的一个算法，所以就是你再去写数据的时候呢，你可能不需要先把数据去做一个这个<外文>，然后你可以直接把数据传上去，然后它可以自动去进行这个向量化。就。就是非常的一个简单方便，然后后续的像这个呃相似度查询啊，聚类啊，这个呃降维啊，他们都有提供这个算法，那这个也是一个开源的算法，但是它因为是你看它的公司成立于这个19年。相对来说呃还是比较新的一个相当数据库。

翠仪

24:08

然后还有一个就是呃，Q，这个是一个比较小型的一个向量数据库，它只专注于这个向量搜索和相似性，收的相对是一个比较呃，轻一点的一个向量数据库。然后呢。呃，它主要是做这个查询，就是这个相当搜索和乡村度查询，如果我们使用的一个场景，就只有这种相似度的一个搜索的话，那么我，呃，个人觉得用一个轻量一点的一个数据库呢，也未免不可。

翠仪

24:41

然后他。它它的话呢也是一个开源的一个数据库，你，呃，它的官网上呢也提供了一个这个呃云数据库的一个版本。当然呃，你要用一个比较大型的一个数据库的话，就是你存储的数据比较多的话呢。可能也需要去付费啊，但是呢呃，你可以在它官网上去注册一个账号，他现在是有呃，每个账号的话都有一个这个小型的，大概是EG b啊，库存的一个相当数据库，供大家去直接去使用。

翠仪

25:16

用，然后这个下面我们讲的一个案例，例子的话也会基于这个轻量级一点的这个qjoy这个呃，相当数据库，那大家的话呢？

翠仪

25:28

可以去呃打开它的官网注册一个账号就可以了。它的一个使用是非常简单的，就完全不需要自己搭建。呃，在我已经是一个登录状态了，因为我已经申请过了，呃，就是。就是大家呢呃呃当看到它官网的时候呢，你可以就是呃staff here啊，它会让你去注册一个账号，注册完账号之后的话呢，会让你去呃创建一下你的这个呃。Caser就是一个集群啊，就相当于是一个数据库啊，你随便说一个数据库的一个名称，然后它里面的一个资源大概就是会有0.5B的一个CPU然后EGB的一个内存，然后还有。呃，20G的一个硬盘啊，大概是这样的一个节点。那在这个数据库上面的话，你可以去区分不同的一个，呃，data的一个基础，就相当于其实在上面去，呃，同一个数据库上面去。储存的不同的一个方向的一个数据类型，然后的话呢，创建新的数据库的话，你只需要去呃，写一下它的一个名称，然后这里也可以选一下它的一个数据呃，的一个。物理的一个地点，那这个地点都无所谓，反正都是这个美国那边的，然后呃输完之后的话，你就可以创建一个自己的一个小型的一个相关数据库。

翠仪

27:00

然后这个数据库的话，访问的话，它会需要有一个呃API的一个key你可以在这个。数据资产里面去看到这个API的一个key的一个内容，就是如果你，呃忘了的话呢，你可以就直接就点一个<外文>就可以新成呃，新生成一个。然后就是在这里去选择你对应的一个数据库，然后去新生成一个。然后这个的话呢，这个API key只有在你生成的时候呢，你复制下来了，然后你可以看到它的一个全部内容，就是一旦你关闭了那个复制窗口的话。它就是永远都是一个呃，加密的一个状态显示，就是你不能看它第二次，所以你呃创建了一个key之后，你记得要把它保存起来，不然的话你只能把它删掉，然后重新再创建一个。当你可以碰见多个key啊，都是一样的。嗯。

翠仪

28:10

那这是这个相关数据库，那大家趁着现在有空的话，也可以去先去申请一个账号，因为待会我们的一个演示啊，包括这个呃，早上黄老师所说的一个非常简单版本的一个问答机器人的话，我们。都会用到这个相当数据库去储存我们的这个的一个数据信息。那如果是有听过上一次这个特训营的同学，应该也有创建过这个账号的，因为上次我们用的也是这一个呃，相当数据库。

翠仪

28:48

然后就是说我们这个做这个相关数据库啊，很重要的，第一步是要做这个，呃，数据的一个<外文>，然后刚刚说了啊，其实<外文>这一步的话呢，呃。在这个早上的话是只讲了这个what we what we其实它在这个去做向上画的时候呢，其实是有点呃不够实用的，因为通常我们需要向上画的是一段文字或者是一个句子。那如果我们只是做<外文>的话呢，其实我们就会涉及到一个问题，就是首先我们需要分时。分成了之后呢，我们这个每个句子或者是每一段话，它去做这个<外文>的时候，如果得到一个大小不一的一个，呃，向量矩阵，那这时候这个向量矩阵之间的一个运算，你就会显得。非常麻烦，你到底是怎么去算这个两个句子之间的相似度呢？因为我们再去做这个向量比较的时候，我们不是比较两个词相不相识，我们是比较两个句子啊，特别是我们这个用户输入的问句。和我们储存在这个数据库里面的一个呃，知识的一个内容之间的一个相似性，而不是说两个这个。词之间的相似相似性，所以就是what和这种技术的话，现在基本上很少用，是大家基本上现在用的话都是一个text，是直接把这个句子呢转换成一个向量。就是把这个它句子转换成一个语义空间上的一个向量，那这样我们就可以直接去计算这个句子之间，或者是这个，呃，某一段文字之间，它们的一个相似性的。

翠仪

30:41

那在这里的话就不得不提一下这个<外文>的一个<外文>的这个包，那上次特意运营呢，特意有一天呢，是给大家去介绍了这个包的一个使用，因为，呃，在做这个<外文>系列或者是这个。GPD系列的话呢？transformers这个包呃是非常的好用，因为它是一个集合了基本上现在所有开源的一个NLP类的大模型的一个呃。

翠仪

31:12

这个社区。

翠仪

31:17

那在上面的话呢，最开始呢，它只专注于NLP相关的一个模型，那现在就逐渐发展起来的话，它也有这个计算机视觉类的。呃，还有这个音频类的，还有其他的一些。类别的一个模型啊，那主要还是以这个呃，LP类的，所以说比如说像这个<外文>它也放在了这个<外文>上面。哼哼。那呃，大家呃，大家可以如果去看这个charge IM二的一个官方使用，说明一个简单的一个接口调用的话，它其实是直接通过这个transformance这个包呢，去把它的一个权重给下载下来啊，但一般就。

翠仪

32:02

在国内的环境的话，可能就不太建议这么多，因为首先啊，大家可以看到它这个模型权重一共七个，然后这个一到六号的基本上是接近两G的一个大小，然后最后一个可能小一点就是EG啊比。这个。一的一个版本呢，少了一个模型的一个呃分块，呃，就是这个内存的呃这个模型大小相对于呃那一来说就减了一点点，但其实这个显存占比的话，大概也是13居中。Do的一个内容呃，就大概是这样的一个显存占比。呃，效果倒也差不多。然后像呃，我们常用的一些模型都可以在hungerface上面去找到这个呃，模型权重的一个下载，因为权重不只是这个呃，它的一个训练权重，还有其他的一些，比如说像这个托克兰的一个model啊。然后这个它的一些配置文件呢，都在上面可以下载到，那我们在使用到的时候呢，只需要一个简单的命令，就可以去切换到不同的一个模型去进行使用，它都是统计的一个接口啊。所以对于我。去使用一些这个，呃，<外文>系列的一个包来说，或者是其他的这个任务的一个包来说，呃，包括到我们后面待会要讲的这个test这个框架上来说的话，都是一个非常方便，简单。

翠仪

33:33

单的一个存在那这这个？啊，我就不再详细的去介绍这个transformers这个拓展包是怎么用的啊，因为呃，上次讲就讲过了，我就不重复了，如果上次没有参加的同学的话，也可以自己去呃，收一下，就他的一个教程还是挺。

翠仪

33:54

多的。然后它是非常多的一个语言模型。然后<外文>这个是<外文>同样也是在这个，呃，<外文>上面有提供这个权重，那这个<外文>的话，它其实与其说是一个模型，不如说是一个框架，它就是统一了不同的这个。呃，模型啊，去做这个<外文>这样的一个文本向量表示的一个模型，那它本身有提供呃，这个。两大类的，然后第二单也它它也有提供这个what weather的。但是呃我们之前what weather的话基本上都是谷歌的一个模型，它这里提供的是这个更适合中国人使用的。呃，用这个腾讯这种。<外文>由这个腾讯所这个开源的一个呃，大规模的一个中文模型啊，然后大家呃，大家如果需要的话也可以去呃，下载它的一个呃，全球文件去使用，然后这里呢主要去介绍。到这个呃，它提供了两类模型，一个是利用这个sentence word呃去做的，去训练得到的一个呃，这个相当化模型，然后它主要呢是直接将这个句子呢。呃，向量做鱼弦，然后通过这个<外文>这个模型呢，去进行预训练和预测的。

翠仪

35:31

啊，下面这个是这个CS sentence呢，它这个其实整个的一个架构也还是基于这个but模型，只是它的一个损失函数呢，再去训练的时候呢，有点不一样啊，它用了一个呃排序的一个损失函数。那让它的一个预测结果呢，可以更加好的去评估这个句子之间的一个相似性，然后它的效果呢会比这个呃sentence的要好一些。那这个<外文>评测的效果，大家也可以去看一下它的一个比对结果，它有提供这个英文数据集，中文数据集的一个<外文>评测的一个结果。然后它本质上的话，基于的基础击中模型的话是这个哈工大的一个呃but模型。然后它的一个模型权重的话呢，你可以在这个，呃<外文>上面去下载到，就比如说我要用这个。其中一个你点过去的话就会转跳到这个黑根费上面的一个呃，模型权重啊，当然你这个直接调用这个框架的话，它也会直接去进行这个下载。啊，当然最方便的一个方法就是先下载完，因为呃你直接用这个代码去下载的话这个。一般来说国内的环境的话，它可能是连不上这个<外文>或者是下载失败啊，如果你用呃，预先下载了之后，用本地模型调用的话呢，可能会比较简单一些。

翠仪

37:11

然后它的一个使用呢，其实也非常简单啊，大家如果呃想测试一下效果又不想不的话呢，你可以直接去。

翠仪

37:22

黑根这上面呢，它有一个这个在线试用的一个过程。就可以直接在这里输两个句子，然后它会直接去输出这个句子之间的一个相似性，比如说，呃，它用这两个，然后它会有一个相似性的一个结果。那呃，但可能有同学会问，我们不是要做这个test sweater就只要把它做一个就好了，为什么它这里算的一个这个是这个句子相似度呢？呃，是因为呢，它这个模型其实它的一个下游任务就是要判断的。跟句子是否相似，那这样的情况下，在这个呃，所以他通过这样的一个下午用户训练得到了一个呃。形状态的一个向量的话，应该更能表征这个句子的一个语义信息，所以我们在做的时候呢，只要截取到它的一个就模型在算最后结果之前的那一层的一个这个向量。好，把它提出来，作为它的一个标准向量就可以了，就非常的一个简单。然后至于两个句子之间是否相似的这个结果的话，我们也可以就是，呃。

翠仪

38:43

不就是不太关注这个内容？然后这个户的话呢，你如果使用方便的话，你可以去安装就P IP安装一下就好。如果你觉得这个呃不需要用，我只要用这个transformers也可以啊，不一定要安装这个new transformers也一样可以达到这个效果，其实就是相当于这个who，它就是封装了一些这个。函数而已。那我们可以看一下，就是这个。它是的一个样例，它有使用非常简单，就是我们只需要就是加载这个模型。呃，就是它有两个模型，就是如果你用这个就是一个sentence model。如果你是用<外文>的话，它直接可以去加载一个<外文>，然后这个它调用的时候也非常简单，初始化的时候呢。

翠仪

39:51

不说话的时候的话呢？我们只需要在这个<外文>model里面输入我们的这个呃模型的一个权重就可以了，然后模型权重的话我也放到了这个。啊百度云上面？呃，这个是这个不是一个官方的一个模型权重，就是不是它上传的一个模型权重？呃，是经过了这个，在它基础上去经过了微调的，就专门适用于这个医疗领域的一个<外文>的一个模型，所以就是跟它这个官方上面所。提供的这个模型呢都不太一样，所以它就更专注于这个医疗领域的一个呃。

翠仪

40:43

然后我们，呃加载的话直接用一个语句呢，它就可以自动的去搜索到这个我们本地的一个模型，就是我们的一个模型路径，然后加载完之后的话呢，我们这个encode呢，其实就是一个encode。那我们可以直接用mo？点，然后输入到我们的一个文本，我就可以去呃，获取到它的一个向量化的一个结果了。那这就是这个句子的一个向量化的结果。那这个句子的一个维度的话呢，我们也可以看到它是这个做成了768位。那它本身提供的这个模型的话呢？它有这个就是一个base版的一个模型，是768位有一个大板的模型的话呢，是这个呃1024位的一个呃维度大小，大家可以根据自己的一个需求的去选。的这个模型。那在这里的话，这就是当我们去对一堆的一个句子去进行这个<外文>的时候呢，也可以直接把这个句子的一个列表输入进去，它都可以直接去做这个<外文>。就是这个<外文>，那就关于他这个最开始训练的语料应该是关于这个阿里之前的一个比赛，那就他们的一个语音相似度的哦，忘了这个比赛就很明智了。然后的话呢，如果你没有装这个<外文>这个包的话，你用这个<外文>同样也可以去使用，只不过这个代码呢，稍微呢复杂一点点。

翠仪

42:39

就是在这里的话呢，我们直接用这个<外文>的一个<外文>的一个<外文>和这个<外文>去进行这个模型的一个加载。然后这里定义了一个这个，最终它的一个输出和这个attention must的一个呃。计算它其实就是这个模型，怎么说呢？就是我们输入的这个句子。我们会通过这个token来去转换成一个这个呃token的一个列表啊，这个列表输入到这个模型里面的话呢，它就会。它就会转换成这个向量。那这个向量的维度呢，其实是跟我们token的一个长度是一样的。就比如说你一个token啊，去转换成这个768为，那我们一共有十个token的话呢，我们就可以有一个十乘768的一个矩阵。那这时候的话呢，我们就需要把这个句子算出来的话，就求这十个to。

翠仪

43:47

它的一个呃。Imbalent的一个平均值这样去算，但是因为了我们一。一组句子去输入的时候呢，它可能会有一个must就是去补全它。那这时候的话呢，呃，我们就是把这个must的一个部分呢给它去除掉，就不需要这个must的一个。大概是这样的一个原理，就算最终的一个呃。然后的话呢，加载模型的话呢，还是跟我们呃这个transformer加载其他模型一样，我们可以用这个auto这个。啊，你的一个这个预设的这个函数呢，去加载我们去初始化我们的，然后同样的用这个auto model的一个呃加载方式呢，去呃初始化我们的模型。

翠仪

44:40

然后它使用的时候呢，首先要把这个句子呢通过这个通过来呢转换成一个呃，这里虽然写的是这个encode input，那实际上它其实就是把文本转换成这个。实表上面所对应的一个ID，比如说如何这个词对应到我们指标里面呃，零号的这个ID。更换这个词对应到我们词表一号的IE，那它就会变成一个01234这样类似的一个向量，因为我们直接输入文本呢这个。模型是没有办法去计算文本的一个，呃，去就是让文本去进行计算的，它必须要转换成数字，所以就是转换成十秒上的一个ID，然后再经过这个模型的一个计算，最终才能得到它的一个。所以去把这个通过来词呢，把我们的句子呢去做呃，转换成一个token的一个列表之后的话呢，我们直接去预测这个模型的一个output，然后把这个output和我们的这个呃，input的时候，它的一个呃。Attention must呃，一起输到这个important这个函数里面就可以得到最终的一个呃important的一个sentence。他原来的一个叔叔呢，就是呃。呃，两个句子，然后因为它要考虑到这句子的一个长度啊，就最长取最长的就13个token。然后768位，然后我们做了这个important之后呢，它就变成了就一个句子，就是一个一乘768位的一个维度，那大概就是这样的一个。

翠仪

46:24

模型。那大家如果有需要的话呢，就是说。<外文>大家如果有需要的话呢，可以去训练，就是他这里也提供了这个。看。就这里还有其他的一个例子，大家可以去使用一下。就大家可以去自己去追加训练，调整这个模型，它其实用法都是一样的。呃，这个模型的一个下游业务，它主要就是这个<外文>句子相似度的一个计算，所以说的话呢，呃，如果大家有需要去训练自己的一个模型的话呢，呃，可以去参考它的一个。

翠仪

47:22

呃，训练的一个代码，去重新去微调一下自己的一个模型，让它可以更专注于在你的这个呃，本呃，这个垂直领域的一个知识上面，让它可以，呃，在这个呃，专业的一个知识领域上面。对不同的一个知识的一个区分度会更加好一些，就通常在这个，你用这个<外文>或者是这个GBT等等的这些去做<外文>的时候呢，它可能专注的是一个通用力。那你就是你一个职业领域里面的一个内容，就是不论如何，它就是看起来都很像，那这个区分度可能就不太够，所以就是建议大家可以根据自己的一个，呃，饮料去调整一下这个一个模型。

翠仪

48:17

那下面我们再回顾一下黄老师早上所说的这个向量数据库的一个问答，他说的一个简单版本的话，其实呃呃就有点类似于比较旧版的这个。嗯，字数顾问呐，它就是预先有一堆这个QA，然后就是，呃，根据这个用户的一个输入的问题，找到跟它最接近的那个问题，然后把它的答案给它输出出来。那这样的一个输出呢，其实可能会很。死板，然后大家现在这个很多的这个问答机器人，你们在使用中也会觉得这样的一个啊，根本就有时候没办法去解决用户的问题，因为用户的问题它可能不只是一个答案就能解答出来。它可能是多个答案啊，去综合去进行这个解答的，但是以前在大于研模型没有出来之前的话，你用单纯用算法，很难把这个不同的问答里面，它所需要的内容给它提取出来。呃，这个算法层面上呢，就你写一些呃写模板的话，你很难去做。

翠仪

49:27

<外文>但是大语言模型出来了之后可以去解决，帮我们去解决这个问题了，我们可以把之前的这个思路啊，然后结合到后续的这个大语言模型，然后让他去把我们啊，比如说我们这个。呃，去查问题的时候，我们可以把啊这个问题最相似的这个十个问题和它的一个答案一起输给这个大语言模型，让语言模型呢，帮我们去组织最后的一个答案的一个输出啊，应该是怎么说的，然后再返回用户。

翠仪

49:59

那整体的架构的话呢，可以就是跟图上所显示的是一样的，那这个，呃，思路和想法的话，在上市的一个特训营里面呢，也有讲过它那个做法，其实就是把我们的一个数据集，通过我们的这个<外文>的一个模型。那上次的话用的是这个open AI的一个<外文>，那后续的话呢，我也试过去换成了其他的一些这个，呃，更加精细的一些子领域的一个做法。那搜索出来的一个结果的话，因为。它本身这个训练的时候可能就是一个通用领域，在一个子领域上面呢，它去做这个相似。就是新的一个计算，有时候说出来的东西，我觉得是没有什么关系的，但是它就是说出来了，就，呃，然后我觉得真的有关的，他就说不出来，所以这个<外文>的这个效果呢，只能说一般般啊，也不能说它完全不好。在某些问题下面，他也可以回答的很好啊，所以就是要看概率。因为这个也没有办法去解释它到底用怎么去做这个呃用的什么语料。

翠仪

51:13

所以就是啊，这是一个不可控的，然后呃做了这个把数据，我们的一个知识，本地的一个知识，呃，做了之后存到这个向量数据库，然后当有用户有问题的时候呢，我们首先把这个。用户问题用同样的一个<外文>的方法呢，呃，转换成一个向量，然后去跟这个数据库去查询，然后去查询数据库里面相关的知识，共同组成这个P啊，然后呢去发给这个。GPT呃或者是GPT是哈！然后最终得到这个用户的一个答案，这样的一个知识架构。然后这个知识架构是，呃，上次所讲到的是基于open AI，就是无论是这个<外文>也好，呃，这个后面的一个问题查询也好，都是基于这个open AI的，那这个上次就有同学问我就是能不能做一个本地版的。那我们现在呢？呃，就先把这个上市做的一个版本再简单的过一遍，然后再做一个这个完全本地版的一个做法，就是如果大家听过的话就当复习啊，没有听过的同学就可以先听一下。

翠仪

52:39

然后这个代码的话呢，呃，昨天也发给大家了，在这个QA里面有一个data的，然后这个<外文>的话，点PY的话就是之前做的一个版本，它是基于这个。呃，open AI的一个接口去做的，然后做本地板的话，它其实整体的一个流程上面呢也是一样的。

翠仪

53:04

只是呢，它的这个和这个查询的可能是不太一样的，那在这里的话，我们需要加呃，这个连连接的库呢是这个呃<外文>的一个<外文>这个把握大家没有。装的话可以先装一下，那这个是用来去连接这个join的一个数据库的。然后它也这个数据库呢，提供了一些这个呃。这个现成的一个计算的一个函数，后面会用到，然后其他的话就是呃用开始去连接open AI的话需要安装一下open AI这个户，呃，就用于这个呃，open AI的一个连接，那它这里的一个<外文>的话，其实是非常那个简。三。呃，它直接掉这个open AI的一个<外文>的一个接口。然后这个用这个create，然后它摸懂的话用的是这个abundant的一个ada。这个002的model这个是一个相对于费用相对来说会比较低廉一些，所以它的一个<外文>结果呢，可能就呃稍微的一般。然后它的输入的就是我们的这个文本的一个数据。然后这个最后返回，因为我们书的这个items里面呢，我们之前是有这个呃，title和它的一个内容的。呃，这个item零的话就是它的内容。items是一的话就是它的。呃，这应该是写之前这个item一的话是它的一个需要向量化的内容，零的话就是它的title，然后最后把它的这个，呃返回的一个结果，这就是这个sentence里面。它的一个data啊，第零维的一个<外文>的一个结果，就是我们的这个文本的一个呃，向量。

翠仪

55:01

然后我们要做的就是呢，首先的话去连接这个呃Q<外文>这个数据库。那这个数据库的一个URL的话呢，大家在申请到数据库之后的话呢，可以在这里去看到。就在这里可以去直接去复制你的这个数据库的一个UI。

翠仪

55:28

大家去做的时候记得去换一下这几个数据库，然后再输入这个API的一个T，然后我们在这里的这个collection，呃，它的一个名字呢，你可以去写一个这个，根据你实际的一个。呃，数据的类型去写一个collection的一个名字。

翠仪

55:47

然后open AI的话也需要设一下这个open AI的一个API key，那大家可以根据自己的需求，写自己的一个open AI的一个key。那在这里的话是创建一个collection，就是如果这个collection你没有去呃，之前没有的话，你就需要去创建一下，如果有的话，那就连接一下，那这个呃OP AI返回的一个。In bonding的一个向量的维度呢是1536维。所以在这里呢，我们创建的时候呢，这个size呢写的是这个1536。然后呢，我们去算这个呃距离的时候呢，用的是这个余弦距离，当你也可以用其他的一个距离去进行计算啊，这个呃Q join这个都是支持的。然后我们一个数据的话呢，呃呃，之前演示的一个版本的话，可能数据会比较的少一些，因为只用了两个，那在这里的话我试过更换了一个，这个中就是。医学上的一个。

翠仪

56:54

问答的一个数据。这个数据呢，应该也是给到了大家的。这个数据在这个data里面。其实它是一个这个呃，这呃，医疗问答的一个数据集里面的一个数据，它有不同的一些科室啊，然后我用的是这个内科的数据，然后内科的数据的话呢？大概是长这个样子的，它就是呃，其实是一个QA的形式，然后上面这一段的话呢，是它的一个问题，后面这一段的话是它的一个答案。<外文>然后在这里。所以我这里呢就改的说我就只对它的一个问题去进行<外文>因为这个问题跟问题相似的话，那我去把这个相关的一个问题以及它的答案找出来，问题跟问题的相似。更好去这个计算。如果就是你问题加上了答案的话，会导致它的这个<外文>的时候呢，导致它的一个语音香料呢，可能会有一些偏差，所以就是在这里呢，去做了这样的一个改进，然后逐个，呃，便利这个我读入的一个数据之后呢。建立它的一个呃问题和答案的一个对，然后呢呃去做这个呃，imbalance，然后去做这个数据库的一个插入，那数据库插入的话，它这里用的是这个upset的一个方式，然后这里填的就是我们的这个collection的一个名字。

翠仪

58:32

然后位置一处的话，就是要让它一个一个的插入，等它上一个插入完了，我们再进行下一个操作。不然的话，它可能就是，呃，会报错，然后我们输入的一个point呢，它这个其实就是这个相关数据库上的一个点的一个结构，它的一个ID的话，我们通过数，它就是第一个。第二个就是给它一个ID。然后它的<外文>的话，就是我们<外文>的<外文>，然后在这里的话，还可以去输入一下这个，呃，向量的一些辅助信息，比如说我们把它的一个文体问题和答案的文本信息也给它储存起来。那最后的话呢，我们通过这个向量查询，我们最后要得到它的一个问题的答案呢，不是只要这个向量而已。所以我我们这个比较重要的这个文本资料的话，用这个pay node是以一个字典的结构呢，去给它上。

翠仪

59:30

传上去大概是这样的一个结构，那运行这个代码的话，就可以把这个数据上传上去了。那这个数据的话，我之前已经上传过了，所以这个代码了，我也不演示，就是大家可以看到我的这个占用了，大概这里好像是有1000多条的一个还是。3000多四千四千多条的一个问题，对，然后大概占的一个这个呃RAM和这个呃硬盘硬盘占的比较少，就是主要占的是这个类型会比较多一些。

翠仪

01:00:13

然后导入数据完了之后的话呢，我们就需要去把这个。问题和服务给它开起来。那在这里的话呢，其实就涉及到这个pom的一个设计，它的一个流程还是一样的。那我们在这里写了一个函数呢，就是输入我们的问题。还有我们查询得到的答案，把它组合成一个。P那这是一个这个。呃，home的一个demo，比如说呃，我们需要让他去使用一下段落去回答这个问题，然后这是这个他查询得到的一个段落。那后面的话我也试过对这个<外文>去进行修改啊，这个这个大家可以根据自己的一个实际使用的感受去改一下这个<外文>，那它可能是更合理一些。

翠仪

01:01:11

然后宽的话就是我们一个查询，就还整个流程的话还是跟刚刚一样，先连接到我们的一个相当数据库啊，连接的是一样的，然后的话呢？那个是要担？然后我们的这个API key的话也是要设定这个openai的一个API key，然后这个<外文>的一个模型也是一样的。那在这里的话呢，呃，我们查询的话用了一个这个相当数据库的一个<外文>一个<外文>函数，然后输入到的是这个<外文>的一个名字，然后查询呃，需要输入的向料，就是这个问题的一个向量。然后这个查询返回的一个向量数据的话呢，大家会根据失去可以设定这里设定的三呢，是因为这个不想这个太多内容，因为它这个太多的话，它可能这个home的内容有限制。然后它<外文>没有用这个精确查询，它只用一个模糊搜索。然后这个哈希值这个呃是一个设定的一个常数，然后就是，呃，把我们的一个结果呢，组装到我们的一个P里面，就得到了这个最后的一个pound message，然后把它去输入到这个。嗯。Open AI的一个这个对话接口，把内容输入进去的话，可能就可以得到它的一个返回的一个结果。

翠仪

01:02:40

大概是这样的一个内容。那因为这个open AI它需要在国外的这个服务器上面去演示我。这个开启一下，给大家去看一下。那这个接口呢是在这个，我设定的是8080接口。因为在这里呢，这个搜索这个文件里面的话呢，是呃写了一个这个用fast写了一个接口和简单的一个页面。

翠仪

01:03:44

对，他应该是用我的这个服务器的一个端口去进行转调。哎呀。大概是这样的一个智能助手，然后因为它是问的是内科的一个数据，我可以问它一个这个，比如说这个慢性胃炎。有什么。不理。你可以问他这样的一个问题。嗯，告诉我了。数据库连接的时候。说这里没有改。

翠仪

01:05:10

嗯，这个之前可能摆过一些，没有改弯的。这个待会再开吧，就因为我可能之前改了一些参数。然后的话呢，我们来看一下这个本地版本的一个呃处理，它其实整个流程是一样的，只是我们在这个向量化的时候和这个呃查询问答的时候呢，去收错了，然后在这里的话呢？

翠仪

01:05:41

这个本地的助手呃，本地的一个LL，这个LYM模型的话用的是这个T GM二啊，最近上线的呃大家不知道有没有在钉钉的一个机器人上面去使用过这个T GLM。有没有对比过它跟这个一代和二代的一个区别？他自己据说的话他就是这个。<外文>效果会更加好一些，而且需要的一个<外文>资源呢，会更加的一个一些，大家具体。的详情的话呢，可以看一下这个charge。26B的这个模型。然后它的这个模型的一个调用呢，也是非常简单的，呃，唯一的有问题的地方就是在于它的一个模型权重的一个下载啊，它在这里呢，就是默认情况下呢，是用横face上面去下载的。但是呢，他说就是国内可能下的不好，所以他提供了这个呃。就是清华的一个网云盘的一个下载，当然国内用清华云盘的话，它下载速度呢确实够快的，但是呢，我之前下载的时候呢，它就是有点问题，大家会看一下这个清华云盘上面它的更新日期是5000钱。然后这个横根上面呢，它的更新日期了会比它的这个。你可以看一下我们刚刚。就是七天前的一个版本，就是我<外文>这个是真实的告诉大家这个清华大学云盘上面一个版本在加载的时候它是会报错的。我不知道它这个模型是不是还在更新当中，因为它这个本身主页上也说这个模型还在不断的一个调整迭代中，所以这个权重呢，它并没有一个固定的，呃，然后这个清华大学云盘呢，就比这个上面。

翠仪

01:07:48

新的那么两天，然后我用这个这里所下载的一个权重呢，它到呃，这个导入的时候呢，就会报错。呃，就是这个模型结构上面它有一些东西缺少了。然后后来我还是嫁回这个横上面的话，它导入了就是没有任何的错误，所以大家就是大家下载的时候呢，目前的这个版本呢，只能从横上面去下载这个版本，这个会有一点问题，因为。可以看到它模型群中。呃，不知道是不是这个小数位的一个问题啊，好像有点大小不一样啊，我不知道它改动了什么，就是插了这么两天，不知道它改动了什么。

翠仪

01:08:34

当然就是给大家建议的话，就是直接下载P上面的一个项目，就就我可以给大家传这个百度云盘，但是我不觉得百度云盘一定会比这个上面快，你只要翻个墙的话，你这个下载。营业费上面的一个资源还是挺快的。

翠仪

01:08:56

然后它的一个使用的话，基本上跟这个呃，上一个GLM二的一个版本是一样的一个用法啊，还是用这个刚刚我们说想呃说到的这个auto auto model去进行这个。模型的一个呃导入那我们也可以去看一下。有它的一个使用的话呢，可以，刚刚这个使用的话跟这个之前的是一样的，我们只要。<外文>一下他，他就可以得到相应的这个结果了。

翠仪

01:09:35

那所以这就是我们后面的这个呃，像后面做问答的时候呢？它会有一个？对，在这个小那个？叫GLM的一个单吗？它下下载的时候，其实就是跟这个transformers去头的一个模型是一样的，呃，就建议大家预先把这个模型的一个权重呢下载下来。然后的话呢，它的这个模型的时候呢，如果因为它这个线程大概占的线程的话，大概是13G多一点就14G左右。大家可以注意一下就是。加载的时候这个呃显存类大小，那如果你是？那个什么就显存不够的话，可也可以去下载它的一个量化的一个版本就相对于小很多，你看它的模型大小只有3.9G。呃，所以它的所需要的一个显存量大概是七到八G左右，应该都是足够的，那但我没有用过这个量化版本。然后它就是加载进来之后的话呢，它就可以直接去输入到这个模型去进行这个对话了。哦，学生够了，我要把这个。现存占用的重新。重新跑一下。就上次这个上一代的模型，它一共是有八个模型的一个分化，现在是七个，然后也可以去看到它这个现在模型的一个占用的话呢。

翠仪

01:11:36

就就。

翠仪

01:11:37

大概是14居左右吧就。差不多是这个范围，就就只能单纯现在只加载了这个模型去用，用了它之后大概是这样的一个方案。<外文>所以大家去用这个J GLM的时候呢，<外文>可以去考虑一下自己的一个硬件配置。的一个性能。然后再回到我们的这个主题上面，呃，我这个data的话呢，呃，还是跟呃刚刚的是一样的，只不过是这个用的是<外文>，然后用的是这个。它是<外文>基于这个医疗数据相似性的一个模型去进行的一个微调之后的一个模型。所以说的话呢，它整体的这个。模型的话呢，呃，用<外文>跟思路呢是跟刚刚是一样的，我们设一下这个公众目录，然后就是加载一下我的这个。嗯。这个没跑吗？

翠仪

01:12:58

附近应该。哦，这个没放过来。找不到那个模型的一个路径。然后这个模型大概占比呃，显存占用的话就大概两居左右，它就是。呃，大概是这样的一个占用，就是量化模型本地的一个量化模型。然后我们写一个这个<外文>的一个函数，输入的是这个，呃，我们需要的文本，然后直接用这个<外文>我们的这个向量化模型去<外文>一下，就可以得到这个向量的一个数据。然后它的一个维度呢，刚刚也测试过了，是768维的。然后在这里呢，去连接这个相当数据库还是跟刚刚一样的。然后这里我collection的话我就换了一个collection，用的是这个呃名字叫做这个questions，因为跟刚刚的那个大家向量画的一个方法不一样，我们就不可。混在一起去做了，然后在这里呢，需要改一下的就是它的一个赛是768位的，大家如果要用自己的一个象征化模型的话呢？可以去确认一下你销量化之后得到的这个维度是多少，然后这里需要改一下。然后的话呢，这个数据呢是呃。外科的数据用的是？呃，内科的数据也一样能用，大概是这样的一个呃数据集，它读进来它可能分的会更加细一点，因为它是一个CSV的一个格式，它有一列就是这个呃department就是它的一个。呃，医院的一个部门，然后它开头的话是一个简短一点的一个问题。ask的话，它就是一个比较全面的一点的问题。要还有这个answer就是它的一个问题的回答。然后我们可以看到这前三个问题的话，它内容其实是比较类似的。

翠仪

01:15:06

的答案略有不一样，就大概是这样的一个内容。然后的话呢，我们就是遍历整个数据呃，表，然后把它的一个问题呢，去做一个<外文>，然后写入里面，然后我们这里所写的一个内容的话呢，是它问题和答案的一个信息，就方便我们后续呢去查。

翠仪

01:15:27

查找它的一个test的内容。输入到这个<外文>里面。看看。然后因为这个数据量呢，实在是有点大，大家可以看一下这个。这个<外文>的话呢？除了美。他低头。他data一共有12？这个十几万的数据我之前去做的时候呢，它大概提示我要用差不多，呃，这个十多个小时去导入，所以就是呃，当时为了简便的话呢，呃。只做了这个，就这这是全量的，然后后面我只做了前1000个这个数据的一个导入，大家如果有兴趣有时间懂的话，可以就是呃做全量数据的一个导入，就大概只导入了前面的这个1000条。

翠仪

01:16:30

然后最后的话呢，导入完成了，我们也可以查询一下这个数据库的一个内容。它就是给了我们这个collection的一个信息，然后大家可以看到了它的这个，呃，数据的一个状态，然后包括我们设定着这个向量的一个维度大小，然后还有就是我们的这个导入的一个数据。然后就一共是这个1000条？大概是这样的一个结构。

翠仪

01:17:01

然后把数据导入之后的话，我们接下来去做问答了，就简单很多了。这个问答的话，呃，这里写了一个这个<外文>的一个测试版本，就是可以给大家去看一下，如果在本地部署的话，大概是怎么样的。那大概也是跟刚刚的是一样的，我先把这个断开了，免得衣服会不够显成了。

翠仪

01:17:42

然后跟刚刚的是一样的，我们还是要导入到这几个模型去做这个向量化。然后呢，这是这个数据库的一个<外文>，大家就是根据自己的一个数据库去进行这个修改就好了。然后embance呢的函数呢跟刚刚是一样的，在这里呢，我们加载这个TGOM二的一个模型，然后这是它的一个模型路径。然后呢，把这个模型呢转回这个呃。验证的一个模式，不然它的那个默认都是一个trade的一个模式，权重可能会有变化，然后它的一个使用呢，就是要如果我们要向german to问问题的话，其实输入的就是这个test，然后这个。那个。

翠仪

01:18:36

这个历史的一个对话，如果你不需要它支持上下文的话，只要刚刚的这个只有一个问题的话，你可以这个呃上下文呢是保持为空的。然后比如说我可以直接问他一句，就是说他这个**路感染的药可以跟拉肚子的药一起吃吗？那就是它本身这个M二的一个回答。然后的话呢，我们在这里呢写了一个比较简单的一个。然后这里写了，就根据刚刚的这个查询的一个改了一下，那前面还是说我们去连接到这个向量数据库里面，然后去做这个，把我们的问题呢去做向量化，然后呢？呃，把这个数据呢组装成这个。呃，找到它的这个向量化，就因为问题我只对问题做了向量化，所以去找到跟它最相似的三个问题，然后呢，把它的这个问题和答案呢，就是这个test给它输入出来，构成组成我们的这个P。然后的话呢，再去问这个GLM我们可以看到呢是怎样的一个结果。这是刚刚同样的一个问题。然后在这里的话呢？你。还没有查询到这个结果吗？就大家也可以看到之前我跑的一个结果，可能是因为我这里这个重跑了一下。重新导入一下这个数据。就是它这个，比如说你看这两个就是慢性胃炎，应该怎么调养的这个问题，那就是我们失误给这个<外文>的一个<外文>，就是它的一个回复，那大家可以看到了它的回复呢，其实在这个。

翠仪

01:20:48

里面呢是有一些体现的，那如果没有经过这样的一个调整的话，它原始的回复呢，呃，是这样子，大家也可以去对比一下它们两个的一个结果，看一下哪个是更合理一些。好像讲了有点久，那关于这边这个的话大家有什么问题吗？就是这样，基于一个向量数据库的一个呃，加上大语言模型的一个问答机器人。核心是什么？核心那个没有关键一点，还是选这个模型，关键其实说是模型会比较关键一点。

王蓓

01:21:31

有提问的同学可以举手提问了啊！

翠仪

01:21:37

嗯，刚刚是有同学说话吗？

王蓓

01:21:42

当时我在说话，邀请同学们在线上举手提问，目前现在还没有同学举手提问。

翠仪

01:21:43

但是。如果没有的话，我们现在休息十分钟。

王蓓

01:21:53

哦，稍等，有一位同学桑燕举手提问啦！

翠仪

01:21:56

好。

王蓓

01:21:59

已经允许发言。

桑燕

01:22:09

哈喽，能够听到我说话吗？

翠仪

01:22:10

哦，可以听到。

王蓓

01:22:10

可以的，可以的。

桑燕

01:22:11

哦哦，我一直在说，就是我看到老师在在说的时候用的语料都是一问一答的，那如果说是有连续的上下文那种语的语料可以用吗？就是说，比如说。

翠仪

01:22:13

就是我们看到在说的什么的都是你自己。这个。可以呀，可以呀，其实它做的方法是一样的，只是呃上下文的话呢，它可以去对这个呃文章去进行这个切换，然后再去做，但是证明你的<外文>的一个。

桑燕

01:22:23

某那那那我我？

翠仪

01:22:38

这可能就会比较重要，你就怎么去确保你的问题可以跟这个你的这个信息能对得上。

桑燕

01:22:47

对，就是如果就是就是我们现在的就是这种这种语料的话，就是比如说打个比方，比如说是微信的聊天记录，他实际上如果说是讨论一个问题的话，他有的时候可能会穿插着几个问题在讨论他怎么来区分，可不有什么方法可以。

翠仪

01:22:47

这。但是如果是。问题的话。

桑燕

01:23:02

就是。

桑燕

01:23:05

可不可以做到？

桑燕

01:23:07

就是。

翠仪

01:23:07

就是理论上所有的语料都可以做到用这个方法，就只是这个效果问题，就至于怎么去提升你的这个效果的话，其实还需要看你语料本身是怎么样的啊，我们是要去调整它的一个point呢。

翠仪

01:23:25

还是要调整它的这个<外文>模型。还是说要根据它的这个语料的一个结构去进行这个特应的一个呃。

翠仪

01:23:36

这个，比如说像这个问问答，它本身这个数据就是一个问答的一个数据的话，那我们就选择其中一部分跟我们的问题会比较相关的，这个，它本身的问题去进行这个。那如果它本身是一个上下文的话，那我们可能是。

桑燕

01:23:50

嗯。

翠仪

01:23:53

呃，对整个上上文去去分化切断，可能是先提出一些这个关键词啊，等等的这些信息，再去就对它的一个关键词去进行这个。然后再去进行这个比对，所以它其实关键的一个算法在于这个，呃，怎么去找到一个相跟它相关的一个知识输入到这个phone里面。

桑燕

01:24:17

嗯。

桑燕

01:24:20

嗯，其实就是还是模型的选择比较重要，那这个模型的评价和选择我们我们我们会说到吗？

翠仪

01:24:20

嗯，其实这个还是有的选择，那这个就是评价可以选择我们。你们能收到吗？模型的评价选择就模型的话，比如说像<外文>的话，其实就是刚刚所说到的，最常用的就是这个。哦。

翠仪

01:24:46

就是这个C的这个模型，你可以根据自己，如果你有数据的话，可以根据自己的一个数据再微调一个去做这个embody。

桑燕

01:24:50

嗯，反正。嗯，好的。

翠仪

01:24:57

按下游任务是这个句子之间的一个相似性，所以你输入的它这个训练的时候输入的应该是两个句子，输出的是它们是否相似的一个标注。

桑燕

01:25:09

好吧，好的，那我自己再研究一下，谢谢！

翠仪

01:25:10

再准备一下。诶，我有个问题就是有些场景是没有问答的这个。就是一个一堆像一本书一样。然后还能够做吗？可以可以，这种情况下是怎么做呢？这种情况下你就其实最简单的就直接把这个知识分坏就上，老师也说过了，你把知识分我问的问题你就不能说通过我的问题。进行一个找到的，因为没有问题，就是这个<外文>它找相似的话，它不一定是问题跟问题之间，你问题跟知识之间，其实它也会有一定的一个相似性，问题跟知识之间的相似性。对，就比如说啊，你有一个呃比较专业一点，比如说你有一个呃硬件设备，比如说像这个一体机啊，它可能会有一些产品手册和商品手册，可能啊，一开始介绍这个机器它有什么什么组成啊，这12345。然后呃，然后这个每部分可能有个介绍，然后你再问他问题，比如说你问这个一体机由什么组成，它可能就会找到它组成相关的那个呃知识，然后这个这时候你把这个知识跟你的问题一起给这个大圆梦型的话，它就可以。根据你的知识去进行这个回答了。

翠仪

01:26:34

唉，能不能这样，就比如说，比如说假设有一个问题啊，就问我说我想去跑马拉松，请你帮我推荐一双跑鞋是吧？那那这个这个情况我不知道哪些是类似的，我首先把它输到大模型大模型推荐。这乱七八糟的把鞋给我啊，然后我把这乱七八糟的。放到我的知识库里面去去匹配行不行？我他出来的那个应该就是知识了嘛，发不出来的支持，但是他的知识是我的知识库里面没有的，然后他那个的知识在跟我里面的知识再找到最相像的之后就。就出来我的知识了嘛，是吧？呃也是一种思路，但可能这个实现起来。我不不好说好不好，因为这个首先大模型他回复你的，他可能就是假的，假的，它也是一种知识嘛，总比总比问题跟知识这些问题。不一样是..胡说八道不一样......我是的，内..把它变更一。但是我问题跟这个知识我总觉得他问题跟知识怎么可能相类似呢，是吧，我问我问最好的鞋是什么？那我有这些？当然都有个鞋子了..是，所以也要把每个鞋字写..反点，只是说非常好..都嗯对对对对。嗯。

翠仪

01:28:21

在我们这个关系。呃，对，根据制句子呢，因为它的语文法它些变成词了没有，他就直接把一个句子或者是一段文字就转回我转换成一个项料了。对对对对，句子肯定不能太长。那如果pdf的太紧的话，他这个确实。我本来就有这个概念化的。对，你就可以根据它的一个段落或者是这个句号去切分，就你可以限制它的一个切分，就是这个切分算法，你比如说呃，三到五句为一段，然后字数是怎么样的。那就可能300是一段对。

翠仪

01:29:26

对呀。

翠仪

01:29:30

就是是准确的危机。呃。反正我觉得。嗯，..模型....面..方他就有用..对对对对，这周还是只是在后面处理，但下周的内用在前面也有它的作用。

翠仪

01:29:58

哦哦。是啊。就这个1000条的话是四分钟，那10万条你就乘一下，它是一个云数据库，它不是一个本地数据库，而且它是在海外的，就是本来这个网络通信可能就有点慢。

田志军

01:30:17

哎呀。

翠仪

01:30:21

如果你用本地数据库的话，它就会很多，就下周会给大家去讲解怎么去铺一个本地的向量画数据库。嗯。嗯。

田志军

01:30:36

哎，何老师。

翠仪

01:30:43

啊，你说。

田志军

01:30:44

啊我我问一下就是我。

田志军

01:30:47

我们的那个数据库里面不单单有这种文字的那个，还有一些非文字的，像图像的，音频的和视频的。那这样的话，一个问题，用户的问题来了之后，我们要匹配出来的是。

翠仪

01:30:48

二。

田志军

01:31:02

呃，非文字性的那这种怎么来去做做呢？

翠仪

01:31:02

嗯。呃，就是你要问一个问题，但是他要返回一张图之类的吗？

田志军

01:31:14

哦，我没听清楚您的。

翠仪

01:31:15

是你要问的就是用户问一个问题，但是它要返回的是图或者是音频之类这样的一个数据啊，这个其实你可以就是呃，把图或者是声音声音的话就好办了，你可以转化成文本。

田志军

01:31:23

对。对。

翠仪

01:31:33

图的话也有一个这个图的一个语义的一个模型，就是它可以把图上面的一个语义信息给它提出来，同样也可以做这个important。

翠仪

01:31:44

然后他搜索的时候其实也是去做<外文>，但是你可能这个<外文>那个模型，它输入的就是一个多模态的信息。

田志军

01:31:45

哦，是，但是我们的那个文字它它并。

翠仪

01:31:57

你看那个文字？

田志军

01:32:00

那云它并不是说是。

翠仪

01:32:01

并不是。

田志军

01:32:02

文字类型的，它可能就是一段音乐类的，或者说一段呃视频类的啊。

翠仪

01:32:06

呃，音乐类的也可以，其实它就是有这种把这个呃图片或者是语音啊，它转换成一个语义的一个向量，你就是这个你去做imbalance的时候，你这个模型可能需要做一个多模态的模型，它接触。

田志军

01:32:18

嗯。

翠仪

01:32:25

输入可能是这个<外文>文本啊，这个语音啊，图像都需要做。

田志军

01:32:32

嗯，好。

徐华强

01:32:35

哎，那个俄罗斯？能能听到吗？何老师你那里能听到吗？

翠仪

01:32:46

哦，现在听得到。

徐华强

01:32:48

啊，挺好的，就是你能回到那个PPT吗？就是刚才那一。

翠仪

01:32:48

哦。

徐华强

01:32:53

啊，就是我想问一下，就是那个呃，在线版的那个好像是它的那个就是数据集这里就是用的这个open AI的这个嵌入，然后这个用户问答的时候也是用的。AI这个就是我看刚才演示这个本地运行那个版本为什么没有那个C。做那个嵌入的，我看好像是用的是那个。txt那个是我理解错了还是是什么呢？

翠仪

01:33:16

啊，没有理解错，因为这个GLM它只提供了这个对话接口，就<外文>它返回的直接原先返回的就是这个。呃，你看它的一个使用的话呢？它返回的就是，就是它的一个对话的一个内容。啊，嗯，当然你要做呢，它也是可以做，但是还是这个刚刚那句话，因为本身这个像这个CG VT或者是T GM的话呢，它的这个。呃，都是一个通用领域的，而我这个<外文>这个相互化模型的话，它其实是针对医疗相关的数据去进行微调过的，它在去做<外文>的时候呢，效果会更加好一些。

徐华强

01:34:07

哦，好的好的，明白，谢谢！

王蓓

01:34:17

线上暂时没有新的同学举手提问了，要不然课间休息十分钟。

翠仪

01:34:17

还有别的同学？哦，好行，我们先休息十分钟吧！

王蓓

01:44:23

我上班回来上课啦！

翠仪

01:45:46

好像现在也没用。很多。

王蓓

01:45:53

Ok了何老师可以上课了。

翠仪

01:45:55

啊，好的好的，呃，那我们继续吧，那上面的那一部分就是这个基于这个向量数据互搜索的一个智能机器人这一部分，呃，算再次结束了。那下面的话我们来到另一部分就是这个。

翠仪

01:46:11

呃，图机器学习这一块，然后首先给大家简单去介绍一下，就是这个<外文>里面它的一些，呃，比较简单的图分析和图计算的方法，那它这里用到的一个包呢，叫做<外文>。

翠仪

01:46:27

S是一个复杂网络研呃研究常用的一个包，它主要用来是画各种图，然后也可以去进行一些比较基础的一个图分析和图计算，然后它的一个安装呢，也是非常简单直接P IP。一下就好了，没有什么坑。那装完之后的话呢？呃，我可以就是来看一下最快的一个内容。

翠仪

01:46:57

那首先的话呢，我们要这个一下这个包，然后还有就是呃<外文>它本身就是一个画图要用的包，然后这个呃<外文>的话，它本身内置了一些这个。图比如说像有这个空手道的这个图，那这个空手道图的话呢，呃。它里面内置的一个。呃，它是画了一个这个空手道俱乐部之间这个成员之间的一个呃关系，然后大家会看到这个图了，好像就是这这这有两个比较，这个中心的一个点，一个是这个零号。一个是这个33号，呃，它其实是因为了中间呢，它们的这个呃这个这个钻和这个mr high啊出现了冲突，导致了这个俱乐部一分为二。那大家可以看到它们之间的一个关系呢，也是在这个网络图之间的一个相互连接了，也呈现出来这个，呃，两个中心点的一个趋势。那我们首先把这个图呃，作为一个简单的样例啊，去讲解一下，用这。

翠仪

01:48:09

The level as去做一个简单的一些图形呃，分析和图计算啊。然后的话呢呃，我们如果要把它画出来的话，直接用这个level的一个drive就可以把这个图画出来了。然后这是它的一个这个呃颜色，然后还有就是它有没有呃画出它的一个标志，然后它的一个呃位置的话呢，我们是用它的这个呃speed lay out就是它们的一个。之间的一个关系的一个位置呢，去给它提出来。

翠仪

01:48:44

哦，画出来的这张图了。大概就是这样子的。就就每次画的话，它位置可能会有点差异，因为呃这个图之间的一个联系的话就是不固定的。就就就因为这个，所以它可能位置上可能有点偏激，但是它的一个关系呢是没有变化的。

翠仪

01:49:10

然后的话呢，像这个图的一些呃，度的一个计算的话呢，我们也可以去计算一下，就我们一共是有34个点嘛，所以我们设定一下它的N呢是等于34，然后我们可以直接输出这个图的一个度。那其实它就会返回每一个节点，它的一个度，比如说零号节点，它的度呢是16。

翠仪

01:49:35

呃，它就跟16个点呢是有联系的，然后这个一号节点呢是九等等这些。然后我们也可以把它的一个列表给它储存进来，那就是它的一个do的一个计算。然后还有就是这个边的一个数量的计算的话，就直接呢，呃，可以用这个图的一个engine，你就可以获取到它的一个边。那它在这里的一共是有78条边的，然后这个呃最大的一个度呢就是17啊，就是这个33号周围人中啊17这个这个17点呃最小就是一。就是有一个点呢，他可能只认识其中一个人。啊，但是这个有点乱，一下子没有可能就就是这个11号这个人。他只认识一个。所以他的肚子就只有一。然后它的一个平均度的话呢，就是用我们的这个。呃，它的一个度啊，在，然后再除以它的我们的这个呃点的数量，平均每个点呢它度呢是这个4.588，然后还有就是它度的中位数就是三，就大部分呢跟其他点的一个联系了。

翠仪

01:50:52

大部分都是三个这样的一个情况啊，就是一些呃，do的一个简单的一个计算，然后的话呢，也可以画这个do的它的一个呃，直方图啊，就早上那个黄老师也在他的课件上。也有展示过一个直方图。大概是长这样子的，就是可以很清楚的看出来这个点之间的一个度的一个分布的一个情况。然后呃，这是一个<外文>计算，然后早上也说了，呃，一些这个搜索算法，就是图论里面的一个内容，比如说像这个最短路径啊，两个点之间的一个最短路径，那在这里的话呢，呃，我们就可以通过它的一个short。

翠仪

01:51:39

啊，去找到这个，呃，最短路径，然后这个最短路径呢，其实是包括了所有点的一个最短路径，那零的话呢，就是表示了，呃，所有点到零号之间的一个对短路径，那零的话就是自己到自己了，它就不用走。那一的话呢，就是零走到一，它们之间是有一个直接的联系的，二的话也是09到二，它们之间也是有特呃这个联系的。比如说像这26号这位仁兄的话，他走到零的话呢，还有26通过33这个中间人。要33再认识八啊，八再认识零号啊，这这这个啊方法来去走的这个最短路径。所以就呃，比如说我们这里输入的不是零是一的话呢？我们也可以去找到一号的一个最短路径。那这个可能一号的路径也是差不多。那如果呃大家会想象一下，如果我们走的是11号的最短路径的话，那基本上他们都要经过零号去走。嗯，大概是这样的一个呃，最短路径的一个搜索算法。

翠仪

01:52:50

然后还有就是一个单元最短路径呢，这跟刚刚最短路径呢，可能有点差别，那它是找到了给定点和图中其他点之间的所有点之间的一个最短路径，也就是说我们要从，呃，比如说从零号点出发。然后我们要遍历整个图的话，这个路径怎么走是最短的，那它就是呃这呃这个单元的一个最短路径，比如说零号点出发，我们要先走一二三四五六七八十，十一十二这样走下去。然后这样子便利整个图的是最短的，然后我们看可以看到呢，它其实会有一些回头路必须要走的，呃，零号，因为它是一个比较核心的成员，然后它可能跟别的一个成员之间关系的。走着走着他他们之间不连通了，他要走回零号这边再去这呃走别的一个点，那这个是这个单元的一个最短路径。

翠仪

01:53:50

然后还有就是所有匹配的一个最短路径，就是找到所有点对之间的一个最短路径，那其实跟刚刚的这个最短路径有点类似，只是呈现的一个结果是不太一样的，比如说这个。呃，零跟这个呃零之间啊，一跟这个一之间等等的这个这个路径。然后还有就是最小生成数啊，最小权重生成数啊，因为最小生成数的话，它应用于这个无线图。然后如果我们是有方向的话呢，它是有一个这个方向的对比。是。然后用的是这个minimum spending engine这个方法。呃，这个的话呢，它就是画出来一个最小生成数，因为我们哦这个最小生成柱的概念的话，大家可以去图论那边去看一下，就是这个图，这个保持一个最小的一个连接度。因为确保了每个点之间是有连接的，但是呢，有一些多余的连接呢，可以把它去掉，那就是一个最小生成数。

翠仪

01:55:00

然后像一些图论上面的一个分析算法，比如说这个图机器学习经常呢最开始不是在这个社交网络训练，呃，上面去用嘛，就比如说给你推荐这个你的可能认识的好友啊，呃，然后他就是把。这个社交呃，这个群体去分组，然后或者是说这个把客户去分群，群分，也可以去用这个图论的一个社群检测的一个算法啊，或者是说这个网页的一个主题，比如说这个所有的一个网页之间，它相互的一个链接了。是一个这个网络图，那这时候的话呢，我们用色情检测的话呢，就可以去把不同的网页去分到不同的一个主题啊，可能这个这一堆相互链接的比较多的，可能就是这个计算机方向的。然后另外一边可能是这个呃娱乐的，然后还有一些什么军事的等等的，这些网页主题都可以去做。

翠仪

01:56:01

然后呢，呃，这里有一个常用的算法呢，是这个社群发现的一个常用算法，它是通过的这个网络类里面的边来定义，就是它逐渐去移除这个网络之间，就是这个点跟点之间的一个。边的一个数量来定义这个网络的那？

翠仪

01:56:23

它一个算法步骤大概是这样子的，呃，就是说我们有这个，呃，所有的一个边，然后移除的这个边的一个居间性最高的一个边，然后移除之后再去算它们所有的一个居间。呃，这个其实有点类似于这个去做聚类算法，只是它聚类的一个标准呢是不太一样的。那这个level S里面呢也有一个这个类似的算法，然后这个呃community。这个算法的话，这个需要额外去装一下，因为它本身自带的算法没有的就用呃，这个重新安装一下这个包就好了。那在这里的话呢，呃他给出了这个呃两个。两个社群就是，呃，刚刚的空手道的这个网络图，其实它应该是这里一堆，而这里一堆啊，就因为它们分裂了嘛，就像所以它最终呢给出的这个呃，聚类的一个结果，就是社社群的一个发现结果呢是这个。01236这个二跟33这这两个这个结果。但是呢，这个方法呢，在大规模图上面呢，它运行的时间呢是比较长的，所以呢，也有别人去提出一些算法，就是这个衡量分组呢，就是用它的一个模块性的一个呃概念。它大概呢就是说，呃，会有一个凝结点的一个新社群，这个具体的一个理论的话，大家可以去。

翠仪

01:58:06

参考一下这里的一个说明，它上面会有比较详细的一个解释，然后它也可以去进行这个，呃，社群的一个划分，但是我们可以看到了这个用这个方法划分的社群呢，就跟刚刚的是不太一样，它分的可能。更细一些。

翠仪

01:58:24

然后当然我们也可以用这个分层聚类的一个算法去做这个社群的一个划分，然后我们，呃，可以把这个图重新加载一下，然后去计算这个距离的一个矩阵，那这个距离是怎么去衡量呢？因为这个。这个这个特征之间，它可以去计算这个距离，那我们在这里呢，就是用这个呃，最短最短路径去衡量，就是刚刚所计算的这个。所有匹配之间的一个最短路径，那它就是比如说点一啊，点零跟点呃，五之间的一个最短路径，它会直接直达的，它就是一嘛。然后如果是跟那个它跟33的话，可能就。隔的比较远，它的一个路径呢就会比较长，那它这个就可以来用这个点跟点之间的一个最短路径呢，来作为它的一个距离的一个衡量。那有了这个距离矩阵的话，我们去做这个分类算法，呃，这个。分成聚类就很简单的就呃，就是这个按照它的一个分成序类的元素啊，我们用这个SK learn的一个聚类，一个方法呢，就可以去自动的去获取到，那就是它的一个聚类的结果。啊，在这边啊，也是分成了两个类，然后这个两个社群。

翠仪

01:59:51

那还有一些中心性的一个算法，还有一些这个，它的一个计算呢，我这边就不细说了，就是大家有兴趣的话可以去研究一下，就是在拍成在图的一个本身的一个计算上面呢。它是有呃，比较多的一个资源去计算的，但是呃，好像黄老师也没有讲太多图论啊，的一个方面的一个东西，所以就是只是简单的过一下，那我们今天这个另外一个重点呢，就是简。

翠仪

02:00:23

这个PYG这个包，那PYG的这个包呢，其实呢，它是基于这个P。的一个图，神经网络专用的一个包，然后就是它的一个呃主页。那这个包的话呢，它呃好用的地方就在于呢，它因为是基于呃，PY touch来去写的，所以说它整一个这个建图神经网络的一个结构，只要你熟悉PY套型，你就会觉得这个包上手。非常的一个简单，因为它整个局法架构就是跟PY touch是非常相似的，然后它跟PY touch也一样，也提供了很多现成的这个呃呃图层，用网络的一个模型给我们直接去调用。

翠仪

02:01:11

你不需要去扶老说我们怎么去复现这些算法，它是有现成的一个算法，呃，跟跟那个PY touch一样，它是呃，内置了很多的这个经典的算法，但是它也有缺点，就是说。它的这个字符的一个编辑性呢，就是比较差一些，就<外文>，而且它中文化呢，可能就是比较少一些，所以如果，呃，你是一个初学者，你想快速的是实现你。

翠仪

02:01:42

个这个图神经网络的话，我建议你用PYG这个包如果用起来非常的一个简单方便啊，高效，但是如果你想自己去写一个这个图神经网络的话呢，我建议你用另一个户叫做这个。PGL这个who PGL的话，它可能就是入手这个入门可能会难度高一点，但是它的一个呃自主性会高很多，就是如果你熟悉了这个PGL的一个基本使用，你要去自己去写一个。

翠仪

02:02:16

设计一个新的一个神经网络，呃图神经网络的话呢？用PGL的话呢呃就是会。这个呃自主性就是可编辑性会好很多，有很多方法，它的一个支持度还是挺高的，但是因为考虑到大家可能就是没暂时没有这个需求，所以呢，我们现在呃，今天先讲这个PYG这个包。就让大家可以快速的去上手，去用这个图神经网络去做一些这个呃事情，然后后面也给大家介绍了两个案例，一个是这个。

翠仪

02:02:54

这推荐系统这个可能大家这个对于推荐系统来说，就是还是停留在之前的那种。呃。呃，比较传统的一个推荐系统。呃，因为推荐系统它这个新的一个技术好像也没有什么新的一个算法出来，那其实用图神经网络的话，呃，它就可以去做一个推荐系统。呃，你可以把这个推荐系统里面的一个购买者，然后还有就是它跟商品之间啊，去构建一张图，就是这个这个购买者他有没有去买某些商品，那如果有买。的话，它会产生一些连接。然后的话呢？那这时候我们要去给他推荐商品的话，其实就是要去做一个链路预测，那这是一个图神经网络里面啊，一个基本的一个任务，就是链路预测，就好像早上有。

翠仪

02:03:55

人这个有同学问就是怎么去知道这个不同的实体之间去找到他们新的一些关系，其实可以用链链路预测去呃，给它去做，就是可以判断预测一下这个节点，这个人这个节点跟这个商品之间这个节点。他们之间会产生一条联系的概率是多高，如果它概率达到一定程度的话，那我就觉得他会会，他会买这个商品，那这时候我就给这，把这个商品给他推荐就好了，那就是，呃，这样的一个做法。而且它需要的数据呢，其实啊，可多可少，就是你可以有一些这个商品之间的一个商品的一些属性啊，呃，这个购买者的一些属性呐，然后你简单一点的话。也可以是完全没有，就是只有这个呃，购买者的一个点击信息啊，然后还有就是他的一些购买记录啊，也是可以的，那就是稍后那个例子会讲到的东西，那在此之前的话，我们先来熟悉一下这个PYG这个。

翠仪

02:05:02

好的一个使用PYG这个包的话，它安装起来的话呢，因为跟我这个配置的一个主环境，它有一些包的一个冲突，所以的话呢，我是另外起了一个心理环境去，呃，装这个包，以及它相关的一些依赖。然后它首先要求的是这个PY十一点二点零的一个版本，然后我在这个虚拟环境装的是这个一点一，一点零的一个版本，<外文>用的是11.3的一个版本啊，这个版本的话是因为。我原来的这个镜像它装的就是不打11.3我就没有动了。然后的话呢，你先要安装这个P。呃，是如果你不装后的话呢，你就是它在装的时候它它也会自己下套，但是它一般自己下的话都是CPU版本的，那那就没什没什么必要，因为我们用的都是这个，呃，GPU版本。然后因为它的这个包之间呢，嗯。不同版本之间啊，不知道为什么会有一些冲突或者什么样的。所以说的话呢，在这里呢，给出了这个呃PYG它相应的一些依赖包呃的一个版本的一个推荐。

翠仪

02:06:18

就是这个PYG包的话，我们用的是二点一点零的版本，然后像这个我们如果要用这个聚类算法，会用的这个<外文>的算法的话，是一点六点零的版本，然后其他的这个有我们要调用到它这个压力数据库的。呃，或者是这个要做这个离散化的处理的话呢？呃是建议大家按按图上这个版本去装，但如果你不打的是11.3的版本的话，那你这个touch和这个touch vision的话就自己改一下就ok了。那就是<外文>我验证过它是可以实际使用的一个版本的一个配套。就如果你不装这个版本的话，很可能会出现各种到处啊，我之前给大家已经踩过坑了哈。

翠仪

02:07:09

<外文>，然后它的一个使用的话呢，<外文>也是非常的一个简单。我们可以来到这个PYG这个包里面有一个这个。哼。

翠仪

02:07:24

M可以使用。那我首先呢是要加载这个。先把这个。没有用的关掉啊！嗯嗯。我们先，呃，它需要加载那个<外文>，然后还有就是它本身的这个有一个很重要的一个数据结构叫做data，那这个data的话，其实就是，呃，我们一个图的一个data，那我们一个最简单的样例。就是我们要去构建一个无像图，那这个无像图它就是一个data的数据结构。比如说在这里我们定义它的一个边啊，定义了它的点，然后就把它边跟点构成，构成了这个data。那我们可以看到。这里呢，这是我们想要构建的呃无相图，然后它有012这三个点，然后它本身的一些熟悉呢，就是这个S一，然后零号点NE是等于负一的，一号点等于零呢，二号点等于一，所以我们在。点的优构件里面的话呢，直接是写到了这个每个点，它的一个特征向量啊，这个特征向量和它这里只有一维，所以就是负101这三个点的一个值。然后上面这个边的话呢，它其实就是讲了这个，呃，点之间的一个关联信息，呃，然后它是要这么看的啊，这上面第一个是它的一个起点，下面第二个是终点，所以你要看的话就是这样配对的开。

翠仪

02:09:02

零号点和一号点之间是有连接的，然后一号点和零号点就是一号指向零号，零号也指向一号，因为它是有方向的，而我们这个是一个图像图，图像图，也可以看作是一个方向图，所以我们两个点之间的一个关系都有。然后就是一指向二的一个方向和二指向一的方向都要有，那这就是它的一个边的一个呃。

翠仪

02:09:27

说明的一个矩阵，这是它的一个点的一个说明矩阵，然后共同构成的一个data，那这个无相符就是一个data的一个结构。那所以说的话呢？它的一个data呢，它其实是构建这个图，数据对象，那我们只要确定的是两个东西，一个是它的一个节点的一个属性，或者是它它的特征向量。那因为每个节点它都可。有自己的一个特征向量，那这个特征向量的话呢？呃，你可能前面如果比如说它有一些文本的一个特征啊，或者是有一些这个呃，ID的一些特征啊，label的一些特征啊等等。这些可能你要先做一个。

翠仪

02:10:16

特征的一个处理，然后第二个呢，就是它的一个呃，连接矩阵的一个信息，就是它编的一个信息，就，呃，有像图和图像图，图像图的话可以看作是一个特殊的有像图，就是它们两个点之间的一个是双箭头的。那有线图的话，它比如说像这个，它可能就是一个单键图的，那像这里我们的这个图里面一共有四个节点，然后VEV二V三V四每一个节点都有一个二维的一个特征向量和一个标签Y。然后呢就表示这个Y呢，其实可以表示这个节点属于哪一类，那如果你这个<外文>呃是要做一个节点分类的话，那这个Y呢可以是它的一个标注的一个信息。那我们在这里的话呢，如果要用这个tensor，就是你还是cancer去表示它的一个点的特征，我们用两个向量就足够了，因为它有两个不同维度的一个特征，一个是这个S的一个特征，它是这个。呃，这个可以看到上面就是21啊56啊等等这些。第二个呢是它的一个Y的一个特征，那它的一个节点的分类是零基于一啊，这样子，那这完全就是一个pouch的一个T的一个结构。而我们。无的一个边的结构呢，它是用COO的一个格式进行存储的。那COO的格式呢，它就是一个这样的一个维度，它一定是有两个的。

翠仪

02:11:44

第一个表示所有边上的一个起始点的一个index。第二个表示呢，这个边上对应的一个目标点的index其实就啊类似一个连接矩阵这样子，但是它不是简化版的连接矩阵，它用的是。两个向量去表示了它所有的一个连接的信息。这是它的一个呃，起始点就是它的一个忠实点，那所以这样对下来的话，我们就是有呃，零套点指向一号点，一号点指向零号点，二号点指向一号点，零套点指向三号点，三号点指向二号点啊，大家可以对一下这个结构是不是？

翠仪

02:12:21

对的，那这啊，这时候它的一个呃呃类型是一个弄的一个整形。然后上面的这个呃，是一个<外文>的浮点数的一个类型，然后的话它上面的这个顺序是没什么关系的，你从哪个点开始，你把哪个点定为零套点都是可以的，所以就是，呃。

翠仪

02:12:45

这个而且顺序的话，只要对应的上就好了，你从哪个点开始写，哪条边开始点都没什么所谓，这两个是等价的一个信息。所以综合起来的话，我们这个图呢就可以这么去表示，就首先我们是有两个特征。

翠仪

02:13:06

啊，然后这个边的信息呢，我们这个写出来了，然后我们去创建这个data这个图的结构的话呢，我们输入的就是它的一个特征SS YY然后它的一个边的一个in呢，就是我们。呃，这个编的一个in that，然后所以呢，它构建出来的一个结果。

翠仪

02:13:27

就是这样，它data然后可以看到S呢是四乘二的一个维度，就是它的一个特征。一共有四呃这个四个点，然后每个点呢它会有两个特征，然后它边的话呢是二乘五啊，就是一共有五条边的一个信息。然后还有就是Y的话呢，就是有是呃是一维的信息，它就有四个特征。然后的话呢，我们可以去把这个图的一些基本信息给它输出出来啊，不一定是用刚刚的这个level S，它可能是这个，比如说这个图上面它的一个点的数量啊，这个呃。它的一个图的一个呃。边的一个，呃，平均的一个边的数量，然后还有就是它的一个，呃，平均的每个边的一个，每个点的一个度的一个信息都可以去进行计算，因为我们知道的它的一个边的数量，它的点。数量，那我们也可以把这个图呢给它画出来，那用到的是就是刚刚用的这个level。然后它输入的这个呃。H呢是首先要是一个level的一个对象，所以呢，我们这里这个可以用这个，它里面的有一个叫做to level S的一个函数，把我们的data转换成这个<外文>里面的。

翠仪

02:15:01

图的信息，然后直接用这个<外文>这个函数的话呢，把它直接画出来，就是这个叫level S的一个情况。那它画出来了，呃，是这样子的，跟我们预设的有点不一样。你这个D。

翠仪

02:15:31

因为它这个<外文>的话，它展示的是这个，呃，这个整个图它它的这个结构有点不一样，虽然可视化，但是可视化的一个结果跟我们想象中的其实还是有点差别的。诶，我们最高。Ok那这个有了这个data去表明这个11个符。那这时候的话呢，呃，我们要训练的话呢，这个P touch里面其实构建的主要还是这个。Data set一个类型。

翠仪

02:16:15

那dataset的话它内置呢是有两个。不同的类型啊，其实这这个很简单，一个是直接储存在类型里面的一个in memory data set，它适合用于比较小型的一个数据题，然后如果是本身的一个dataset的话，它其实是跟这个拍拖时的一个data set是一样的。它只有在获取到下一个数据的时候呢，它才会去，这个是一个迭代器。然后如果是上面那一个dataset的话，你可以把它看作是全部一次性读入到内存里面。就这时候呢，要考虑到你的内存大小，然后还有你的数据集的一个大小，再决定你用哪些这个数据集类型。然后PYG这个包里面的话，它也有很多的一个预设的一个呃，图的一个数据。那就可以看一下。就这里面可以看到它的一个图的数据，也可以直接当下来。然后这个呃内容的话，大家也可以去直接去。哦，看一眼<外文>去了解一下。然后的话比较常用的就是这个大家去做教程，过去演示的时候比较常用的有一个叫做<外文>，是一个这个论文之间的一个关系去构建的一个图，然后它这个本身这个论文之间呢，它也有一些这个类别。啊，就不同的一个类别一共是270呃08天，然后另外一个这个呃。C. <外文>啊，这个也是一个论文之间的一个数据集的，但是它这个论文包含的内容不一样，然后还有一个是这个生物医学的一个论文，都是这个论文之间的一个关系的。

翠仪

02:18:13

然后呢，我们也可以跟这个pouch一样pouch，它本身也有一些绿色的一个数据集，那PYG呢，它也是有一些绿色的数据集，我们可以直接呃下载下来去进行这个使用。

翠仪

02:18:28

然后的话呢，如果我们用的是自定义的一个data的话，就自定义的一些数据的话呢，我们需要自己去写一个dataset的一个函数，那在这里的话展示的是这个dataset的一个官方说明的一个编写。我们要写的是呢。

翠仪

02:18:47

呃，几个重要的函数？一个是这个word file names它其实就是一个没有处理过的一个数据文件的一个文件名的一个列表，然后这个<外文>呢，它就是一个处理过的啊，就后面的话有，呃，就相当于我们做，做了处理之后。会把它保存下来，放到这边。然后download的话就，呃，一般是不需要写的，我们写个P就好了，因为我们不需要当弄数据，然后最重要的一个函数就是这个process就是它的一个处理函数，它要把这个数据处理成一个data的一个对象，然后去返回这个data的列表。那它一般就是便利，这个我们这个没有处理过的一个data list。然后呢，按照我们的需求，把它包装成一个data的一个数据类型，然后去返回啊，就可以了。然后这个后面的话呢，我。你在案例的时候呢，会详细的说明一下这个应该怎么写，这个大概了解一下就好，然后这是我们的一个预测数据集。

翠仪

02:19:54

然后预测数据集的话呢，它是呃，比如说像这个论文。这个扣了这个数据集呢，它是在这个plan to ID这个呃数据大的一个集上面。然后我们调用的话直接写一下这个呃，函数就好了，然后它这个是我们保存的一个路径，然后我们可以看到的就是我们的一个保存路径，就是下载好的一个数据。然后如果大家通过这个，大家有需要的话，也可以把这个呃原始的文档啊，数据文档呢是这个上传一下，然后这是它处理后的一个文档，都是一个PD的一个类型，因为后面的话它直接读到这个神经网络的。话呢，它就可以直接读取，而不需要重新去梳理，然后梳理的。

翠仪

02:20:47

然后就是这个数据的一些基本的一个信息。啊，它一共它就一个图，然后每个图里面呢都有节点，都有七个类型。然后这个节点的一个特征类型呢，它本身是有一些特征的。它一共是有1433维的一个特征，然后边的话呢，它是没有特征的。然后这是这个呃，它的一个数据集的一个基本的一个呃情况，它其实是有分这个训练级和测试集的。

翠仪

02:21:23

然后有了dataset之后的话，像这个PY touch我们都要把这个dataset呢给它包装成一个data load就是这个像我们在训练的时候，可能可以按批次去获取到它我们所需要的数据。然后在这里的话呢，data它的一个用法跟这个pouch的也是一样的，就是把这个dataset不给去，我们只要设定这个<外文>，然后还有就是，呃，是否乱去就可以了，然后我们就可以逐个去。读入啊，比如说像这里我们这个呃beach size然后B的话它是包含了一个database的一个返回的，是一个database的一个呃节点，然后它呃图的数量呢就只有一个。

翠仪

02:22:10

那大概是这样的结构，大家可以回忆一下拍照群，我们去把这个数据处理之后的话呢，我们就可以去做一个这个呃呃这个神经网这个图，神经网络。它其实包含了几个这个，呃，方，就是步骤，就是它先要把这个，呃，连接矩阵给它接上去，然后去做节点特征的一个转换标准化，这个节点特征，聚合这个连接节点信息。得到新的一个领导人要再去做。那我们可以用这个呃，就是一个官方的一个呃，GNN的一个例子。在这里的话呢，呃，我们数据呢可以先用刚刚的一个color的一个数据，那在这里定义了一个神经网络，那这个神经网络的呃，大家可以看到它跟普通的一个CNN INN的它定义并没有什。区别，因为它就是这么现成的一个调用它的一个写法就是跟PY touch是一样的。那唯一不同的就是这个神经网络层不一样，那它。

翠仪

02:23:23

我们有这个CNN的一个卷积层，那对于图神经网络的话，我们有这个G CN的一个卷积层，它的用法跟普通卷积层其实很像啊，它就是在这里呢，我们把这个数据，它的一个每一个节点。它的一个特征给它输入进去，就是它的一个这个卷积层的一个用，然后返回的是16的一个维度，然后再去，最后呢去转到它的一个呃类型，就是做了两个卷积层。CN的一个卷积层，然后最后返回的就是它一共是七个类嘛，所以就最后分成七个类的一个概念，然后它的一个呃，向前算法也是非常简单，就直接把我们的这个。

翠仪

02:24:12

呃，S和B的数据输入到这个卷积层里面，然后就是我们得到的去呃，做一个激活函数啊，嗯，加一个二层，然后再去做一个卷积，最后做一个这个soft math就可以得到它的一个概率了。就是这个点的一个概率，所以它在这个呃图神经网络的一个编写上。

翠仪

02:24:36

它的用法跟这个普通的一个呃神经网络其实是一样的，只是你的这个卷积层也好，这个网络呃结构也好，它可能是呃跟普通的不太一样。那他在这里训练的话呢，就可以去呃，非常简单的去训练，那他在这里其实就是做了一个点的一个预测，去预测一下每个点他的这个。所属于的一个论文的一个主题到底是什么？然后这个最后算出来就是做了大概是200次的一个<外文>的话，它算出来的一个准确率呢，大概是80%。然后这个如果你训练更多，然后有更多数据集的话，它的准确率呢？可以在进步的一个提升，那就是一个简单的一个样例。那关于这个PIG这个包的一个基础使用，大家没有什么问题。

翠仪

02:25:44

那个时间。图做imbalent就是要把图变成一个这个向料是吗？你说的对我我其实因。肯定是我们的表面的一些环节。呃，这个无论跟聊天机器人其实没有什么关系，真正产生关键的是知识图谱，但知识图谱它本身也是个图。这个图的图嵌入的话是明天会讲的一个内容，然后今天的话我们还是讲的是这个图，这里想的是一个图的一个卷积神经网络。所以明天会把图遍的就是突步，然后讲就是功富和才店面的这变盘我们要可能换，嗯，他们比这种活同的一般的相通数据库会有更多，呃，就其实我们。

翠仪

02:26:46

如果做了知识图谱的话，它可以在这个知识图谱的一个查找方面。就本身你查销量数据库也是查查知识图谱也是查，但查销量数据库你就完全靠那个语义的那个<外文>的一个向量去比对，但是你查知识图谱的话，你是可以通过这个知识图谱它本身的一个查询语句去查。它相关的一个节点信息，所以这时候你的精确度就会高很多。就比如说他识别到你的问题里面有一个这个，我们预先定义好的实体，但是就可以直接把这个实体的相关信息给他给你找过来，我不是说要在一大堆文本里面去找跟这个实体有关的东西。啊，可以找到实体，也可以根据实体之间的一些关系，比如说你的问题里面有实体有关系啊，它可以找到跟这个实体有这个关系的另外的一些实体。然后包括这个本身实体的一些属性啊，他也会给你输出嘛。我感觉。那为什么首先在可能股东会北京再方便一点？啊，这个就是相关的。可以是可以的。就这个图基性学习它更多的一个什么方向，可能还没有到这个。

翠仪

02:28:13

呃，问答机器人，因为它图机器学习的三大的一个任务，一个是链路预测，一个是这个节点分类，还有一个图嵌入，就是把它的图变成一个important去呃嵌入。然后它的应用场景可能就是比如说像赵老师说的这个，呃，这个电动预测就是可能去发现一些好友之间的关系啊，呃，像刚刚待会要做的这个推荐系统啊，去预测一下。

翠仪

02:28:48

这个用户到底对这个会不会跟这个商品这个点它们两个会产生一个联系？如果没有的话我们就继续了，因为时间也不是很多，嗯，就是啊，有问题的话可以在后面再继续去提问。<外文>然后下面的话呢，我们来看一下一个图，呃，神经网络的一个案例，就可能大家就不知道这个图，神经网络学来可以干嘛，那其实。

翠仪

02:29:24

呃，这里的话呢，给大家一个这个推荐系统的一个案例，那这个推荐系统的话呢，呃数据来源于这个开国商的一个比赛，呃数据有点旧了，是2015年的比赛，然后的话呢它就是。主要任务呢，就是判断这个用户经过一系列的点击之后，他是否会对某些商品去产生一些购买行为，然后他会购买哪些产品。然后这个呃比赛的地址呢是在这，大家可以在上面去下载这个数据，当然我也放到这个百度键盘上面了。大家有需要的话也可以去呃下载这一个数据，这个数据及叫做这个呃you data。

翠仪

02:30:11

那里面的话呢？呃，主要我们训练的时候会用到的两个数据集，一个是这个B，一个是这个case。那它其实数据非常的简单，它一个就是这个点击的数据，那它就是什么时间戳啊？然后这个呃ID然后这个呃现实的ID可以看作是这个。我们的这个用户的一个ID就是这个用户在什么时候点击了某个商品，然后这个商品的一个类别是怎么样的？然后就是，然后第二个呢是这个购买行为，就是说用户在什么时候啊，买了什么商品，那这个价格是怎么样的，然后数量是怎么样的，就是这么简单的两个数据。啊，这个如果是用以前的那种方法去做的这个推荐系统的话，它可能这个相关性就是会非常的弱，因为它数据量呢大呢，确实是挺大的。大家可以看一下。我读入到了这个呃。

翠仪

02:31:19

数据的话呢？大家可以看一下，然后它就是这个点击记录它的多少条数据。请的话。对，因为这个点击记录就很容易产生那个，你上网就是随便点点点点，可能一个用户在很短的时间内，你可能点了很多个不同的一个商品，因为你都是在看比较早，所以就是它的数据量呢是非常。大的，这时候你对这个数据去，而且它本身所含的信息也非常少，那首先它没有这个。<外文>数据<外文>这个<外文>商品的这个太多的一个特征。然后这个用户之间的一个特征也没有太多，就是你完全不知道这个用户他是怎样的人，你只有他的一个点击信息，然后这时候去做这个推荐系统的话呢，呃。

翠仪

02:32:17

考了哦，用这个图机器学习图神经网络的话可以是一种新的一个思路。然后在这里的话，因为它的一个section ID就是其实就是我们的一个用户ID大概是因为它通过敏之后，所以说你就看到了它的这个都是111222这样子它也不会给你一个真实的ID。所以我们需要对它的一个数据呢去做一个预处理解，然后包括它的一个item ID啊，我们要把它变成一个呃，呃，它其实可以看作是一个字符串，我们给它一个编码就好了。那就是它的一个点击记录。这是它的一个购买记录啊，购买记录就是刚刚所看到的，它的数据量呢，呃，也是挺大的。100万级别吧，就是这个购买记录，因为购买记录肯定没有那个呃。点击进入那么多？然后的话呢，我们把它的一个item ID呢，去做一个呃labor encoder的一个处理，就是它原来不是这个，不知道什么顺序的吧，我们就统一把它变成1234567跟它的一个session。同样的一个处理，那做完之后的话，它大概是这样子的，就是呃变成一个lambo的一个ID。然后因为呢，这个数据量太大了，就是考起来的话可能演示的就是有点慢，所以在这里呢就做了一个抽样的一个处理，抽了呃，10万条数据。哼。超过100万你还有数据吧？就做了一个随机的一个呃抽取，然后就是从它的一个呃，我们的这个呃。

翠仪

02:34:11

点击记录里面去抽，因为我们待会呢会从这个点击记录里面去构呃这个呃构建他们的一个图。然后的话呢，我们要获取它的一个标签，就是相当于对这个商品这个点去做一个类别。呃，那我们就可以呃把这个点呢？就判断一下，就是如果我这个商户，我这个用户可能点了十个商品，然后看一下这十个商品是在不在我们的一个购买记录类啊，如果在的话就可以做一个number two的是不在的话就是。Else的就可以做，有一个这个对应的一个label。

翠仪

02:34:55

然后的话呢，就是回到我们刚刚的，我们去构建一个dataset的一个对象。那在这里的话呢，我们把每个<外文>都看作一个节点，然后的话呢，我们。就可以把每一个用户的一个他所点击过的商品去构建一张图。那这里我们有多少个用户，我们就有多少个图啊，就非常多个图。

翠仪

02:35:27

然后呢，我们需要将这个，呃，这其实是属于一个它没有构建这个消费者跟这个item的一个关系，那这里的话呢，就转换成一个这个it，它的一个，呃，预测的一个就节点分类的一个问题了。就那我们把这个神经网络这个神经网络训练好了之后，到时候它如果有新的这个item的一个点击的话呢，我们可以把它归到它原来的这个用户的一个图里面，就新的一张图。然后再去预测新的节点的一个内容就可以了。

翠仪

02:36:07

所以的话呢，我们需要将这个数据集呢，按照它的一个session ID呢去进行这个分类。然后分类过程中的话，这个<外文>IT的也重新编码，因为对于每张图来说，它的一个<外文>的一个index呢，应该是。零开始的，它的节点的现代词呢，也应该是从零开始的，所以就按照这样的一个思路的话呢，我们来写一个这个呃you choose的一个呃。分类的一个dataset，然后呃，这个用的是这个呃，内存的一个dataset的一个继承的是一个emerging data set。那如果你们觉得这个自己的一个数据量。太大的话可以记成一个data下就可以了。然后在这里的话呢，我们主要修改的是呃，这一个就是我们的这个data set的一个保存路径。就。

翠仪

02:37:08

就是我们处理好的数据呢，最后保存到这个目录下面就可以了，因为我们读读入只有。呃，一个文件，因为我们已经读过了，然后在这里处理的过程呢，它就是呃，首先呢，我们的data呢是一个空的练习，我们会按照把这个我们的点击记录呢，按照这个用户ID呢去进行这个分组。对于每一个分组，我们会创建一张图，那在这里的话呢？首先呢？对它的一个呃item ID呢去进行重新的一个encode。啊，然后的话呢，把我们的这个group啊，这个data frame呢重新去编码。然后这个回的C ID呢，就是等于我们的这个C ID就用户ID嘛。然后它的一个特征的话呢？呃。

翠仪

02:38:03

就是把它的一个相应的一个特征呢，给它提出来。然后就是它的一个目标的一个节点，我们这个呃，原始的节点，因为我们构建边的时候呢，是要有一个这个呃，起始点和终止点去构建边。然后这个边呢，就可以从它的一个呃<外文>item ID里面去获取得到。最后的话呢，把这个呃Y呢也去给它做一个这个构建成一个cancer，然后就可以得到我们最终的这个图。那这个图的话呢？S呢是我们的这个呃点的一个特征边的话是点之间的一个呃连接。然后Y的话呢，就是点这个点有没有被购买的一个标签，然后把这个图呢放到这个data list里面。那我们就最终呢是啊，这两个是固定的，最终就可以得到我们这个datas。要呃就是呃对特赛我们初始化这个过程，这个10万条数据大概是用了半个小时，然后内存需求呢大概是十G左右。就如果大家想演示这个呃。呃，这个运行这个案例的话呢，又内存不够的话，可以把这个sample的数据量呢呃再改小一点，呃，不过就可能就是每一个这个呃用户它的图小一点，你可以按用户去进行这个。呃，抽样都是可以的。

翠仪

02:39:56

然后还是这个把这个data下去进行这个呃随机，然后分成三组嘛，就是训练的呃，呃，这个验证的还有测试的啊，下面的话呢就是构建这个神经网络，这个神经图神经网络的一个结构呢是基于。官方的一个例子略改。嗯，他其实呃跟。跟那个早上黄老师所说的是差不多的，我们整个神经结构，呃，神经网络的结构呢，就是一个这个SAGE的一个卷积层，然后去用一个token，就是一个聚合，然后它的一个比例呢？乘以0.8，然后再卷积，再聚合，再卷积再聚合。那其实这个就是跟普通的卷积层，用网络去做一个卷积层，再做一个磁化层，再做一个卷积层，再做一个磁化层就ok了。然后在这里呢，有一个呢，就是因为我的爱疼了它的一个，呃，它它，它的它一个ID呢是一个01234567这样的一个ID，所以说我们会对它的这个ID去做一个。去做一个嵌入。然后最后的话呢，会加上一些全年阶层，确保了我们最后的这个呃。它最后要返回的是一个这个概率嘛。所以我们要做一些全连接程序，返回它的概率，那它的一个，呃，整个流程的话，就是获取到数据，然后我们按照据色的一个流程啊，把我们的一个输入的一个特征。

翠仪

02:41:31

啊，去做一个imbalance之后呢啊，输入到这个卷积层，然后激活，然后去输磁化成，然后把这个磁化层的一个内容拼接起来，然后再去做第二个卷积，然后再做第三个卷积，然后最终最后呢去加上一些这个全连接层。最后输出呃，用这个呃去输出最后的一个结果，那这就是我们的一个呃，图神经网络，那它输入的是什么？输入的是一张。图这张图是由这个用户他点击了这个商品所构成的。

翠仪

02:42:15

然后训练的话就没什么好说的，训练其实跟那个普通神经网络训练的一个代码完全都是可以通用的，只是图神经网络，它特别在于你的这个神经网络的结构，可能跟平常的这个。网络结构它的一个卷积层啊，石化层啊，这个全链接层的都略微有些差别。那在这里的话训练呃训练我没有训练，大概训练的一个，它最后的一个这个结果的话，用这个AAC去验证的话，呃大概是呃0.7几的一个准确率。当你把那个呃训练训练的更多一些的话呢，呃，他所所能做的一个呃，因为他其实还没有收敛的，只是因为时间关系也没有运行完，所以后面的话。

翠仪

02:43:13

它这个准确率呢，应该会进一步提升，可获得一个比较不错的一个推荐系统的一个结果。

翠仪

02:43:21

然后在这里的话，这个被拆时呢，其实可以把它设的大一些，因为这个图的，呃，跟那个它所含的信息量呢，其实相对来说，跟我们平常所做的卷积神经网络相比的话，就是非常非常的小。就平常我们这个被size不是都是三十二，六十四等等这些最多就是100多200的。但是呢，你会发现，其实图神经网络里面，因为它一张图并不是很大，而且这个图上面这个点的特征呢，又非常的，这个特征信息又非常的少，所以我们可以把这个被赛时呢设大一点。他觉得让他跑得快一些。呃，它这个对显存道是这个呃要求没有太大就能跑，呃大概就是这个。好起来可能就是四居多吧！大概是这样的一个呃。

翠仪

02:44:26

那早上还讲了这个lot的话，PG里面的话呢，它也有一个现成的一个网络层，我们就不需要格外去写。所以这个lot的模型的话就是现成的。只是我们去做的时候呢，可能就是数据呃，可能不太一样，那我们也可以去看一下这个<外文>的一个呃，代码的一个类型。然后它这个是一个现成的一个神经网络，就是连神经网络。在这里我们还自己构建了一个呃呃图神经网络。啊，但是在这的话，你只需要直接从这个PYG里面一下这个lv这个神经网络，我们就可以直接去完成这个训练。

翠仪

02:45:16

然后这个数据的话简单一点，用这个color这个数据去进行这个使用，然后在这里的话，我们初始化我们的模型在这里输入的是什么呢？输入的是首先是这个连接矩阵，然后就是这个<外文>，就是每个节点它去做这个<外文>的时候的这个维度，然后还有就是它去做这个，呃。探索的时候啊，它的一个步长是多少？然后它的一个采样窗口是多少啊？每步走多少？每次每个节点走多少步啊，然后就是P值和Q值。然后还有就是这个每个正样本对应多少个副样本啊，这是一个可选的，就这些信息我们就可以呃，初始化一个<外文>的一个模型。

翠仪

02:46:09

然后呢，呃，这个模型的话也可以有一个模型的loader啊，去呃主要是这个dataset的一个loader呢，去确保它的一个<外文>，然后还有就是我们是否并行啊，这样的一个做做法。然后优化器的话，这个就普通的一个优化器好，好，然后就是训练训练跟刚刚的是一样的，其实就是这个去计算它的一个损失函数，然后就是这个<外文>然后就是，呃。说实话。然后它的一个结果的话呢，就是我这个上次进行的一个结果。我可以看到它，随着它的一个运行的话，它呃。是这个基本上收点在这个准确率在70%多左右，然后我们也可以去做一个这个相对化的处理，这就是它的一个呃，那4000多篇的论文。它去做了这个，呃，就每个节点它都映射到了一个这个模型的一个<外文>这个向量空间的一个点上面，然后它再把它的类别呢，给它这个。不同颜色去区分，然后画出来的一个可视化的一个结果，当然它实际的一个维度可能就是更高一些，所以我们可以看到有些这个。节点，比如说这些黄的啊，这完全就是混入了其他的类型。在这个区域里面，它的这个节就是非常的混乱，它们就一直有交叉，但是像这种区域啊，这种区域啊，这种区域大体上还是分得比较轻的。那这就是这个lotw它现成的模型啊，我们去去训练一个这个点的一个呃，分类的一个任务的话，直接可以用这个lotw去训练，然后训练完之后的话，你再调用这个lotw。

翠仪

02:48:12

然后我们就可以直接去获取到这个。漏它的一个。他最终的一个waiter的一个信息。大概是这样子的一个结果的应用，那这里只是展示成一个二维的一个节点，如果它维度更高的话，说不定这些节点虽然看上去重合的，那实际上比如说来个三维，他们可能是在不同的一个立体。空间里面的？就是只是只是我们人眼啊，能看到的最多就是三维的一个信息，在高维的话就没办法表示出来。

翠仪

02:48:54

那大概就是这样的一个呃结构，那大家看一下对于这几个呃这两个案例有没有什么问题想问的？因为的推荐的案例，它的构造，它的输入是什么呢？模型的输入是？就是一个图是吧？对它的图的构造是一个人，然后边就是他购买了什么商品是吧？嗯。这个点击的什么商品？是啊。就是图的，那输出是什么呢？输出就是他购买某个商品的。就是每个节点的它一个类别。节点就是人，商品就是它的节点。商品就是节点，就哪个商品就两个类别会买，或者是不会买是吗？他是指的是某个人会买的是某个人。

翠仪

02:49:53

啊，某个人会不会买这个商品，就是我我这个模型训练出来了，然后我就输入这个人的图图，这个人的图就是这段时间他点击过什么商品，然后输进来，进来之后他就会输出一个，就是会输出每一个节点。

翠仪

02:50:11

一个商品对对对对对！贝贝看看线上的同学有哪些同学需要提问的？

风铃之音

02:50:33

嗯，大家还有什么问题吗？有问题的话可以举手啊！

王蓓

02:50:40

田志军同学有问题，我看他举手了。

翠仪

02:50:42

看。

田志军

02:50:49

哦，我我没有问题，我可能悟出了。

王蓓

02:50:52

啊ok。

翠仪

02:50:54

嗯。

王蓓

02:50:56

那线上目前暂时没有其他的同学提问。

翠仪

02:50:56

看。Ok那这个推荐的那个直接就就是可以。放松它这个能够直接用了吗？呃，理论上是可以的呀！就就只是你每次输入的时候，其实就是要重新获取到他最近的一个点击行为去构建这个图，要输到这个网络里面。

王蓓

02:51:21

所以固定的收入就是这么多。希望能够要交1000？

王蓓

02:51:27

四个零啊。四个人也不多。

翠仪

02:51:31

诶贝贝你那边是不是有问题还是没闭麦？

王蓓

02:51:32

15块六。嗯ok啊，抱歉抱歉，刚刚误操作了一下。嗯，现在我们是要课间休息吗？

翠仪

02:51:43

嗯，不用了，他就快结束了。二级。

王蓓

02:51:47

Ok那你就继续吧！

翠仪

02:51:49

Ok呃，其实今天还有最后一点内容，这个图数据库反正这个数据图数据库的话呢，其实等到明天讲也差不多，因为明天才会去讲到那个知识图谱的一个构建，那在这里的话也先简单介绍一下。然后这个大家网上回去这个可以先装一下这个图数据库，因为它不是这个P。的一个拓展包，所以就没办法在这个虚拟环境的一个安装列表给大家去呃这个提供，但是我提供了这个windows版的一个安装包，然后如果是呃linux版的话，可以按照后面的一个教程去进行这个图数据库的安装。

翠仪

02:52:31

那在这里图数据库呢，它指的是它以图这种结构形式去储存我们这个数据的一个数据库啊，并不是指说我们只储存图片的。然后的话呢，它这个跟传统的一个关系型数据库里面，呃是完全不一样的，因为它以这个呃，它存储的方式就是这个图加上这个图之间的一个连接。它对于这个知识图谱来说的话呢，呃会更加的一个简便方便。

翠仪

02:53:06

然后现在呢主要有这几个主流的一个图数据库，然后我们这里的呃，主会，呃，主要是讲这个<外文>，因为它做图数据库呢是相对比较成熟的，而且它那个社区版开源。啊，虽然用的限制比较多啊，但是如果作为一个样例来说的话还是足够的，但如果你要做这个比较大型的创业板的话，可以用这个呃，U GG是这个百度去做的一个图，数据库。

翠仪

02:53:40

也是开源的，然后它会支持呃，这个更大规模的一个数据，然后这个数据规模指的是它的一个节点的一个数量，因为你这个图数据库你随随便便都可以。这这个弄出几百也就几万个节点。然后它的一个效率呢，也是相当于比较快，然后这个单可能这个完善程度的话可能比不上<外文>，因为<外文>的话呢，它这个做的比较久，然后它是这个。呃，<外文>这个它是这个16年的时候才开始启动的，但是因为说国内的这个厂商，所以它的中文的支持度还是比较好的，嗯，它其实也是改这个开源的一个数据，嗯。然后还有就是它的一些拓展性啊，还有这个等等的这些比对，它也支持这个HTTP，然后像的话主要还是以这个呃。

翠仪

02:54:45

Web的形式去展现它的一个图的一个类型，因为图数据互不跟，跟传统的数据互不一样，传统数据不就是关系式，数据库是表结构，然后所有的数据你都可以直接用表的形式来去。查看去查询那图就图数据库它本身结构是一个图，所以它都是要配合这个呃web端去进行这个图的一个查看的。然后它的一个存储的一些功能啊，节点大家可以就是就回去的时候仔细看一下这个图，数据库，然后接下来简单的去介绍一下，如果对的话，它是一个<外文>的一个图。

翠仪

02:55:31

数据库，然后它本身呢是以java就是已经写的，所以的话你要装的话呢，呃必须要先装这个java，然后呃它有几个版本，然后现在的话<外文>呃你去搜这个<外文>的一个教程的话。

翠仪

02:55:48

大部分是基于这个硫的一个3.2的一个版本去做的，然后的话呢，后续就是我们用拍出去连接六合堆的话有很多这个PY to六这个包呢也是基于这个六G的一个3.2的一个版本。但事实上的话，这个它现在已经支持了，呃，现在就已经发展到了，最新的话是4.2和5.1几的一个版本。呃，那个四个五是一个并行的一个状态，是是一个相对稳定的一个版本。五的话相相对于一个这个开发板之类之类的。

翠仪

02:56:24

但是<外文>建议大家先装这个4.22的版本，就四点S那个版本呢，相对来说会比较稳定一些。然后大家去装的话呢？嗯。可以去。下的这个去官网上面直接去下载这个安装包。呃，记得呢，要选择这个社区版，因为社区版是免费的，然后商业它闭园的话呢是要收费的，这是一个企业版，然后就是我们的一个社区版的一个服务器，它有5.9的版本，还有四点四点二二的一个版本。好，大家可以根据自己的一个需求去安呃下载就好。比如说像windows的话，它其实连安装包都不是，它只是一个压缩包啊，我们把这个压缩包下载下来，就可以直接解压到你想放的一个目录下面就可以了啊，就是完全没有任何的一个安装成分。但是的话呢，呃它需要用这个java去启动，那如果是这个你下载的是<外文>的一个试点S的一个版本的话，呃，配套的是应该windows下面呢是java11的一个版本。

翠仪

02:57:41

那昨晚11的一个版本的话，因为下载它可能要登录要什么的啊，我也尝到了这个。呃，这这个百度云上面在windows版本的话就可以去使用，然后它下载下来读，说我下载下来的，我放在了这个D盘这个目录里面，然后它解压出来就是这个东西解压出来就是这个六。的社区吧，然后完了我们进入到这个并这个目录。然后呢，它启动呢就非常的一个简单，我们用这个呃在这个CMD窗口拉出来，然后CD到该目录下面。然后呢，直接输入六合？

翠仪

02:58:26

他就可以启动这个loft的一个数据库了。当然，你也可以把它这个作为一个windows的服务呢去启动，那就是这个运行一下这个。就可以了。哦，就说这已经运气了。孙权是没有关是吧？然后如果你就说这服务开启的话呢，它默认的那个呃。端口呢是7474？就是我们用这个local host的骑士骑士。就可以访问到我们的一个六合追的一个。这个浏览器那在这里的话呢，如果你是第一次去进入的话呢？呃，你的初始账号初始密码就是六密码也是六G啊，当然我这个你第一次登陆之后呢，它会要求你。改密码。比如说我改了个123456啊，就是呃就可以连接了。连接进去之后的话呢，我们会点击一下这边。呃，如果你是一个全新的数据库的话，你应该是看到啊，看不到任何的一个节点的，那因为我这个明天的一个课程上面是会讲到一个这个。金融数据库的，所以我这里呢是已经有一些节点在的。

翠仪

02:59:58

它的一个查询语句呢就是。大概是这样子的一个查询语句。嗯嗯。那在这里是什么意思呢？其实我这个节点呢，它有节点的一个类型。比如说我这个金融的一个数，这个数据库啊，它有这个股东啊，股票啊，啊，然后还有概念公告等等这些类型。然后我可以看到这个，呃，股东之间呃，的一个节点，然后因为这个边可能没有这个连接，是因为这个股东之间呢，我没有构建股东和股东之间的一个连接。然后修改完密码就可以了，然后如果是乌班图的话，这个你安装更加简单，你甚至不用下载。呃，你就直接呢去把这个new for J它的一个呃密钥添加到你的这个本地上面，然后把new for J的一个储存的一个库呢，也添加到你的APG。

翠仪

03:01:09

第一户里面，然后呢就可以去进行这个APT的一个安装了，安装完之后呢，启动一下就可以了。那我这里显示不了，因为我的这个，呃，乌班图的一个服务器呢，它没有可视化界面，所以就是我开了这个服务的话呢，我可以用开始去连接它，但是没有像这个windows这个界面，那如果你本身的这个服务器。它本身这个一般都有这个图形化界面的话，你直接访问呢，也可以得到相应的一个呃内容的知识。

翠仪

03:01:44

内容的一个情况。然后它启动的话也是这样子，那方便我们后续去这个呃安装，然后的话它本身是有这个像sql一样的一个查询语句，那S sql它叫这个cyber。的一个查询语句啊，包括这个节点的一个创建啊，关系的创建啊，索引的创建啊，还有就是一些这个查询啊，删除啊等等的这些节点啊，大家有兴趣可以看一下。

翠仪

03:02:16

就它会在这里直接运行的，就是这个cyber的一个查询语句，那这个查询语句的话呢，建议大家去呃，先学习，先先瞄一眼吧，因为在后面到了知识图谱查询的时候才。用到这些查询语句，然后没什么的话呢，呃，因为它这是最原生的一个查询语句，所以说的话呢，有时候就是即使我们用拍摄去连接的话，并不是所有的一个查询都能做的。

翠仪

03:02:48

可能还是需要用这个<外文>的一个语法去写我们的一查询语句，那最基础的一个查询语句就是它的一个呃，match的一个查询语句。它就是里面是它的一个内容，就是把这个节点去查出来。那刚刚我们所看的，比如说我要看这个股票，那它的一个查询语句呢，它其实自己生成的就是这个N它的类型就是股票，然后。呃，这个股票之间呢是有一些关系的，我们可以把图放大一些。那这是这个股票之间的一个相关性，这个边的话大家可以可以看一下，然后这个图呢都是可以动的。大概是这样子，那大家今天晚上的任务呢，就是先把六合给装起来吧，然后后面就给大家去介绍一下怎么去啊，创建我们的这个数据库，然后还有就是呃，这个知识图谱。然后还有就是知识图谱查询的一些内容，然后还有就是这个图嵌入的一些内容大概是这样。那今天的那个课程大家有没有什么什么问题呀？

翠仪

03:04:02

好吧。如果没有的话，我们今天的课程就先到这了。刚才我说的智能手上生产自己的是别人已经预训练好的那个模型可以直接拿来用，就刚才那个推荐的那个推荐那个，他其实我好像没有看到过有预训练好。没有，你心里好了。哦，你自己去练一下也很快。我看。取了十分之一都要好多个钟头。30个基。嗯，就就是拼硬件了。因为别的训练好的那个，他可能就就是大数据结构可能不太一样，就图的一个构成什么什么的结构不一样的吗？呃，也可以这么说，跟你不一样啊！呃，商品不一样。你跟王的肯定不一样的啊！但是就是商品我增加一个商品就得重新训练过一次吗？他的那个他的那个就可能你可能提供更多的一些信息，也不一定。

王波

03:05:35

Hey. 还能提问吗？哦，你好。那个我那个今天下午有有一块没有听啊，所以我不知道讲没讲，就那个你embedding你说你是微调之后的，然后我看那个微信上那个福凯说了两句，但我没看太明白，就是说这个微调具体是。怎么做的。

翠仪

03:06:03

那条其实就是做一个这个句子对的一个相似度就是用what去训练的呀。

王波

03:06:13

哦。

王波

03:06:15

对，我看那个微信上，他说是用模型来做向量的，你这模型。

王波

03:06:21

嗯这这。

翠仪

03:06:21

嗯，不好意思说我没有看微信，因为在上课，所以所所以我不太知道你说的微信是哪个。

王波

03:06:24

哈哈。啊。就是这。哦就。那个福凯福凯那个回答的我没看明白，就是那个embedding微调你是说用birt重新做的是吗？

翠仪

03:06:37

呃，就是这个<外文>它本身也提供了这个训练的一个。语句啊，在这就是他我我现在没有共享屏幕，稍等一下。就是它的这个模型的话，它也提供了训练代码。你只要把数据上呃这个调整一下，就都可以去进行这个呃追加训练的呀。它的格式类型就是两个句子输入进去，然后给它一个<外文>就是，然后它这个的话可能就是一个相似度的一个打分，还有如果你没有打分，你也可以是一个零向量都可以。

王波

03:07:07

哦。

王波

03:07:22

哦，就是再加一些你的自己数据局加到这个<外文>里面，再重新训练一下，是这意思吧。

翠仪

03:07:27

哦，对，你可以在基于它的那个提供的一个数据模型上面再去训练去加训练吗？

王波

03:07:36

就是具体的来说，就是说我如果我对这个，假如说我对这个。

王波

03:07:43

呃，大圆模型我做了一个语料的。

王波

03:07:47

微调那么相应的是不是我这个embedding也对应微调了，还是说我就直接用你这个？

翠仪

03:07:47

这几年。嗯。不是可以<外文>是吗？然后这个大语言模型的微调跟这个emailing模型的微调它不太一样。这个模型的微调主要是为了让我们在这个相当数据库里面查询，可以查到更精准的一个结果。

王波

03:07:54

啊。

翠仪

03:08:09

而大语言模型的微调的话，主要是为了让这个大语言，呃，它一般都是涉及到一些指令的微调，呃，可可以更专注于我们想要它这个专注的一个领域上面。

王波

03:08:09

哦哦。哦，明白了就就是说。

王波

03:08:26

呃，就是说我明白了，这应该是两码事儿是吧，嗯。

翠仪

03:08:26

背好了。对对对，就就是两码。

王波

03:08:31

哦，好明白了嗯！

王波

03:08:33

好，谢谢。

许涛

03:08:40

喂。

王蓓

03:08:40

许涛同学可以提问啦！

翠仪

03:08:40

诶那个。啊。

许涛

03:08:42

嗯嗯，行。

翠仪

03:08:43

好，那行。

许涛

03:08:44

所以说我问这个问题就是那个。

翠仪

03:08:45

那个。

许涛

03:08:47

呃，就刚才咱们说那个项链数据库，就是它，咱们现在常见的这种项链数据库里边，它自己没有做这种嵌入的这种功能啊。

翠仪

03:08:47

这个。他咱们现在？<外文>看。有啊有。就呃，不过不是我们现在用的这个相当数据库。

翠仪

03:09:06

别的是有的。

许涛

03:09:08

呃，就比如说刚才刚才不是我看咱们PPT里边演示的，就是说比如说送给一个大模型，然后项链画完了之后，然后把这个结果放到项链数据库里边去吧。

翠仪

03:09:14

需要。

许涛

03:09:21

嗯，那那我的理解是不是有一些什么，现在他本身他自己就可以做这种切入？

翠仪

03:09:21

对呀。你要嵌入的话，你可以去设定它，你在创建的时候是否要用嵌入啊？

许涛

03:09:28

不。这种向量化的这种操作是吧？

许涛

03:09:36

嗯，那那哪一种方式比较好用一些都是让让他自身自带的这种这种，比如说。

许涛

03:09:43

让它自带的这种嵌入的功能是好使还是用单语言模型的这种微完了之后这种。

许涛

03:09:49

下来。

翠仪

03:09:51

它其实这个模型它并不能算是一个大语言模型，因为它<外文>更多的，像这个它还是基于<外文>系列的一个模型分系列的模型，它。

翠仪

03:10:06

啊，你说它大吧也大，但是没有像这个GPT那样子，这么大规模的这个预训练的一个模型呀。

许涛

03:10:11

那比如说他这个<外文>下列数据库，它自自己的这种，如果他有这种，比如说自嵌入这种操作的话。

许涛

03:10:22

那他应该那个模型应该基本上也可以理解为，也就是一个。或者或者这种一个模型的一个东西。

翠仪

03:10:29

对呀对呀。对呀。

许涛

03:10:31

嗯，是那那咱们刚才说的那个叫叫什么叫下来数据过来那个那个开源的那个就是。呃，就是那那那个里边他本身他那自己是不是也也也有这种是嵌入的这种？

翠仪

03:10:41

嗯。这个，嗯，我们现在用的这个案例里面，这个<外文>是没有做这个<外文>的一个模块的，它因为只是一个很轻量级的一个相当数据库。

许涛

03:10:46

不能。啊好对对对！

翠仪

03:10:57

它提供的功能就是向量搜索，就是进相似性的一个数据。如果你，你要做这个embody的话，你可以选择像这一个，它是由。

许涛

03:11:00

啊，你就是说。呃，就是咱们就咱们把。

翠仪

03:11:08

自己的一个<外文>一个模块的。

许涛

03:11:11

那那我再问另外一个问题，就是那个下列数据库里边。呃，就咱们用这个东西的话，它是不是也得需要一个比较好用的显卡啊？还是说怎么着嘛？

翠仪

03:11:25

呃，现在数据库的话显卡的。它是可要可不要，它不是一个B选项，但是你有显卡，你有CPU这个硬件的话，它可以这个查询速度更快，那这就要看你的这个。嗯，数据的一个规模了，比如说像我这种案例的这种，其实基本上用不上这个GPU的一个加速。

许涛

03:11:53

嗯，你这种GPU用在下单数据库上，它这个效果明显嘛，我是不是应该也会？

翠仪

03:11:58

明显，当你这个呃数据库的量级很大的时候，它这个呃效率就高很多。

许涛

03:12:08

哦，因为因为是我的一点是因为咱们平常说是GPU干嘛就是说呃。比如说在机器学习里边，或者是做一个这个神经网络里边。这种做操作可能它是比较好的，肯定是要这个东西肯定是。呃，最优化的嘛，但是如果仅仅是说做一些向量的搜索查询什么，那应该说用GPU还是还是也会好的是吧？

翠仪

03:12:26

因为你涉及到这个向量之间的一个计算，你肯定用GPU定型的话比CPU快很多。

许涛

03:12:33

哦，那那那这个比如说咱们刚才说的这个Q的什么，就是说这些，他们这个他他是带着这种GPU的吗？或者或者支持这这个操作的嘛？

翠仪

03:12:46

哦哦。支持，只是它云端的一个免费版本就没有这个C PGP U的，它用的是CPU，你在本地附属的时候可以用GPU。

许涛

03:13:01

行，那我那我知道。然后我再问一个问题就是。就刚才说的那个图的那个就是机器学习里边，我想问一个问题，就是你像这种节点，如果特别。

翠仪

03:13:13

对吧。

许涛

03:13:13

大量的节点的话就是巨量的这种节点的话。

许涛

03:13:16

嗯，就是这个他这个效率这个问题。

许涛

03:13:20

呃，会会造成会成为一种瓶颈吧！

翠仪

03:13:25

啊什么没没听清后面的那个问题？

许涛

03:13:28

就是它这个图里边这个节节点数特别多，就比方说是那个。

许涛

03:13:32

呃，数量特别庞大的这种。大数据量的这种。接点。呃，那是不是应该跑集群呢？是不可以这么理解这个图也可以跑在集群上是吧？

翠仪

03:13:39

呃，上个的话因为图把它本身所带的信息。它其实所占的一个内存是非常小。就除非你这个图的节点，它的属性可能是很高维度的属性，不然一般的话，你只有点的一些信息，还有这个边的一些信息肯定是比你。这这个图像啊，音频呐，这种小很多。

许涛

03:14:13

嗯，是是这样，因为我们现在就是。

翠仪

03:14:13

这节点多的话，它这个跟普通的一些。呃，跟跟你说你去做这个就是比如说计算机视觉，它这个图片特别多，就不是一个量级的呀。

许涛

03:14:26

呃，就是因为我们现在有有一个项目是。呃，大概的预估的话，大概是在1000万左右的节点的这么一个一个图。呃，这这个的话就是用在咱咱们说的这个刚才咱们说的这个这个这这这个上面它应该是是应该是还是可以的是吧？如果1000万这个量级的这个节点的话。

翠仪

03:14:50

呃，应该是可以啊1000万。

许涛

03:14:54

我就怕这个1000万的话，是不是可能就是他最后就算不出结果来了？

翠仪

03:15:01

呃，也不至于要看一下你做的是什么运算啊？

许涛

03:15:04

你这就是做一些搜索嘛，然后就是。

翠仪

03:15:08

搜索的话，这搜索还这个有一些这个优化的算法。

许涛

03:15:15

然后做一些分类，还还会一些做一些类别的一些操作，就这种。就是。大概种一些东西。嗯，那这个然后就是我再问一下这个图计学习和这个图计算它俩有啥区别啊？

翠仪

03:15:36

啊什么怎么没听清？

许涛

03:15:37

他他他他他俩有有有有区别吗？就是这个图机器学习和这个图计算，因为我之前也经过图计算这个概念。

翠仪

03:15:45

图计算是，其实说的是图论本身它的一些计算，比如说像这个图的度啊，结结点的度啊，然后还有就是它的一个什么最短路径搜索呀，最小生成树啊，这些都是属于。

许涛

03:15:55

啊。

翠仪

03:16:00

图的计算，它比较偏向于图论的一个呃范畴，图机器学习的话，其实就是呃，把图当做是一种特殊的一个数据输入，然后其实我们做的还是一些机器学习，去做一些预测。比如说像节点分类啊，这种预测。

许涛

03:16:23

就是，那就是做一些这个预测或者做什么东西，可能是就要初级学习了。

翠仪

03:16:25

你说。

许涛

03:16:28

然后那些算法里边那个就应该可以。

翠仪

03:16:28

对。

许涛

03:16:30

你基本上除计算是吧？

翠仪

03:16:34

嗯。

许涛

03:16:35

行行行那那我没有钱。

翠仪

03:16:35

对的。

王蓓

03:16:48

目前线上没有其他的同学在举手提问了。

翠仪

03:16:52

嗯，那行，那有问题，那明天再说呗，我们今天就先到这了，明天再见了，拜拜！

王蓓

03:17:00

啊。