# 分布式特训营B
关键词

大语言模型

分布式训练

机器学习

参数服务器

函数

并行

多机多卡

神经网络

数据集

搜索

王蓓

00:00

进来了。

翠仪

00:00

嗯，好的行。呃，我们再等个一分钟啊，等大家进来了，我们就正式开始下午的一个课程。

翠仪

00:50

Ok时间差不多，那我们就开始下午的一个课程，那黄老师上午讲的是这个分布式，还有这个，呃，为的一些大概的一个介绍，那为这个框架大家可能还没有太熟悉，那在下午这个开始的时候的话，我就。

翠仪

01:09

给大家再详细的介绍一下这个框架，这个框架其实它最开始做起来的时候，当时还没有这个大语言模型。呃，所以它最开始做的时候呢，是针对于这个机器，呃，机器学习的一个分布式的一个计算。呃，因为当时就是他这个大家如果有留意啊，去或者是搜索过一些未的一个呃资料的时候的话呢，呃，其实会发现一些比较老旧的资料的话，他写的这个模块跟现在的一个模块可能有点不一样。啊，因为那个是1.0的一个版本，现在这个已经升级到2.0那个版本了。那所以说呢，它对这个它最开始的话，可能更偏向于机器学习的一个训练的一个支持。然后的话呢，呃，现在的话呢，更偏向于跟，呃，这个其实就是大语言模型起来了之后的话，它就是呃，可能会更偏向于这个AI的一个应用啊，当然它也仍然是有这个强化学习的一个模块。那它我们可以看一下它现在的一个架构的话，其实它主要能做的就是这个，呃，去做一些这个前期的一个数据的一个预测部分，然后分布式的一个训练，然后还有就是分布式的一个部署，然后还有就是这个模型的一个超参的一个调，呃，调优。

翠仪

02:35

然后还有一些这个呃商业逻辑的一个模拟计算仿真，因为它前期的话更多的呃，在于这个呃强化学习上面强化学习，它就需要非常大量的一个呃，环境的一个仿真环境去进行调用，去进行交互。就是其实就很适合用分布式去进行这个相互的一个操作，因为它可以分出来一个呃，一个worker啊，去做一个呃，环境的一个更新模拟，然后另一个去计算它的一个收益，然后来回去进行这个通信结算。啊，这个是它的一个。

翠仪

03:15

呃，RL是这个V的一个强化学习的一个呃，很重要的一个拓展包，然后它底层的话呢，就是就是它底层，然后它本身呢也是支持云原生的，所以它很容易呢可以拓呃拓展在一些这个呃K八S的一。服务器上面或者是这个在docker的一个容器上面。那呃，any school的话呢，就是说它其实是R的一个商业公司，就是如果大家觉得这里部署一个这个we的一个集群比较麻烦的话，也可以直接在这个any school上面去。购买到位的一些呃，现成的一些服务，它其实就是基于为的去搭成的一套现成的一些服务框架，让你可以更方便，更便利的去使用这个位，但是他他们现在可能还没有大规模的一个拓展这个。

翠仪

04:06

呃，它就是它没有，还还没有达到一个很成熟的商业化，所以他他们公司的一个呃，试用的话还是很传统的，外企的，它可能会有一个销售来跟你聊一下你的这个呃应用的一个情况，再给你通过试用。大概是这样子吧。然后它的一个核心的话，因为它的本质上呢，就是非常呃，这个呃是支持这个python的。所以跟很多python的一些原原生的一些函数都可以使用起来。那呃，我们只需要将原来的一个python程序呢？进行一个简单的一个改变，不需要完全不需要重写就可以让同样代码呢在多台机器上去运行，或者是在一台机器上去做这个并行的一个操作。而且的话呢，它本身呢是呃相对来说会比较稳定。就不像这种spark它如果你某个节点你断掉了，你程序可能坏掉了啊，它本身的一个设计呢，就是有一定的一个容错率的，就是你某个节点不通了，或者是有问题的，那它就会自动的去调节。就是说我不用这个节点啊。我再用别的节点去做啊，就这个节点通信啊，拼不通啊，那我就当它这个暂时是故障了，然后等后面我拼通了之后，我再给它分配任务，大概是这样子，然后它性能上的话呢，其实啊，在现有的这个分布式的一个系统上面来说，它性能上的上非常的一个。

翠仪

05:36

呃，当你这个呃，在同样的一个网络环境上面的话呢，它的一个呃通信的一个损耗呢，其实已经压到了很低。

翠仪

05:48

然后它的这个2.0呢，相当于于1.0的一个新特性的话，它就是把它的这个ML就是机器学习啊，这个机这里的机器学习其实包括了这个，呃，深度学习在里面，它把所有的一个机器学习的模块呢，都放在了一起啊去。都有这个叫做R AI R的一个模块啊，统一的一个工具包。然后在2.0版本的时候还是一个贝塔版本，但现在最新的rain呢，已经到了二点六点一的一个版本了，所以说它已经是一个相对来说比较成熟的工具了。它非常简单啊，而且封装了非常多的一个接口，可以跟这个常用的一些深度学习框架像。啊，还有就是有一些这个呃，我们这个大语言模型啊，或者是呃B面的一些模型啊，他们都在哼，跟上面去做的，那我们也呃，他们也拓展了这个对亨等等的支持，那后续我们就讲到它的一。

翠仪

06:51

核心运用的时候，大家可以看到，其实我们仅仅需要对我们原来的代码去做一点点改动，我们就可以把我们整个程序呢去啊，快速的变成一个分布式的程序，去做一个更好的一个并行的一个处理，提高我们的这整个。呃，价模型它的一个呃运算速度啊，还还有对并发的一个处理能力。而且它现在的话呢，就是2.0版本的话，对这个呃数据呢，本地数据它有了更大容量的一个支持啊，现在可以支持到100TB了啊，或者是就更多的一个数据，特别是在我们在。训练的时候，同学好像需要对这个数据去进行一个乱序的一个梳理，那他都能做了。

翠仪

07:38

然后还有就是加了一个就就，呃，酷比其实就是在K八S上面去运行的一个a的一个包啊，然后就可以取代了传统的这个ray的一个operation。然后还有就是它的一个就是它的一个服务器，端的话呢，就是建立了一种更简单的去部署我们的应用，那或者像我们之前去部署应用，你可能还得写一个fast接口，或者是写一个fast API接口。啊，现在有了这个的话，你只要一毫厘米啊，你就可以建立起你的这个呃接口啊，这个API了更简单，更方便。

翠仪

08:21

那它整个力能力上来说的话，它可以做什么呢？第一个可以做数据集的一个预处理啊，那这些预处理的话，通通都可以去做一个并行的操作啊，就是包括这个写预呃，就是读取啊。呃，然后去呃这个去做一些转化，像这个可能大语言模型对数据的呃，就是呃这个呃，LRM对这个数据的预处理部分可能会比较少一些，但是像一些这个计算机视觉上面的一些模型啊，都是它对图片的预预处理可就多。呃，比如说像这个图片啊，它可能都要统一成一个呃输统一的一个格式，因为我们输入的模型里面肯定需要统11个格式，然后可能会做一些数据增强的这个这个操作，比如说你在做这个。呃，图片分类的时候可能会对这个图片啊，去进行这个旋转啊，随机切割啊，等等的一些操作，为都提供的这个线程的函数，然后可以对我们输入的一个呃，数据的一个B呢，去做一个批量的一个操作啊，就是大幅的提。

翠仪

09:29

把我们的一个呃计算的一个速度，因为原本我们可能是一个串行的啊，有的包装最后呢我们就可以做一个并行的一个呃框呃，这个计算了，那第二个就是深度学习啊，它去呃对这个。呃，拍啊啊啊等等的一些知识呢，呃就就更多了啊，待会待会会讲到，然后第三个就是说这个超参数的调优，那可能在这个深度学习里面，超参数的调优会比较少一些。但是像传统的一些机器学习啊的话呢，我们经常需要去做这个调优，那调优的话，大家不知道有没有做过，就是如果你的数据量很大啊，你用的像这种什么啊，茶具boost之类的一个模型啊，你训练一个模型。等久了你还得去做这个那么大规模的一个调优的话就速度贼慢，那有了这个bing之后的话，可以加速我们的一个参参的一个搜索啊，并且可以使用各种的一个搜索算法，提高我们最终的一个搜索效率。

翠仪

10:35

啊，这个是啊，这个比较偏向于机器学习方向的，然后训练大模型，这不用说了，就是大家都说这个TPT啊，其实都是用这个谓语的一个框架呢，去进行这个分布式训练的啊，当然这个它到底实际上用的是不是谓语？啊，这个也不好说，因为其实为出来了之后的话，也有很多公司了，基于这个位的一个架构了，去呃再去进行二次的开发去呃呃做了自己的一些这个呃分布式训练的一个。呃框架啊，比如说像呃蚂蚁金服，呃，他们这个整个这个呃并呃分布式训练的一个架构，底层都是为，但是他们针对自己的一些金融场景呢，去做了更多的一些优化。呃，大家有兴趣的话可以看一下蚂蚁金服的这个相关的一些文章啊，他给我介绍他们的那个模型好，呃，不是不是那个框架好像叫做淋雨啊，当然我不知道他为什么叫这个名字就有点拗口哈，也不太容易。啊，就是这个呃训练大模型的，就我因为现在这个就我谁也不知道到底它底层用的是we和肯定是为，但是是不是原生的就不好说了。

翠仪

11:58

然后还有一些资源的一个调度，那就是它可以动态的去调整我们资源，就是我们的一个节点的话，你可以随时去加入到我们的这个资源池里面，也可以随时的一个退出啊，它都是可以动态调整啊，对我们的一个任务影响倒不是很大。然后还有就是模型服务啊，就是它可以快速的将我们的一个大模型呢部署过来，而且而且可以这个呃，你只需要写一副一步呃，一次的一个代码，你就可以把这个模型呢推到你的整个这个集群上面。啊，非常的一个简单方便。

翠仪

12:33

那强化学习就不说了，呃，这个从1.0来说的话，这个都是它的一个呃很常用的一个框架。呃，包括这个大语言模型，它后期不是用这个呃人类反馈的一个强化学习去最呃，这个调优嘛啊，这其实也会用到它的一个框架，但现在可能强化学习的一个热度会相对比较少一些，就自从alpha go之后的话。现在好像就没怎么人提这个概念了。那所以后面的话，我们对这个who呢，可能就简单介绍一下，就不会深入去讲解。那它这个针对所有的一个P应用呢，其实都是通用的，就不仅仅是说机器学习啊，深度学习这一块，只要你想做这个分布式啊，只要你想做这个。呃，变形数点啊，你可以单击的一个便携啊，多机的一个分布式，你都可以用vc中的一个呃这个装饰器啊，简单装饰一下你的这个函数或者是这个类啊，就可以达到这个呃分布式的操作。啊，但是这个效率等等提升多少，这就是要考虑一下你本身的这个呃，运算的一个效率，还有就是通信之间的一个所带来的一个时间损耗。啊，它还有这个批处理的功能就啊，提高我们就是大量的一些呃，推断的时候啊，去批批量去做这个推理的时候的一些能力。

翠仪

14:06

那整个架构来说的话，它其实是基于一个呃呃，云服务器啊，这个云应用啊，你用什么云的无所谓，你用亚马逊云，腾讯云安利云啊，这个都行，就是其实是一个云服务器啊，然后因为的话呢？呃，我们要布这个集群的话，这个集群之间的一个机器呢，肯定是要能通信的，那你要不就是就说你的这个服务器有一个固定的一个公网端口啊，是有对外开放的，那这个相互之间可以通过公网端口去进行这个通信。啊，要不就是要有这个内网的一个端口啊，内内网是互通的，要不就是公网是互通的啊，这个都需，呃，对对，你起码网要通，你网不通的话呢，这个就没办法去做了，然后这个它的一个底层的核心呢就是。

翠仪

15:00

呃，基于这个we的一个在上一步的一个呃顶层的呃，稍微顶层一点的一个运用呢，就是呃这个RAR的一个几个模块。那它的一个核心有几个概念呢？就是这个呃，通过这个task和object去分配我们这个任务和资源。它呃，有一些相关的系统啊，大家可以看一下，比如说它的一个集群的话呢，其实它呃提供了在K八S上面去运行的一些操作啊，或者是说直接用这个容器去实现的一些集群。

翠仪

15:45

然后跟呃也有同学问为什么这个我们要用位，而不是用？那其实的话呢，它这个对你来说的话，相比这个斯巴克乐克会更加灵活。另一方面的话呢，如果你这个呃，就是你目前的一个工作流程上，其实已经用了这个spark的话啊，但这个后续比如说要去进行这个模型的训练。哦，要用的话它们两个也可以是这个相互的一个呃数据的一个流动，你现在这个18的呃这个上面处理完数据再传到位上面去做这个模型训练也是可以的啊，就是并不是说我们用了。但是它非常灵活，所以就是呃，可以是一个流水线的一个工作流程。然后就是跟其他的一些这个系统啊的一个对比。那呃，它的一个底层的话呢，是这个呃是一个呃，本质上就是一个分布式的一个计算框架。然后它的话呢，呃，其实它这个分布式的话不限于这种深度学习模型。你只要任何有一个函数或者有一个类，我们都可以转换成一个分布式的一个20和test。啊，就是pass的话我们会封装成一个ar。然后如果是分的话呢，就是函数的话呢，我们就会呃分呃分呃这个转换成一个分布式的一个任务，然后呢在所有的一个机器上呢，去进行一个分布式的一个运行。而且的话呢，呃，这个text和a之间呢，可以通过这个分布式的一个对象存。就是有一个这个object来达到这个呃共享的能力。那这个的话就是呃，这些能力的一个基础的一个框架。

翠仪

17:42

那大家可以看一下，这是一个温柔的一个样的，它其实看起来跟我们普通的这个呃，python的一个呃，脚本没有什么区别，那待会就会给大家去对比一下原生的一个python脚本和这个我们用V去做的一个脚本啊，它到底需要做什么去改？那其实大概就是会在这个cross啊，这个类上面去做了一个river。呃，remote的一个装饰器，然后写了这个之后的话，后面的话呢，只需要启动它的一个remote，然后通过这个get一个方式呢，去运行，获取最终的一个结果，大概就是这样子的一个结果。然后刚刚说的这个呃R AI R的话，其实它全称叫做R。AI NT啊，这个简称就是a呀呀了，那它就是针对于这个机器学习应用场景的一个扩展和这个增强。那这个其实你要你不用这个框架，你直接用这个vc呢，也可以完全写。但是就是没有那么方便。那VAR的话，它就是提供了更多的封装好的一些这个呃，接口给我们，要不我们可以在不同的一个场景下面呢？可以快速的完成我们的一些任务啊，比如说它提供的一些这个data啊。这个去做一些这个数据的一个加载啊，数据的预处理啊。当然你如果你用这个去训练啊，你这个不用它的。data你用的也是完全没有问题的啊，这个像这个训练啊，调优啊，还有后面的一个部署环节呢，都可以有它的一个才女。

翠仪

19:26

那你可以在呃你？非常简单的，你用的自己熟悉的一个AI框架的话呢？呃，我们去开发一个主业务，你可以这个开发一个单机版，然后这个等你觉得这个单机版这个呃没有bug了之后啊，就是调通了之后这个模型。我们再稍微的去改一下我们的一个代码就可以啊，去做一个分布式的一个训练啊，就可以去做一个分布式的部署，那这样就可以了啊，更好更坏的就是在开发的时候呢，还是自己熟悉。

翠仪

20:02

在我们训练的时候呢，可以用到这个分布式的一个能力啊，这样就嗯。极大的降低我们的一个呃代码量，也可以提供呢一些比较好的一些这个训练的一个资源。那它能做的主要还是就是刚刚所说的就是一些数据预处理啊。当然这个数据预处理并不是说你一定要用，你可以用之类的啊，它其实都是啊，将它的一个，呃，数据转换成我们最终要用的数据。当然在大语言模型里面可能用的会比较少一些，在这个机器学习里面用的这个呃预处理呢可能会多一些，然后这个缺点的话可以去集成，除了像这个touch啊，txt for等等之类，还有这个呃T boss啊。GBM啊，呃，等等的这些数数据框架都可以，然后还有推的，然后它可以用这个peter去进行这个预测和推理啊，去得到这个最终的一个评分，然后把它部署啊，在线部署成一个。

翠仪

21:12

呃，打呃这个B型的一个模型去帮助我们去呃。应付一些这个高并发的一些需求。那就是它这个底层里面它支持的一些数据样式，比如说在它的一个data里面，它其实也支持一些这个，呃，我们常用的这个P啊，这个LP等等那个数组样式啊，这个在这个train的时候呢，也支持一些非常。常见的一些这个框架，然后我们的这个调优的时候呢，也会有一些这个像呃。比较常见的一些框架，然后它部署的时候呢啊，可以跟一些这个呃，像这个FABI去结合在一起啊。或者像这个？

翠仪

22:05

呃，去GM二的时候它用的是这个呃，去进行一个快速的一个部署，它也支持呃，就是做一些小小的改变就ok了。那它对于，呃有几个核心的一个概念，刚刚所说的一个叫task。Test的话其实是这个呃，我们写了一个函数，那这个函数如果要将它做到这个分布式上面，在不同的一个呃地方去执行这个呃，test的就执行这个函数的话呢，我们呃会对它去做一个这个。一个呃，rain的一个装饰器的一个装饰啊，那这个一个这个remode的一个函数的话就是一个test。那同样道理的话，一个remove的一个。就是一个类的话，它就是一个。那这两个的话就是一个最常见的一个状态，就是，呃，要不就是一个test要不就是一个art，然后的话呢，其实就是它的一个分布式的一个数据，呃，对象，那我们可以把一些需要分发到各个。呃嗯，这个任务上面各个worker上面的一个或者是各各个drop上面的一个呃。数据呢，通过这个的一个方法呢放进去，然后它就会去进行这个分发，然后这些对象的话一般都是只读的啊，是不是呃，不可写的？

翠仪

23:42

然后java的话一般就是我们的一个主程序啊，所在的一些应用。drop的话呢，其实就是这个txt和ar的一个呃集合。那它这个架构大概是这样，我们会有一个had no，就是一个头节点，然后还有一个worker节点。那头节点的话呢，它会负责成一个分布式，它的一些这个呃，命令的一个分法。嗯，比如说我这个。然后创建了有这个多少个啊，然后就是每个worker给我们给它分派去任务去做什么，它有什么schedule啊，有这个，如果有一些需要分发到work的一个对象的话，我们也会去进行这个。呃，通信，那所以这里的话，它这边的一些通信呢，可以有这个ABCDE F之间的一些通信。那它的一个集群的话，通常我们就是有一个或者是多个worker的一个进行程序，呃，只负责这个任务的一个提交和执行的，那它这个worker的话呢，呃，可以对应着一个task，要不就是对应一个arter。就是一个。呃，对应一个worker，一个test对应一个呃worker。就相当于我们要给它分成多少个节点我们就。啊，每一个这个test worker呃，这个ar的话都是对于一个worker的一个节点，当然你一个机子上面你也可以去多做几个节点，比如说呃我电脑上的这个CPU啊，可能是八核的，那我可以创建这个七个worker一个。

翠仪

25:26

嗯，那个节点？然后它里面会存储的一些东西，比如说。好的一个表，然后就是会指引它的一个系统的一个，呃，数据的一个，呃，对象的一个位置，然后还有就是这个呃，进程的一个内存存储，会放一些小的一个对象，然后呃，ren的话就是一些共享的一些资源。然后schedule的话它是负责一些资源管理啊，任务放置。还有就是呃我们这个呃，test完成之后的一个参数呢，呃会放到这个里面去统一去进行这个呃。这个数据的一个传承的就是一个内存的一个储存，它如果溢出的话呢，它就会移动到外面的一个存存数据，那大概是这样的一个架构集群去划分的。

翠仪

26:35

那这个是一些细节部分，比如说像刚刚所讲到的这个其实是一个这个呃，分散控制的一个呃管理，就管理它的一个work的。就确保它呃可以就是它它这个task可以正常执行，可以获取到正常的一个呃value。那实际上的话运行呢都是运行在我们的一个呃，我可能进程包括它的一个demo和put。那就是它的一个内存管理。就大家可能会比较好奇，就是他们那这个谓语怎么去组织他们的一个内部的一个存储。

翠仪

27:16

因为我们这个每个worker它在执行这个task或者是运行这个ar的时候都会使用到堆内存。那一般呢，它都是一个变形啊，变形的一个运行。所以的话呢，每一个task的一个对应内存的话呢，都需要留意一下。那如果内存压力太大的话呢，就会就会自动释放掉一些内存大的一些进程。那当一个worker调用这个或者是从一个txt里面啊，去返回值的时候呢，它就会把它的值呢复制到这个。呃，内存共享对象里面，然后这些对象的话是在这个所有整个集群都可以访问到它，那如果我们有一些故障啊，我们就可以恢复它。啊，如果这个内存这个存储的话，它超过了我们匹配的这个容量的话，它也会转移到其他地方，然后也有一些这个垃圾回收的一个呃，机制。就确保呢，我们这个数据呢，有一个足够的一个缓存区啊，可以给这个不同的一个worker去进行这个数据的一个存储。

翠仪

28:36

那再说一下就是说为为什么现在突然火起来呢？其实为在这个好几年前已经出现了。但是到现在大家可以就是说互联网上搜索一下。就是各种的一个呃资料的话还是比较少。那最全的资料还是它官方的一些文档啊，大家如果想更详细的去学习谓的一个相关的一个资料的话，建议大家去看一下谓的一个官方文档，它里面有非常丰富的一些样例啊，就各种的一个函数说明。啊，都在里面，然后其他内容的话基本上呢是不怎么能找得到的？那就是妥起来就主要是因为这个。

翠仪

29:20

AI它的一个GPT啊，突然也磨起来了，那大家可能就会好奇它这么大的一个模型是怎么部署的，那这才引出了这个味。那味的话呢，跟这个大语言模型呢，它其实是天生就非常的一个兼有。因为呢，我们可以把这个大模型拆分成很多小的模型，然后把小的模型呢放在了不同的一个主机上去运行，然后来完成这个分布式的一个训练或者分布式的一个部署，那这个就是web的一个核心功能之一。啊，你就是根本就不需要考虑它们之间是怎么通信的，你只需要把这些东西啊放到放给不同的一个worker就可以了。那同时的话呢，这个数模型可以分辨，数据集也可以分辨。那这样的话呢，这个呃不同的一个数据集分配的时候呢，可以让它呢呃去在去预测的时候呢，这个效率会更加快，在训练的时候呢，它的一个效率也更加高。而且它可以按需扩容啊，充分利用这个资源，所以总的来说的话呢，这个。

翠仪

30:31

它本质上就是提供了很多大语言模型所需要的一些能力，因为它这个设计的一个理念就是说要求这个高性能，低负荷啊，可以横向扩展，有这个稳定性，那这里的大概就是列了一下方面，这些方面就是说这个。

翠仪

30:52

它跟这个AI GC的一个兼容性。那像它本身的话呢，它是呃，基于这个阿特尔呢，可以去做一个异步并行，然后可以提高我们的一个性能。那这个分布式的一个内存共享的话呢，可以使得它们之间呢，呃，来这个读取相同的一些对象和数据啊，就包括这个。模型的一个权重啊，分布式在上呃跟这个比较方便。然后它呃，这个本身这个rain它的一个稳定性呢也是呃比较。很强的哈，这个兼容性呢，容错性呢都比传统的这种哈，豆腐的一个呃容错性要高。嗯，而且的话它本身就是一个这个python的一个框架，那现在很多AI的应用呢，都是以这个python为主的，所以R会更好去整合这些框架啊，去提供这个，呃，训练的一个能力。那呃，它提供了一些大型车切片的能力啊，比如说像这个啊阿。阿塔的话其实是一个基于这个位啊，去编写的一个包啊，它底层是位，那它可以呢，呃，在此基础上呢，做了一个更好的优化，就是说自动的将这个模型去进行这个切片。啊，这个就是烧和误会收到，因为这个模型切片也是一个学问，怎么去切啊？这个怎么去切才是最高效？那数据集切片的话也是它本身的一些能力。哼哼。

翠仪

32:38

啊等等的这些。那这些内容的话呢，基本上呢，呃，就是我们为什么会突然间火了？因为它本质上跟这个AI GC呢，就是非常的一些兼容。那大概的一个框架呢，就介绍到介绍到这了，大家应该对这个。一个基本的，一个认识。那下面的话呢，我们来从一个非常简单的例子来开始入门去学习一下这个问的一个应用。

翠仪

33:09

呃，我们安装的话呢，呃，其实非常简单，我们是因为我们刚刚所看到的会有很多不同的一个框架。啊，所以说的话呢？它安装可以按照自己所需要的一个组成部分呢，呃，去进行这个安装。那如果你直接安装位的话，它只会还是用它的一个核心部分。那如果你要用其它的部分，比如说像这个。Of data train up to now你就是要额外去安装，所以为了这个呃就如果你不是说这个存储空间特别小的话，那你可以直接用这个呃P P don't read or就可以了。那这个。

翠仪

33:53

无论是linux啊，呃mac windows啊，都可以安装，但是呢，它在拍什么一个版本呢？是最低呢？是要到三减七的一个版本，因为它出来的时间其实是相对来说比较晚的，在再低的一个版本的话可能不支持。那如果有其他的一个需求的话呢，可以去官网呢，去查询一下这个安装的一个细节。哼哼。

翠仪

34:18

那大家的话呢？呃，这个安装呢应该是嗯，没有什么问题的，没有什么坑的啊。好，安装完之后的话呢，我们要启动这个rain的话呢，只要用这个V啊，这个作为一个hload去启动就好了。就比如说你只在一个机子上去安装的话，那我们就这个也只能在单机上面去启动。那我们可以看一下单机启动的一个小。比如说我在我自己电脑上也装了一个。那大家的话也可以去安装试试，然后安装视频的话，因为我现在备课过程中已经全部都安装了，然后这个安装视频的话，我之前录了一个，待会可以给大家去发一下。

翠仪

35:30

那在这里的话呢，呃，我们定义了它的一个端口和8265，那这个要看你们自己的一个服务器，就是如果你在服务器上面去安装的话，也要看一下，呃，你们这个端口的一个开放情况，因为并不是所有的一个云服务器都会把所有的一个端口开放。

翠仪

35:49

啊，就比如说你用一些比较传统的服务，呃，这个云服务器，腾讯云啊，阿里云呢，呃，这个华为云呢，像这种的话，826股不是一个常见的一个端口，所以你应该在一个需要在你的一个安全组下面呢。去申请一下这个端口的一个开放，那才能去访问到相应的链接。那现在我是个单机版，所以我没有指定这个，呃，开放的一个端口。那所以说的话呢，它就是默认在内网地址，就是我的一个localhost上面就可以去。查看到这个呃8265的一个端口的一个情况。

翠仪

36:31

那这个是一个位的一个呃，仪表盘的一个主页，你可以在上面呢去看到所有的这个呃，我们所有的一个啊，的一个情况啊，然后所有的一些应用啊，然后还有就是我们集群类型化，那现在的话拥有只有。一个机子所以的话呢，我的这个呃。就只有我本机的一个机子，然后它状态呢是这个alive的。然后里面的话呢，呃，因为我这是一个八核的一个CPU，所以它默认呢会把每一个核上面呢都去启动了一个拍成的应用。那到时候我们去做的时候就是就呃去呃，用where去运行的时候的话，它其实就会自动的去并行呃并行，呃，就是开了八个进程，然后把这个应用呢，这个我们所需要并行的一个代码呢分到这个八个进程上面。去使用，那这个这是CPU的，然后GPU的话呢这个呃都没有分配到，那这个呃如果后续我们的应用有需要用到GPU的话，我们也可以设置相应的一个GPU参数，那它就会应用到相应的一个GPU。然后就是整个集群的一个情况，因为现在还没有跑任务，所以就是只能看到这些节点的一个运行情况。然后呃，我们也没有任何定义任何的一个arter，所以也没有看到什么，然后它的一个呃运行的一个日志都可以在这里去看。那大家趁现在有空的话，也可以去在本机上面去装一下一个位啊，我去看一下这个，因为这个V的一个代码的话，非常的一个简单，所以我相信在这个自己的电脑上呢，也可以运行起来的。

翠仪

38:32

那我同样的在服务器上面呢，也装了这个呃谓语的一个代码。那呃在服务器上的话呢？呃，待会再演示就是我们需要把它加到集群上面，那我们来看一下这个V的话呢，就是当前我们啊，我先把这个。机子开了呃，这里的话呢是呃，我如果我要外网也能访问到它的话呢，我就需要在这个呃。对呃，这个网页上面呢去设一个这个。臭死了。那官方的这个我还是这个先把我服务器上面的也开了。因为它默认你看到它的一个local IP其实都是内网地址，但是我们这个QQ用的服务器我访问不了它的一个内网地址。啊，所以说的话呢啊，我们可以用外网。那在这里的话呢，它呃，这里也说了，如果你要去把其他的一个机器呢去放到这个集群里面的话，你可以在其他机器上面去呃，运行这个R，然后这个输一下这个address你就会看到其他的节点。那我们先来跑一下单机版的试试。

翠仪

39:59

那这个呃代码的话，作为一个简单的一个样的，那我们在启动了这个rain之后的话，我们在自己的python代码里面呢，我们就可以了，应付一下位，然后用为嗯，这个的一个初始化呢，我们就可以看到。啊，它的现在的一个版本的一个信息。这个版的。然后在这里的话呢，我们的一个python信息位的一个信息，然后还有就是我们刚刚的这个仪表的一个地址。呃，当当然他写的内网地址直接点过去呢，是访问不了的，你必须要用他的一个公网地址。

翠仪

41:02

那我们可以在这里去看到我这个机子上面啊，它是有两个GPU的啊。然后这个GPU每一个GPU显存呢是这个大概七点多G啊7.5G。左右啊，就不到呃不到八。然后这是我们的一个节点的一个情况，那在这里的话，我们呃来介绍一下这个微扣怎么去做一个并行的一个运算。

翠仪

41:32

呃，首先的话呢，对于一个普通任务，比如说像这样子，我们去写了一个函数，这个字会不会小了一点？我们在这里写了一个函数呢，是这个去求这个数据的一个平方。然后的话呢，呃输入呃，比如说我要实行这个呃，函数设置，那按照这个拍损的一个逻辑上来说，它是一个串行的一个操作，那它应该是这个呃，01是呃，这个从零到三的一个。哦，数的一个平方呢，我们可以得到这个数的一个结果，就是0149是没有问题的。那我们如果要把它放到这个ray上面去，并行去计算的话啊，我们需要做的是第一个啊，在这个我们定义的这个函数前面去加一个这个呃，R的一个。

翠仪

42:33

那这个里面呢，还可以去写这个参数？啊，比如说我们要用多少个GPU多少个CPU去运行？啊，这个呃自呃这个任务，那到时候的话，比如说我们有两个GPU啊，我们可以用两个GPU去运行，比如说这个CPU的核心数啊，我们可能只有四个，那我们就用四核去运行。啊，这就就可以自动去调节的，然后的话呢，就是定义好了之后呢，我们需要部署四个任务的话呢，我们把这个。函数啊，调用一个remote的一个方法呢？它不是需要输入参数吗？然后得到了一个任务的一个列表。那在这里的话呢，用户其实并没有执行，那就只是啊。就是部署了是个并行的任务，那到我们用的时候呢，才会得到它里面的一个结果啊，得到这个预算的一个结果。那这就是我们得到的一个结果，那就是0149。就也是刚刚那个数，那如果我们要定义一个arcer的话，它其实也是用为，呃，这个remote这个装饰器去装饰我们的一个内容，只不过这时候装饰的不是一个函数，而是一个类。

翠仪

43:57

那比如说我们在这里呢定义一个这个counter啊，去计算啊，就是每调用它一次的话呢，我们就获取到它的I就是每呃intel一下的话就增加一。那这样的话呢，呃，我们就定义了一个。呃utter然后的话呢，我们去创建它实际化的时候呢，就直接utter点目，然后就可以得到这个呃utter的一个实例。那这时候的话我们去呃调用实际参与的话呢，我们也可以这么写啊，就是呃说这个I那么然后我们就不做，因为反反正我们就只加一啊。然后这十次那在这里的话呢，那它同样的也是异步调用的，所以它还没有得到最终的一个结果。那同样的我跟我们的一个呃test一样，我们需要呢用get才能去得到它任务，那这相当于创建了一个。任务列表，那我们呃，获取到它的一个get一个函数的一个remote的一个结果，我们get一下就可以得到最终输出了。因为我们调用十次，每次加一的话，最终的一个结果呢就是十。哼哼。

翠仪

45:15

那大家可能会好奇，就是说我们这样并行的话，效率上是不是真的会有提升呢？好，那其实我们刚刚啊有用的这个呃，job它其实会有这个面板会在这里去所看到，那我们可以呃。待会去对比一下，因为我example去对比一下，那在这个呃位的一个调用的话呢，呃。我们最终的一个结果，我们还可以把这个数据呢，呃，预先放到一个储存对象里面，比如说我们在算一个矩阵的一个和的时候呢，呃，我们可以这个直接在这里创建，在这里面输入进去，也可以先把这个对象呢。

翠仪

46:04

放到我们的一个谓的一个对象存储里面，用谓的一个put的一个方法，然后就得到这个对象的一个指引。然后我们直接把这个指引放进去的话，我们也可以得到最终的一个对象。那理论上来说的话，我们先把这个数据放到这个。哦，对象里面它速度呢会呃快很多。然后会快一些，就大家可以去看到，就是刚刚这里运行的一个速度。我这里的一个速度基本上是差不多出来的。

翠仪

46:42

那我们V的话呢，一般来说，我们remote的一个结果的话，一般都是返回一个，那我们可以设定它的一个返回数量，比如说要返回三个，那我们这时候的话，我们会这个函数就是返回三个字。那这时候的话呢，就可以返回三个对象，然后每个对象呢就是就分别是012哦，我们可以设置这个remote的一个参数。那当我们的这个呃，任务运行完之后的话呢，你可以设带它就是相当约了我们一开始初始化嘛，然后我现在不用了，我们把这个任务给设带一下。那它就会显示呢，在我们的这个draft里面呢，也会显示它已经运行完了这个状态。那我们，呃，也可以在这个初始化的时候去增加一些环境的一个参数，因为很多时候的话呢，我们在各个不同的一个机子上去运行的时候，呃，可能每一个机子上的一个环境会有些差异。

翠仪

47:45

但是呢，我们这个有一些代码，它运行的时候是需要依赖包的。那这时候的话呢，为了避免它的一个依赖出现问题的话呢，我们可以在初始化的时候呢，去定义它的一个环境，比如说P IP我需要安装这个emoji的一个。哦包。那我们在初始化的时候呢，就把这个需求给。写上去。那这样的话，我们只要初始化的时候它就会。在各个环境上面都先去安装了这个emoji的这个包，那这时候它就不会出错了。就比如说我本来没有装这个emoji的包，它可能就会包括说它找不到这个包在某个这个walker上面，那这时候再去并行。其它的话呢，就会确保大家的环境上面。的依赖呢是一致的，就不需要说我在每一个机子上都需要去P IP仪式这个去安装一下这个包啊，就这个比较麻烦。那我们也可以去对比一下，用一个简单的。这个函数去对比一下，先把这个任务稍带一下。那我们看一下那个？

翠仪

49:01

那在这里的话，同样的，我们还是要初始化一下我们的这个结果。那在这里的话呢，呃，我们定义了一个简单的一个数据。呃，database你模仿一下这个database的一个查询，那它输入到的是这个item的一个序号。然后的话呢，呃，我们就呃返回这个item的序号，还有在这个database里面，这个item具体的一页纸。啊，这里的话呢，我们也写了一个这个具体的一个计算是哦，这个运行时间的函数就是从它开始的时间，然后我们减去最终结束的一个时间返回出来。就比如说我们通常使用就是逐个遍历去跑玩它的话，大概要用2.8秒去完成这个。

翠仪

49:49

啊，八次的一个查询，那我们如果用read去并行去做的话，比如说我们刚刚的这个函数啊，我们现在这个用read remote。去装饰它。呃，得到一个这个远程函数。然后的话呢，我们再去重新去这个创建一下我们的这个远程的一个任务。呃，就目列表就是这个点。remote item item就是我们要输入的一个参数。然后通过get方法获取它它最终的一个结果，然后打一下这个时间。那在这里的话呢，我们可以看到呢，它的一个用时呢会比这里呢略微快了一点，从2.8秒啊提升到这个。

翠仪

50:33

2.28秒。哼哼。那这个提升呢，说可以说有一点，但是不多。啊，但那我们可以再试一下，就比如说我先把这个database，因为它database本身是一个啊python的一个对象，就是一个这个列表嘛。我们先把这个列表呢放到这个对象的一个存储器上面啊，然后。我们把这个对象呢一起输入到我们的模型里面，嗯，这个函数里面。那这时候的话呢，我们还可以去做一个异步的一个操作。啊，因为其实你本身你的这个查询呢，你就不需要去，就说这个一定要等上一个查，我们再查下一个，那这个串行其实是没有什么必要的啊，所以说的话呢，我们去啊，创建这个V的一个操作的话。

翠仪

51:26

啊，我们可不用get我们用wait wait这个函数的话呢，它就是说我不需要等啊，所有的这个都处理完了，我们再返回最终的一个结果啊，我可以只要等其中有一个处理完了，那我们就获取到它的一个结果。然后再等下一个处理完了，只要处理一个，我们就获取一个的结果，处理一个就获取一个结果，那这样去做的一个异步处理，加上我们把这个对象都放在了这个共享的一个对象存储里面，它的这个读取时间和数据传输时间就。读快很多，那我们可以看到的话呢，最终呢，它读完了，建立了整个这个呃列表，只用了0.71秒。那么基本上是这个每一个读取的一个速度呢，是这样子的。那它除了可以做这种判刑的一个就是这种并行的操作，它如果相互任务之间是有依赖的话，其实我们也可以去做并行。

翠仪

52:31

就比如说这样子看的。我们在里面这里有一个新的函数，它输入的是我们查询到的一个结果，然后查询到的一个结果的话呢，呃，我们再去查询它下一个结果，比如说我查询到第零套，那我们再把第一号给拉出来，我们查询到了二号，我们再把三号给拉出来。

翠仪

52:50

那在这样的话定义了这样的一个，比如函数，然后我们可以去创建两个列表，一个是第一个首要去查询我查询这个0246的一个对象的一个结果。然后的话呢，我们把这个结果呢再传到这个for a path里面。去查询它的一个下一个结果，那它这是一个依赖的关系。那从这个并行的一个角度上来说的话，其实它这个01234呃，4567应该是这个，这个是一对一对出来的。那我们可以看到它在ray上面的一个处理呢，也确实如此，而不是说他要这个呃零茶湾呢在茶一啊这个，然后这个202茶呢在茶山啊，山茶呢在茶是这样的一个串行操作，它其实就是做了一个。并行的一个结果。那它得到的一个结果呢，啊，也是这个啊，一对一对出来的，它可以去做这样的一个依赖关系。它可以从从一个数据库或者是不同的一个表啊，都查询其他的一些相关的数据的话。我们可以这样去啊，获取到它的一个结果之后，再基于它结果，再去做别的一个查询或者别的一个操作，那这是也是可行的。这种任务之间存在依赖关系的话是ok的。就好像我们如果要做这个模型推断的时候，我们需要把模型去进行这个切换。

翠仪

54:20

分化然后在这个？

翠仪

54:22

我们这个只有79也有一个任务依赖关系，我们需要先算这个模型的呃，第一部分得到结果，再输入到第二部分，再得到第二部分结果再输入到第三部分啊，要一直得到，直到得到最终的一个结果。那我们这个类的话呢，其实可以用来去记录一些这个信息，因为我们的这个test它是没有办法去记录任何状态的。

翠仪

54:51

呃，它本身是没有任何属性，因为它只是完成了一个呃，函数，那如果我们想记录一下，就是这个我们的一个运行情况，比如说我们运行了多少次啊，跟踪这个运行的一个结果，就比如说我们在去做训练的时候。那我们可以以这个实时的去记录每一次它的一个损失函数的一个值啊，等等的这些啊。然后我所以呢，我们可以定义一个这个utter。呃，去记录一下它的一个，比如说这是一个呃抗去记录一下它运行了多少次。那在这里的话呢，呃，我们这个函数的话呢，也需要把这个呃这里的这个输入进去，然后每运行一次的话呢，它就是增加一次，调用一下这个呃函数。然后最终把它的结果返回，然后在这里的话呢，不好意思，这个没有意见。那在这里的话呢，我们可以看到啊，它的一个结果呢？

翠仪

55:57

运行的一个结果，但是我们可以看到它的一个阿哲呢，它的一个统计呢，呃，其实是有点这个不对的，理论上它应该是返回时，但是我们可以看它每次返回的结果是不一样的，就是因为呢。我们在执行的时候，我们去调用这个的时候，它的一个状态呢，可能还没有执行到那个。呃，八次的一个状态。它官方上的一个教程呢，说它理论上是应该得到八这个数字啊。但是其实我这个测试的时候呢，它运行了好多次，它每次都可能是一个随机的一个，呃，结果。所以我就觉得它这里的一个说法是有点问题。呃，不知道是我代码改的有点问题还是怎么样，那大家可以回去去研究一下这个到底是由什么啊发生的，然后再再讲一个简单的例子，就是基于这个我们简单的这个record。

翠仪

57:08

去做一个分布式训练的话，我们要怎么去修改我们的一个代码？那其实很简单，那我们前面的话呢？先把一些有用的也说淡一下。那在前面这些的话，其实你看上去就是一个part的一个代码。啊，我们先去加载到我们可能会用到的一些包啊，这个pouch的一些相关的一些包啊，还有就是啊，我们首先第一步大家如果熟悉用pouch的话，我们去构建这个模型的话呢？基本上啊，我们首先第一步呢都会去。

翠仪

57:51

呃，定义我们那个data road？那这里的话呢？呃，我们用了一个。呃，手写体识别的一个数据啊，一个简单的例子。那在这个手写体识别里面的话呢，我们这个data loader呢定义过来呢，它呃，首先定义了一个这个数据的一个转换，就是我们需要把它转换成这个cancer。然后去做一个normal啊，去把它就做一个标准化的一些处理。然后的话呢？这个数据的话呢，呃可以直接去下载到，就是我已经下载下来了。啊，在本地上面。呃，如果没有下载的话呢，呃，也可以就这个，它这个代码呢就会自动的去把这个熟悉的数字呢登录到这个data的一个路径下面，那如果你们不想改下载到这个路径的话，你可以改一下其他路径。

翠仪

58:58

就网上的一个下载。啊，就是那个touch loader啊，都是直接用T touch的一个任务的呢，去进行这个封装的。然后我们就可以得到最终的一个结果啊，就是它在下载data loader的时候就实现了这个数据的一个转换。然后第二步的话，我们定义一下这个评估码函数啊，就是我们把这个模型输入进去，然后去算一下这个呃，catload里面的一个数据，它的一个准确率是怎么样的，然后把这个准确率返回过来，然后就是我们的一个神经网络，我们定义了一个比较简单的这个。

翠仪

59:39

卷积神经网络。那到这里为止的话，大家好像看到的一个代码跟ray是没有一毛钱关系的，呃，这其实都是pouch里面的一个代码。啊，我们都没有做任何一个调整。那在这里的话，我们要并行处理的话，大家可以想想这个模型训练，如果它有要并行的话，它是必就是呃需要有有多个服务器，然后每个服务器上面呢，它可能会有呃这个。呃，自己的一个参数，那其实它要做每次训练的时候呢，它每次这个去进行这个参数的更新之前啊，去算这个favor的时候呢，它可能是先要接收到最新的一个参数，然后再基于这个参数呢。把这个数据的应用过去，然后得到这个结果，把这个结果再返回到这个服务器上面，然后的话呢，呃，然后再嗯再去获取到最新的一个参数，因为你把这个数据传过去，然后再传回来的过程中呢，就是有别的一个。

翠仪

01:00:49

夫妻呢？去给他返回，回来就在这里的话呢？我们定义了一个参数服务器，专门去接收这个参数的一个修改。那我们这个参数服务器的的，我们这个模型呢，就是我们刚刚定义的这个卷积神经网络，那在这个优化器就是我们后续的一个优化的一个方面。然后它的这个漆度呢？呃，就是我们将呃获到获取到的一个输入的一个梯度呢，呃，就是综合起来，然后的话呢去做一个这个呃梯度清零，然后去设置新的一个梯度，然后再去把。

翠仪

01:01:33

函数这个呃step一下这样的一个权重更新的一个过程。那在这里的话就是获取到这个模型的一个权重。然后把模型最新的一个权重呢，给它这个返回回去，那下一个模型去训练的时候，它获取到新的元素，然后就获取呃，计算到新的梯度，也可以传到这个参数服务器，那参数服务器在接触到新的一个梯度之后。啊，再去更新模型的参数，再把模型发放给这个啊，我们这个。网络啊，去进行这个梯度的一个更新。那大概是这样的一个过程。

翠仪

01:02:12

那这个data worker的话呢，我们需要也需要去做一个并行。哼哼。那其实就是我们每一个这个呃，相当于整个架构的话，我们有一个参数服务器呢，呃，去负责接收这个呃，我们的一个data worker所计算得到的一个梯度，然后去更新我们的梯度之后呢，把这个新的一个模型参数给它这个。回回去，那data worker的话，每次在去做这个呃训练的时候就会获取每个size的一个数据。去训练的时候呢，他先获取到啊，我们的这个数据。的一个新的一个呃，权重，然后去呃设置一下权重，然后呢再获取到我们的一个数据，呃，去做一个这个计算它的梯度和损失函数，然后我们把梯度呢给返回，呃。返回到我们的一个参数服务器上面，那通过这样的话呢，我们的这个。呃向前计算呃和这个梯度更新，这两步呢就可以分开，那向前计算，因为我们的数据量可能很大，那我们可以用很多个不同的一个向前计算的一个data worker去做，然后参数这个呃这个参数更新的话，只要用一个这个。呃，surfer就可以完成了。那这时候的话呢，我们再去做训练的时候呢，整个效率呢就其实已经是达到一个，并且那原来我们可能就是每次都要自己去算啊，这这个算梯度比较更新梯度。那现在呃算梯度和更新梯度把它分开了，那特别是算梯度这个过程呢，它会比较漫长，那我们可以用这个，而且数据量比较多的时候呢，啊，不同的数据去就可，其实它是可以并行去更新我们的梯度。

翠仪

01:04:11

那就是可以达到这样的一个目的，那所以呢，我们定义了这两个。那在这里去设定一些这个呃，参数，比如说我们要这个算多少次啊，一共有多少个？worker啊，然后的话呢，我们就可以初始化我们的这个内容，那，呃，如果这里设置了一个。Error的预处就是说啊，如果你代码前面不小心已经初始化了，然后你再运行初始化呢，它就会报错啊，然后正理的话就是把这个报错给忽略掉。哼哼。

翠仪

01:04:47

然后的话呢，我们就实力画呃，画我们的这个两个服务器，第一个就是我们的一个参数啊，这个服务器啊，就只要一个就够了。啊，第二个的话就是我们的data worker呢，有两个并行，就两个同时去跟呃计算我们的一个梯度。那在这里的话呢，我们的一个model也可以实际化，我们的data呢，也可以去获取下来。那在这个里的拨号呢，我们就可以正式的去开启我们的这个服务了。那这个整个过程呢，其实就是说啊，我们的这个data work。呃，就是呃会去计算梯度，然后把梯度呢呃，给这个参数服务器参数服务器更新完梯度就可以得到新的这个呃模型权重，然后给它就分发下去这样的一个过程。所以说的话呢，我们先获取到这个是呃目前的一个权重啊，可能是一个随机权重，然后去呃去进行这个训练。那这个梯度的话都是由我们的这个呃data worker呢去计算得到。然后把这个梯度呢传给我们的这个呃。这个参数服务器去进行这个更新。然后我们比如说美食制呢，就可以去获取到一个这个呃，这个呃，打印一下它的目前的一个准确率，那最终呢，啊，打印完的话就会获取到最后的一个结果，那大概是这样子。那在这里的话，我们每次更新这个梯度的话，其实它是一个。

翠仪

01:06:23

同步的一个操作，因为我们会把他两个人算完的一个梯度呢，一起汇总过去，再一起去更新。这是一个同步的一个流程啊！因为我们可以看到我们代码写的就是一个同步的流程，那注意一下，那这个结果的话大概是这样子啊，它这个确实准确率上升的很快啊，那一共只做了200次的话呢，大概是准确率呢，可以到89.6。

翠仪

01:06:59

然后其实我们也可以去做一个异步的一个操作，那异步的话呢，我们重新初始画一下刚刚的这个内容，因为刚刚已经刷淡了。啊，在这里的话呢，呃，我们把这个worker得到的这个值呢，呃，和我们的这个更新的这个过程呢，给它分开，我们用的是rate函数，这里的话就不，没有用get函数。因为的话呢，用get的话呢，我们就需要等它们所有的这个呃都更新了，我们最终才可以得到这个呃结果。那在这里的话，一个word函数去过，每每当有一个这个data worker的这个服务器。算完了它的一个梯度之后，我们就可以获取到这个结果，把这个结果呢传输给这个呃我们的这个T到呃这个参数服务器去更新我们这个梯度。然后的话呢，呃，那这样它就是一个呃异步的一个操作，就这个效率速度上来说呢，应该是快很多，那所以说的话呢，它异步呃，但是它的一个准确率上升呢，并没有那么快。呃，可能就是这个异步所带来的一些这个偏差，大家也可以把它的一个push呢，给它这个设多一点，那在这里的话，我们会看到我们这里，呃，它的一个循环呢，是刚刚的一个两倍，因为相当于没有。每有一个这个每次啊，只要其中一个这个data worker去更新它的一个数据的时候啊，我们的这个参数服务器也会更新一个数据。

翠仪

01:08:40

呃，然后之前的话同步这里的话就是等两个呃这个data worker都算完了之后我们再更新这个呃梯度，那就是一些区别，那效率上来说其实应该是这个呃异步的会快一些。就大家可以也可以去呃自己去算一下呃感受一下它的一个呃准确呃，这个时间上面的一个情况。那其实它可以根据它的这个准确率情况可以看到，其实最后的这个200多次的时候啊，它已经是有一些这个准确率，有一些这个来回回的一个网票了。呃，就说明它其实已经是一个接近收敛的一个状态了。呃，然后上面这里的话呢，它还是。呃啊，还是一个还是有一个略有提升的一个状态，大概还是有一些差别。

翠仪

01:09:40

那大概是这样子，那大家可以感受一下，就是如果我们要用这个recall去做一个分布式训练的话，其实我们呃比较简单的这个呃，方法呢，就其实差改动差异不是很大。特别是你前面的这些这个data load呃，这个屏幕函数的这个模型呢都不需要改，我们只需要改它的一个参数更新的一个过程啊，把它写成一个这个呃ar的一个形式。啊，通过不同的方呃，就是呃，写了两个不同的一个类啊，一个负责去算我们的这个梯度，另一个负责去更新梯度啊，然后我们在。算梯度的时候呢，可以去把它并行，那么我们可以多写几个worker。啊，然后再去更新梯度的时候只要一个就够了，那不就一个，那就可以把原来我们这个串行的。呃，一个流程呃给它改成一个并行，甚至是异步并行的一个状态，去有效的去提高我们的一个训练结果呃效率。

翠仪

01:10:54

那在这里的话，它这个分布式训练其实的。然后要说分布的话，其实也只是分布的这个数据。这个参数去更新过程。呃，并没有把这个模型去分化的，呃，因为本质上这个也不是一个大模型，因为是用不着去做这个分化的一个处理。呃，后续的话我们再看一下这个模型的一个切分。呃，它是怎么去做的？其实你呃到了这里你应该是对整个rain它这个去做一个分布式的一个训练也好。啊，部署也好，应该有了一个大概的了解，那我们其实如果后续要做一个，比如说要把模型给它切分的话呢，那应该是。

翠仪

01:11:40

需要呢呃，把这个先把这个模型呢，我们预先分成了不同的块啊，用不同的一个类呢去读取到这个模型，就比如说一个大模型啊，叫做a，我们把这个模型拆分成很多小块，a一a二a三a四a四a五。然后通过这样的一个方式呢，我们会获取到每一个呃remote的一个结果要传，就是获取到a一的结果传给a二，a二，结果传给a三，那甚至的话这个呃它是不是一定是要按串行去分？啊，也不一定，因为，呃要看这个模型结构，那不同模型的话，它的一个划分的一个方式其实不一样啊，没有一个统一的一个方法的。嗯，所以说的话，如果是比如说有一些块可能是a一和a二，它可能是并行的啊，然后a三a四a五呢可能是串行的啊，那我们可以把a一a二并行去处理，然后再串行传到这个a123a四a五那里面去做。大概是这么流程，那这个中间它们之间是怎么通信的，其实你完全不需要管你只要把这个模型拆分好。然后这个写好我们的这个remote还呃这个呃就可以了，那这个整个流程就可以走通起来了。呃，那这个例子的话，我们可以看到，呃，它其实是可以有效的去提升我们的这个。

翠仪

01:13:11

速度的，但是并并不是说每一个这个呃。每一类函数它都能听到效率，那接下来给你们讲一个就是完全你用了集群之后这个效率。啊，完全没有提升的地址，甚至是这样更低。那这里的话有一个这个呃open CV的一个高，这这个就是我们原始原始的函数吧。啊，在这在这在这普通的一个代码啊，普通代码它运行结果的话大概就是一秒不到，呃，就是直接可以运行的这个1万次。那它在这里的话是做了一些open CV的一个操作，那主要的话呢，就是读了一个这个呃，图片。然后的话呢，就是应用了这个函数，那这个函数就是对图片去做一些这个呃操作。那速度非常快，1万次，那设施上本身的open CV它自己也有自己的一个内部的一个并行的一个规则。那所以它的效率非常快，那当它但是呢，把我当我们把这个代码去转换成一个。Brain的一个代码之后啊，我们在这个函数上面去加一个装饰器啊！然后的话呢，呃，去创建我们的这个呃，remote的一个函数啊。那跟跟刚刚是一样的，然后就是得到了这个future然后get future就可以得运行这1万个这个呃运行这个函数1万次了。

翠仪

01:14:44

然后最终大家可以看看，唉，我这个是单击上面运行数数28秒。啊，这个上面啊，没有做任何的一个的一个封装啊0.04秒。那为什么速度差异这么大呢？主要是因为首先这个呃，这你要考虑的是这个函数本身它运行时间可能就不长。又可能嗯，还是挺快的。第二个就是说我们传输的这个对象，这个呃图片它这个本质上。它是一个大的一个数组，那它需要分发到这个。呃，服务器这个呃，我们的程序上1万次，那这个分发之间的一个效率呢就低很多，就是它通信1万次，所以我们要考虑我们的代码用位，能不能提升我们的一个效率的时候要考虑它。原本的这个呃，函数或者是呃，原本这个函数运行的时间，它是不是呃，很久，那如果它九道是比这个通信时间。久的多的话，那这时候去做并行才是有意义的啊，不然你本身函数运行的非常快，你在单机上运行就很快，那你根本不需要考虑并行，你这个串行效率已经很高了啊。

翠仪

01:16:07

哼哼，那继续刚刚的那个集群，因为现在我们的这个集群啊，我们啊就只启动了一个服务器，那比如说我还有别的服务器啊，就这呃这个它上面也中了不是。啊，然后的话呢，版本应该是也是一致的，因为它们两个呢，呃，我用的是同一套的一个镜像。

翠仪

01:16:31

然后呢，啊，刚刚我们在启动的时候呢，它这里呢就有提示了，就是如果我们要把其他的一个机器呢，啊，加入到这个集集群里面的话呢，我们只需要运行这一个就可以了。那在这里的话呢，我们在运行，那注意一下，这里是它们两个机子之间的一个。

翠仪

01:16:50

呃，内网的一个地址，那如果你们两个机子呢，是不在同一个内网里面的话，那你这里需要把它改成一个公网地址。啊，那肯定，呃，它的效率上来说的话，肯定是在局域网内，它的一个速度呢，会比局域网外呢快很多。那在这里的话呢，我们运行的这个我们就可以看到了，它已经是成功加入到这个集群里面了。那我们可以看看刚。方的这个。啊，这个页面哈，我们可以看到已经多了一台机子在里面了啊，大家的一个CPU啊，这个GPU的一个情况。啊，都是一样的，就是这个硬盘存储可能会有点差异。你要这个使用的一个内存啊。CPU使用的一个情况。那呃，那这时候我们有两个集群呢，有两个集群的话呢，我们呃要运行代码的话也还是会在这个。就是had的一个load节点上面去运行。就刚刚的这个？代码我如果在这重新运行的话，就是test。MPY. 他应该呢。啊，它前面这个启动的其余时间呢还是挺多的。那我们可以看到啊，两个这个呃，机子它的CPU都开始这个增加了啊，就是刚刚运算的时候，那当用了两个机子呢，比我这个单机上面运行的速度呢。啊，就是要快的一些。那我们呃，可以再做一个实验，进一步把它的一个速度给它提升上去呢？

翠仪

01:18:54

就是我写了一个T二，它是二，跟它的一个区别在于哪呢？在于我事先把这个图呢放到这个read的一个对象存储里面啊，就和他们在后期去读取这个数据的时候呢。这个可以减少他们的一个通信时间，那这时候的话我们可以再来对比一下这个。还首尔。那他们都是有在跑的啊？两个机子都会有在动。那这时候的话，我们可以看到这个时间，大概呢，就是这个减少了一半，从11.6秒变成了5.265秒。所以就是说呃，对于一些这个。它本身体积比较大的一些数据的话呢，我们可以事先用呃，把它放到这个R的一个对象存储器上面，那可以进一步的去缩减它们之间的一个通信时间。就这个，因为它需要不断从这个。啊，要不就是从我们本机传过去，要不就是从这个对象存储器上面去读取到这个对象啊，去传过去，那这些数都其实都比不上，我直接这个都存在自己本机内存里面的一个单机版本的时间。但是呃这个通过这个实验确实能对比出来，就是呃，如果你决定了要去做这个并行的话，你这数据呢事先把它放到这个对象存储器上面呢是可以呃比较显著的去看到就。

翠仪

01:20:39

它的一个，呃，并行运行的一个时间，这其实它这个预习时间不在于这个，呃，程序的这个函数的运行时间啊，主要在于它们之间这个函数跟这个节点之间的一个通信时间，那我们是先把我们要用的数据这个。呃，用put的一个方法放到这个位的一个对象存储里面的话，呃，它的一个。呃，时间的一个，它的一个呃，通信时间呢，可以进一步的一个降低，得到一个比较好的一个提升啊，当然这个数据量可能比较少，你可能可能这个呃，只提升了一倍，当你的约数据量大了之后，这种对比呢，其实还是很明显的。那这就是这个recall它的一个简单的用用法，就是刚刚的这个例子。啊，他去做这个呃数据和这个参数服务器。嗯，这个worker data worker之间的一个不断的一个通行的一个结果。那大概。就是这是这个rain的一个入门级的一个教程，那大家呃学了这个可以尝试的把自己的一些这个。那自己的呃，这个代码改成这种分布式的一个代码去测试一下这个效果怎么样？啊，当然这个呃，如果涉及到一些训练呐，或者是这个数据的一些预处理块，也可以参考一下后面我们这个教程啊，就是这个为了一个AI R的一个教程。

翠仪

01:22:20

那首先的话呢，呃，我们可以看一下，就是它会封装的更多更好用更方便的一个算法，就不需要我们啊，对自己的一个算法去。进行更多的一个改造。那首先的话呢，就是这个red data red data的话，它其实是一个这个适用于这个机器机器学习。工作负载的一个可以扩展的一个数据处理。然后的话它比较适合去做的是一些一项的批量的一个推理，前面的一些呃数据梳理部分。然后训练的时候他的一些呃这个数据的一些获取。那它其实也提供了一些比较简单灵活的API，比如说像这个呃。P里面的一个map的一个操作啊，P里面的一个goodby的一个操作，还有这个呃，这个随机乱序的一个叫做排序啊，这个抽样啊等等的这些操作。哼哼。那它这个数据的话呢，其实来源可以很多，像这个。啊，这个你这个所有数据经过18个去处理之后得到结果，也可以直接输入到这个R里面，像我们本地的一些数据，像这个J CSV啊，啊这个excel啊。呃这些数据TST啊都可以存储，然后还有就是拍的一些对象相当啊啊啊啊，还有这个呃pa error啊等等这些都可以去处理，然后在这个。Rain的一个dataset里面的话呢？呃处理完就可以传输到。后续的一个过程就是这个P和这个呃tensorba或者是这个T等等的这些框架里面。它的这些函数的话，其实通通都可以去做这个B型的一些处理。

翠仪

01:24:29

呃，然后它的一个代码呢，也相对的来说呢，会比较简单，就跟刚刚的是差不多的。那针对不同的一个数据的话呢，啊，比如说像这个我们要去做离线离线的一个批量推理的话呢，它会对一组固定的一个输入呢，去生成这个模型预测的一个过程。然后它会获取到它这个。高效的一个解决方案。就前面的数据的话，它可以通过多个a呢去进行呃，多个CPU的一个test去进行这个并行处理，然后是输入到这个ar的一个GPU上面的一个模型，然后呃可可能是一个或者是多个模型。然后模型出来的结果呢，我们也可以继续去做这个批量的一个呃，模型的数据的一个写出。等等这些过程，那预售里的话其实也是跟这个你用data啊，呃，这个ETL啊等等的一些等等的一些工具，其实是一样的啊，我们就可以把它前面的一个结果也可以直接输入到我们的这个。呃，为里面去做一些这个呃，其他的一些批量的操作，然后再批量的输入到这个模型的一个训练里面。因为你预先把这个数据呢放到这个位上面的话，后续他们去做这个呃处理的时候呢，他们通信时间呢会更加的一个短啊，更加的一个节省。

翠仪

01:26:14

那这里呢，给出了一些这个例子。呃，比如说像这个呃本地数据，我们可以读取的是什么？这个呃program image test GSV啊，呃这个呃它二进制的数据就主要是这个excel的数据。然后还有就是TF的一个V的一个数据，都可有相应的一个函数。大家也可以去参考一下官方的这个文档的一个说明。那这些都是一些本地的文件。就是不同的一个接口。然后云服务器的话，它目前呢已支持这个，主要是这个亚马逊云呢。呃，这个GCSL还有ABL啊，那你本地的这个像我们国内的一些数据库的，只要你有这个地址的话，它一般也支持的。然后像这个NFS的一个服务器。啊，还能呢，去这个读取这个压缩文件，就比如说你本啊，唉，很多数据集它可能是一个压缩包的形式啊，我们之前做的时候，我们读的时候可能就是要先这个把它解压出来。啊，然后在这个逐个读取，那这个R的话，你也支持这个压缩形式，也可以增加这个这个解压的一个处理方法，比如说用这这把这个啊压缩包给解压出来。然后还有本身开始的一些对象，比如说像这个字典，还有这个列表都可以直接转换成V的一个data数据啊。还有这个PPP P的话，这个呃可能。也是一个比较新的一个数据形式啊，这个大家如果没有了解的话，也可以去了解一下，它其实是一个table的形式。

翠仪

01:28:24

然后像这个分布式系统的话呢，像这个dust spark等等的这些，它都可以去获取到啊，这个最终的一个结果就是你用where的话，你完全不需要抛弃啊，传统。嗯，你们现在目前在用的一些分布式系统。它完全是可以这个两个系统一起上的，因为每个系统这个像这种传统的系统，你们可能已经用习惯了。就如果这个要改的话，可能也是需要费一定的一个时间和精力啊，就是我们完全可以在后续的一个处理过程呢，接上这些技术的所得到的一个结果啊，继续去做。然后还有就是它非常灵活的是支持这个H。横上面，因为我们知道横上面它有很多的一个呃，大家上传的一些公开的一些数据集。那。呃，它就可以去直接读取下来，而不需要我们直接去登录下来。我想看。但一般呢，这个其实啊，不是太推荐的一个算法啊，做法，因为它本质上它不支持这个并行读取，就相当于你省了这个下载这一步啊，它就是帮你下载了啊，有好有不好。然后这个tensorf官方的一些数据的话，同样它也。这个不太支持这个B型的一个。啊，然后还有也可以直接读取数据库里面的数据啊，常用的一些mac口啊PGC啊，它都是支持的。

翠仪

01:30:04

啊，这个时间有点久了，不如我们这个看一下，就是刚刚所讲的部分，大家有没有什么问题？啊，如果没什么问题的话，我们先休息个十分钟，待会再继续。呃，背背看一下。

王蓓

01:30:17

好的，同学们可以踊跃提问了。目前暂时还没有同学举手提问噢，有王波同学提问了。好了，您可以发言了。

翠仪

01:30:31

哦，你好。

王波

01:30:32

嗯，黄老师那个？我就想这个多个节点，它必须要部署在linux系统下嘛，如果是windows系统可以部署。

翠仪

01:30:41

可以啊，可以啊，可以，你在那个上面装了这个rain的话我也可以不？

王波

01:30:47

哦。啊，谢谢嗯。还有一个问题加我，就是如果没有多个系统的话，用用docker模拟多个节点可以吗？

翠仪

01:30:57

啊，可以可以可以。

王波

01:30:58

也可以哈。

翠仪

01:30:59

可以对你开了多个do，可能只要就是你开了你的一个head low的话，其他的就是按它上面的一个地址去，这个开启它都可以加入到集群，就，呃，它唯一的要求就是，呃，你们的一个位的一个版本是要一样的。然后拍摄版本是要一样的啊，不然的话后面跑起来可能会报错。

王波

01:31:23

嗯嗯，好，谢谢！

许涛

01:31:28

何老师那个？

翠仪

01:31:31

啊，比如说。

许涛

01:31:32

之前说的那个能跑在这个上头吗？

翠仪

01:31:36

哦，可以呀。

许涛

01:31:40

应该也不会有什么问题是吧，也是。

翠仪

01:31:43

呃，完全是可以的，就是嗯，就是看你的这个并行的一个需求，比如说它前期它不是要把数据呃导入到这个向量数据库吗？这个过程可以用rain去做一个分布式的一个导入。嗯，然后后面这个提问的话，你也可以把那个模型呢做一个分布式的一个部分。呃，就比如说你可能就是单11个模型并发量。啊，可能撑不住，你需要不多个的话，你可以用位把这个一个模型推到这个各个集群的一个节点上面啊，去并行去跑都是可以的。

许涛

01:32:24

然后它这个ray的话，它能支持到。他有没有官方或者是什么资料里边儿，有没有说他最多能支持到多少个屋里的屋里的设备？屋里的机器啊？

翠仪

01:32:36

啊没呃，它没有这个节点数的上限，就是目前我看到的资料。

许涛

01:32:43

就随意的往上怼都可以，它都可以扩展是吧？

翠仪

01:32:46

哦，对。

许涛

01:32:50

好。

george

01:32:55

哦，我问一下就是说什么这个瑞和人之间是什么关系，因为我看国内很多人也用那个deep speed。因为很多像那个多机多卡的我嗯不是后面会演示这个嘛，因为在工业界用的话就公司里边儿。跑一个稍微大的模型肯定要多机多卡的一般是八卡三G，一般都这样。后面会说一下这个演示嘛，类似的。

翠仪

01:33:22

其实呃，其实这个db它也是一种并行的一个操作，但是它可能会更重点是在于这个模型的一个并行上面。然后这个ray的话，它其实是会是一个更底层的一个架构，它的重点呢，是在于这个我怎么去把这些呃，worker之间的一个通信呢？这个服务给它做好。所以它们两者的话其实是可以一起去使用的，就是不冲突的。

george

01:33:55

哦呃，就是说那个。呃，像那个就那个呃那个底层的。类似于那个虾之间的那些是吧？

翠仪

01:34:08

哦，对。

george

01:34:08

这个跟罗斯计算。我因为我因为我看那个。我也在那个瑞那个比较早期的群里边。那那个好像那个你点开那个openai的官网？比如说你付款的时候，在你买gpd四的时候，它会那个显示一个R。那个我我觉得应该是应该用瑞训练的吧？

翠仪

01:34:29

嗯。

george

01:34:33

啊行啊，谢谢谢！

翠仪

01:34:35

嗯嗯，好。哦，好的行，那我们先休息呃，十分钟吧！

王蓓

01:44:55

何老师可以开始上课啦！

翠仪

01:44:57

啊，好的，那我们继续，那刚刚说完这个R的一个读取，那我们啊，其实它读取的话呢，它其实是啊一个并行读取，那它分坏的话呢，就是呃要看它的一个。哦，并发量啊，那就是我们要分了多少个这个。多少个这个task？当你分的一个代，它越高的话，它的这个数据级就会分的越小，所以它就是，呃，变形的一个效率会高一些。那有了这个V data的话，我们还可以去做一些这个数据的一个预处理部分。那呃，比如说我们可以自己定义一个函数来进行这个数据的一个转换，那这个只要在这个函数上面去封装一下这个我们的这个数据就可以了。那比如说在这里定义了一个。嗯，这个那个那个那个鸢尾花数据集的一个转换，它就是输入的是一个被的一个数据，然后它会获取到的它的这个呃啊。AB嗯，就是它的什么那个长啊，宽呐。还有它的一个面积的一个数据，然后它会获取啊，把它的一个呃的计算得到，就是这这两个相乘的一个面积了，也写到这个被里面，然后去返回，然后我们再去做的时候呢，可以调这个。

翠仪

01:46:36

Map batch的一个做法呢，去进行一个变形的一个计算。就我们如果普通的一个计算的话，它可能就是一个串形的，因为python里面的话呢，其实panel的那个map函数的话，它是一个伪变形的函数。呃，它其实本质上还是做一个串串行的一个操作，那在这个。我们的UV data里面的一个函数的话，它其实就是做的一个用用到的这个mac flash里面呢，就是用的啊，真的是一个并行的一个函数。啊，我们来看一个呃，看的代码来讲解一下这部分。就比如说，我们可以先从这个数据的一个读取部分，那在这里的话就是。不嗯。

翠仪

01:47:35

哦，这个环境。环境更新一下。啊，这环境可能是我之前装过那个别的包有点乱啊！

翠仪

01:48:35

装吗？那再看那它这个路径的话是这个亚马逊云上面的一个路径。我读完之后的话呢，我们可以看到啊，用so的一个方法的话，可以看里面的一部分数据。呃，有点像这个pad里面的一个head的一个函数。

翠仪

01:49:11

那在这里的话，我们直接调这个ra的话，其实是不需要去对ray去做初始化的，因为它其实在里面已经包含了这个呃内容了。那就是它的一行的数据，这个鸢尾花的一个数据集。然后我们要去做这个变换的话呢？呃，这是我们读取之后的一个。DS就是我们的函数。所以呢，我们DS map。这个的话它就会直接去进行一个并行的一个操作，然后我们可以。

翠仪

01:49:52

稍等一下，我把这个环境给他。重新装一下。如果你要上我中文应该是最新版的，但是。刚刚初始化的时候还是2.1？这一个版本就有点怪。应该用的是二点六点一的一个版本，那在这里的话我们先停了它。嗯的，vc的一些服务器呢，也需要更新版本，就两者之间一个版本不一致的话，它一运行的话它就会报错。

翠仪

01:50:46

Ok然后刚刚的？给重新。

翠仪

01:50:57

问一下。重重启一下我们的这个服务。

翠仪

01:51:43

我才看到它刚刚显示出来的任务，其实在一个呃这个。CPU上面去做？那就是它的一个并行任务用的是多个CPU去做，因为我们并没有要求它用GPU，说它默认呢都是没有用GPU的。那在这里去算完之后呢，这些数据呢啊，就可以得到多了一个这个啊，我们的这个变量。那如果我们需要在本地去呃，使用这个数据的话，我们可以直接按照我们的一个呢，我们就可以得到这个数据，离这个数据可以传到别的地方去使用，也可以传直接传给这个task去使用。哦，这里输入的是我们的这个呃，DS就是我们的这个读取到的一个data的一个内呃，这个数据集，然后的话我们可以的遍历它整个这个。呃interplation。啊，就是遍历它整个这个数据集。然后这里呢会返回这个数据集的一个大小。你可以看一下它整个结果。那在这里的话，32的话其实是因为我这个服务器上面。这里有一个呃，其中有一个可能会有报。就是他读取这里的时候呢？

翠仪

01:53:32

呃，有点问题。Time out所以它呃，虽然它报错了，就是这个节点可能不可用了，那我们呃有其他节点跑出了最终的一个结果，那这个我们的一个data也可以传给我们的一个arter去使用，然后跟刚刚不一样的一个做法，那如果我们这里有。这个结果的话我们可以传过来。那我们worker的话呢，我们呃用了四个worker呃去做。然后呢把它等分成四个，然后把这四个呢再等分进去的话，最终啊可以看一下它的这个使用的一个效果，就相当于我们现在先把数据啊这个分成了四个。然后再把每四个传到这个work里面，再去做每个bash的一个并行的一个操作啊，返回的是个空的一个数据集。

翠仪

01:54:31

那如果我们要保存的话呢，可以直接就可以这个保存下来了，那它是一个缓存数据的一些形式。

翠仪

01:54:43

那可以看一下，现在的话应该是。还没有这个数据的我们。保存的话呢，它就可以自动的去保存下来。在data里面啊，我们保存的这个是一些这个它的一个缓存数据的一些形式保存下来的。那事实上的话呢，我们可以拿这个去做一些我们以前去做机器学习啊，去做的一些数据探索的一些处理。呃，像这个嗯，这个这这是用到了呃之前一个开口上的一个比赛，有一个信用卡违约率的一个预测的一个数据集。哦，这这这这这时候才对我们的一个rain应该用的是二点六点一的一个版本，应该用最新版本的才能跑。这些代码因为这些代码这个有些人模块的话，它是后面才更新出来的。

翠仪

01:56:04

然后我们读取这个本地的一个数据，那这个数据的话呢，呃，我们可以查看一下它里面的数据结构，它里面呢是有这些列的。就是呃，这是它的一个Y，就是它在两年内有没有违约？那就是它其他的一些变量啊，大概就是它的年龄呢。不在抵押这个月收入呀，然后这个呃，其他的一个违约情况，比如说90天内啊，60到89啊等等的这些啊，不同时期的一个违约率的一些情况，还有他家庭成员数的一些情况。按那它里面的一共有？呃，15万条的一个数据。如果直接输入这个DS的话，我们就可以看到这个DS的一个详情。它里面会有一个呃元数据。就是只分了一个block，就没有指定它分很多，然后它每一个数据的一个具体的一个类型。然后如果我们要看这个一行的数据的话，我们可以用。啊，跟那个had是一样的啊，这个是？呃，就是他写数据的时候这个呃他所以他没有列名，然后这个是一个违约的一个情况。嗯，年龄45岁的一个呃情况。那如果没有查询啊，最大值的话呢，我们也可以去直接去查询到，比如说年龄的最大值啊，负债比的最最大值，月收入的一个最大值啊，这个啊，在这个数据里面。

翠仪

01:57:55

然后还有就是。我们可以。看一下每一个这个分组的一个情况，就因为它是一个是否违约，那我们可以看到正负样本之间的一个分布呢，还是有点差异的。那呃，这个数据里面呢，它原本呢是有一些这个数据错误的。理论上呢，国家规定呢是大于等于18岁以上，就成年的用户呢，他才可以是申请信用卡。因为未成年用户是没有任何的一个呃还款的一个能力的。啊，所以就是我们可以把一些这个不合理的数据给它筛选掉，然后也可以直接在这里就去做乱序，那后续的话就会传输到这个啊，我们的模型里面去进行这个训练了。

翠仪

01:58:49

就大概是这么的一个用法，那其实关于这个data这一部分的话呢，它其实还有很多的一个用法，但是因为我们时间确实呢是比较有限啊，所以就是更多的用法呢，大家可以参考一下官方文档，再去探索。呃，而且也考虑到这个大语言模型的时候，可能前面的这些数据预处理部分会比较少一些，在训练模型的时候。如果有需要的话，呃，大家可以去看一下它的一个呃。它的一个用户手册，包括这个各个部分，包括这个我们模型的11个呃加载，呃，这个模型的一些呃数据的一个转换，数据的加载，然后就是它的一个抽取，还有就是便利整个呃这个。一个遍历保存，然后就是呃，也可以去处理一些这个图片啊，文本这个T，然后也可以去处理一些这个pouch相关的一些预处理文件啊，还有就是一些呃，端到端的一个样例。就比如说批量的一个呃。他的一个推理。等等这些。

翠仪

02:00:06

再看一下，就这个，因为时间有限，我就不讲那么多了。那就是这个data那啊啊有了data之后，接下来的话呢，我们其实就是要做这个呃训练了。那这个的话，它其实是拓展了一些这个常用的一个框架，比如说像对啊等等的这些框架，然后它封装了很多个圈，其实就是刚刚这里所看到的这些它都封装了。啊，包括这包括这个her face的一个trainer her face就是那个呃，刚刚有同学提问的那个D其实就是hogan的一个出品，那其实它也封装了那个D的一个trainer。

翠仪

02:00:57

那所以说的话，你要去做这个呃，多机多卡的一个训练的话，你直接在ray上面去调用的一个train呢，你就可以完成了啊，你就不需要考虑说我到底用ray比较好，还是用这个。

翠仪

02:01:14

呃deep speed比较好，其实它本质上呢是可以综合一起来使用的。那这个trainer呢，它主要就是，呃，去实现各个不同的一个封装的不同的一个框架，它的一个，呃，训练器，然后我们只需要在我们原来的一个代码上面去稍微调整呢，就可以，呃，输入到这个trainer里面去得到一个训练器。那这个训练器里面它就会自动根据我们呃，时间定义好的，因为每次训练的话可能有不同的一个配置参数啊，它有一些标准的配置参数啊，运行的配置参数，比如说你的学习率啊。这个呃，训练的一个迭代次数呀，然后batch size啊等等这些会有一些参数输入到这个trainer里面，然后这个运行的一个结果或者是我们模型保存的一个checkpoint的话都可以保存下来，最终的话可以输入到我们的一个picture上面呢，去做这个。哦嗯。

翠仪

02:02:18

那深度学习最快的话，它其实支持的啊有这个touch china thanks for china呃呃差距不是NGBMS ken还有这个transformers的一个china啊latin china china是这个。呃，PY touch更上层的一个呃训练包，它这个训练起来的话比touch本身的话更加简洁，就是更高层级吧。然后还有就是IL就专门用于这个。强化学习的。啊，然后这个就是它们保存的一些车啊，不同类型有对应的具有不同类型的一个车啊，还有不同类型就是对应的不同类型的一个P。那就这三大模块就是深度学习的，基于数模型的，像这个差距，还有这个light GBM，然后其他框架呢就是这个SK和NF。那我们可以来看一下它的这个呃，简单的一个例子，是官网上的一个快速例子。

翠仪

02:03:22

啊，比如说我们要做一个差距boost的一个训练啊，那首先的话，其实整个步骤还是呃，一样，就是我们先要读取让我们这这个数据。呃，然后的话呢划分这个训练级和测试级啊，这个呃你无论用做也好，还是用呃做的是一样的，前面这些，然后的话这里呢它就是去实呃实例化我们的这个。呃，的一个圈，那它这里输入的一些参数呢，首先是它的一个，呃。我们分布式训练的一些参数，比如说我们要用多少个worker去训练，是否用这个GPU啊，因为这个它就不是的话，它可以用GPU去加速。然后的话呢啊，我们的一个目标链是哪一个链，就就就就它列名叫什么？然后我们这个？它的一个boost的一个呃，多少次，多少次的去做一个boost，然后还有就是呃模型的一些参数就是差距boost里面的一个参数，就比如说它这里做的是一个呃，二类的一个分类。啊，它就有一个目标函数，还有一个这个评价指标啊，这些都是差距boss里面的一些参数。如果你们熟悉差距就换掉的话，这个应该是很眼熟的。然后还有就是我们的一个dataset输入进去啊，就就前面的这个dataset啊，这个验证集。然后。这个封装好了之后其实跟我们用是一样的。

翠仪

02:05:02

大家看看这个代码。呃，如果你之前有用过SK的话，其实它们整个步骤是一样的，就无非就是读取数据啊，有了数据这个呃有了这个dataset之后呢，我们就是呃定啊，就这个实际化我们的这个呃训练器，然后训练器。那他就开始去念了，然后最终就可以得到相应的一个结果。嗯，就是完全是一样的，就这些代码的话，不会给你去用这个，它就会带来什么样的一个不一样的地方。

翠仪

02:05:37

然后类似的还有这个？大家看一下，它其实跟刚刚的差距不是基本上是一模一样的，因为它本身两个都是呃数结构，只不过是两个不同的包，而且LG VM的话，呃实测用起来的速度呢会比这个差距不是快很多。那整个过程流程没有任何的差别。呃，那这两个机器呃，就是这两个LG BM和这个差距的话，主要是用于这个传统的一些机器学习，你可以去做一些结构化数据的一些呃，分类。呃，回归就比较少了，是分类。呃，如果有兴趣的话，大家讲这个数据的这个数据量特别大，就是特别是你用差距不少，就是有当你的数据量一上去啊，你的这个效率可能就是极其的低下。

翠仪

02:06:36

嗯，你想要分布式去加送一下它们的话可以参考一下这两个代码，就是整个这个流程，你可能需要修改的是前面的这个，呃，数据的一个读取的一个方式，因为你可能是有自己的一些数据集啊，可能不一样，可能会加一些预处理的过程。那其他的就没什么区别了。那下面的话呢，我们来重点讲一下这个呃。

翠仪

02:07:03

KY touch. 呃PY touch的话呢，我们先从一个这个。

翠仪

02:07:34

我看看。

翠仪

02:07:51

Ok我们先从一个这个。最简单的例子。呃，先先讲一下这个touch它原生就是没有用任何的一个分布式啊，去做的话，那怎么去写这个呃模型，那这里用的是这个呃，这里做的是一个。

翠仪

02:08:14

情感分析啊，然后这个数据集的话呢？这是一个这个微博上的一个数据，就是大概有10万条的一个微博数据，然后它的上面呢呃会有一个分类一，呃，就是01的分类，就表示了它的一个是正面负面的一些情感。然后呃，我们如果要选一个PY touch呃去训练这样的一个burst的一个模型的话呢，那非常的一个简单呐，首先是加载了相应的这个。呃，把然后的话呢，我们需要把这个呃这个呢？给它这个初始化。

翠仪

02:09:00

那这里的话呢，我们直接用这个but最基础的一个版本啊，注意作为一个演示。就如果呃没有下载的话，一般可以，它会直接去进行这个下载的。然后的话呢，我们会呃，用这个P touch里面的一个dataset去定义一个呃，dataset的一个类去读取我们自己的函数。那因为我这个数据的话呢？哦，我已经预先分好了，两个数据大概是。这两个一个这个训练级，一个测试级。大概是75%比上25%。这一个数据。所以说的话，我们这个数据的话，只要给到一个路径，我就可以直接用pandas呃里面读取过来。然后的话呢，land的话就返回到这个数据的一个程度。然后get item的话呢，我们就是呃，它里面有两列，一列叫做review，一列叫label review的话就是我们的一个文本。label就是我们的一个标签，然后我们就返回这个touch和label。找回进去，然后我们就可以。获取到了。我们这个。啊，数据就有时候像这个，它是一个这个负面的一个数据。太好。

翠仪

02:10:32

然后呢再定义一个collection collection就是啊，去是啊，我们获取输入的data之后啊，我们需要去对这个数据去做的一个这个操作去去处理。就是主要其实就是把它啊放到我们的一个，通过医院呢啊去进行这个编码啊，补齐补齐的一个操作。

翠仪

02:11:00

这里match the life嗯嗯，这个长度的话呢，是500个字。呃，可能微博没有那么长，可以把它设小一点，就比如说避免后面这个向量呢会过于大。然后呢把它的这个之后呢，我们返回了几个东西，一个是idea，然后一个是它的一个attention mask。然后呃token的一个type ID还有label都给它返回回来，然后由一个data loader呢去加载我们刚刚的这些东西啊。然后我们的这个呢，我这里设的一。

翠仪

02:11:35

然后的话我们可以啊，读到其中一个呃，第一个的话它大概是这样子，全，这是零呢是补全的一些位置。然后就是它的一个label呃，这就是它的一个must就表示了它哪些呢是呃有的哪些是没的。然后还有就是它的这个呃open type还有最后就是我们的label。那我们一共是有8万9000条的一个数据。然后呢呃这个input ID的话呢是呃呃因为我补全到256嗯最长的长度。然后它的，所以它们的维度呢，都是一乘256。然后因为呢？它是一个这个去做分类的一个下游任务。所以的话，我们直接从这个transformers里面去呃，加载这个分类用的一个函数，然后还是用这个呃，这里设置一下我们最终的一个分类，因为只有零和一，所以就是两类，并且把它放到这个显存上面。

翠仪

02:13:03

我们可以尝试的把这个数据呢呃输入进去可以得到一个这个最终的一个log的一个预测结果，那一开始呢呃，这个都是不对的，而且它也没有去经过相当呃，就是这个。Short mess所以这个值的话，我们只最终呢，因为只需要去看它哪个类别，没有需要输入它的一个呃概率的话，我们都没有做。short mess。所以呢，我们的一个训练的话，其实整个流程是这样的，这里定义了一个训练函数。

翠仪

02:13:40

那在这里的话呢，我们从这个data loader里面去获取到我们呃的一个呃，数据。然后在这个数据里面的话呢，我们输入到这个模型里面，然后去获取到我们的这一个最终的一个输出。然后呢去计算我们的损失函数，然后就是这个损失函数。然后去啊，更新我们的一个参数啊，就算一下这个当前的一个呃，数据集，然后之前的话用的是可能是一个比较差一点的一个。啊，服务器这个一开始报显存的就是这个模型，加载到显存里面的话呢？

翠仪

02:14:27

Thank you. 下下载到这个显存里面的话大概是。我不是这个模型五G多，然后之前用的是一个卡的模型，那就这个一放就爆了。然后现在还好。大概是这样的一个效果。那他训练的话已经是这个呃准确率。这个其实没有什么参考意义，因为我是每一次都算了。

翠仪

02:15:17

那我们整个这个训练的话，其实它要改造这个代码的话呢，也是非常简单的，首先是这个呃，数据的input里面的话呢？呃，我们加了位相关的一些这个，然后的话呢，我们其实是呃不是都写了这个图不打或者是图CPU等等的，这些其实都可以不用写，有了这个位之后，它可以自动的帮我们去放到合适的一个设备上面。然后这个R touch它本身也是支持这个DDP。啊，就是这个并行训练的，但这个这一步走着背之后呢，也不需要去写这个并行训练的。

翠仪

02:16:01

只是了，我们在模型呢，呃加了一步，就是我们原来的模型，我们需要把它放到这个ray的一个呃，用prepare model里面去再去封装一下，去得到新的一个模型。然后data loader的话，可以将这个data loader呢转成一个分布式的一个sample。啊，可以自动将数据呢分发到合适的一个设备上面，那如果这个数据它本身就是用red data去读取的话呢，不是用这个touch的一个data roader去写的话呢，这个这个步骤呢，甚至是呃可以忽略的。

翠仪

02:16:40

呃，这里注意一下，就是data load里面接受的B赛呢是每一个work的这个size，所以说的话如果你就一，然后你有两个work的话，就相当于整个呃全局的一个B呢是等于二的。这里有一个计算公式，大家可以注意一下。然后所有的这些函数呢，都可以这个忽略掉。就是这个data loader呢，一样是这么写的，只是我们去用了一再用了一个popular data loader去包封装我们的一个data loader就行了。然后接下来的话呢，就是创建一个这个rain trainer。这个然后输入的是我们训练的一个函数。然后还有其他的一些参数，比如说用多少个GPU啊，就用是多少个墨盒。

翠仪

02:17:35

然后还有就是如果我们要设定这个PI touch的一个参数的话，可以增加这个touch的一个参数，去增加它的一个touch的一个参数，要去训练模型就ok了，就是这几步的一个改造。那就可以把这个pay a touch啊，直接改成这个brain的一个分布式训练。它在这里的话，我们就可以对比的这个。跟刚刚的训练的一个都是一样的一些数据一样的任务。那在这里的话，我们一开始呢，就用这个呃read data去读取我们的这个CSV的一个数据。重提一下，免得它占用了我的显唇。然后把它读取进来之后呢？呃。嗯，这路径。我去写完整一点吧！

翠仪

02:19:03

然后的话呢，在这里呢去定义一个预处理的一个函数，因为我们不是要用这个to去将它这个呃编码吗？首先我们加载这个to。然后在这里的话，这个呃，把这个被输入进去的话呢，直接会把它的一个review的这一列啊去转换成一个列表，然后输入到这个里面，它是自动的去会进行一个这个呃编码。然后的话呢？去跟呃呃去把这个input in的，然后这些啊把它敷进去，因为我们的模型里面其实这个。Token啊type ID呢是没有什么用的，所以我就把它忽略了这一个。然后用一个bapper去封装一下它，然后它的一个呃数据来源呢是这个。

翠仪

02:20:06

然后在这里的话呢，我们也定义了一个函数去计算它的一个准确性，就主要是呃这个。好，这是一个二类分类计的一个准确性啊，就没什么好说的。然后我们的一个模型的话呢？这里呢借用了这个呃PY的一个函数啊，就是写了一个类。那其实呢，它本质上跟我们自己写的那个呃，模型呢，就是还是这个模型本质上是这个模型只是多了其他的一些东西，比如说它的一个呃，评估函数啊，然后它的一个P，然后这个。呃，参参考的内容，然后for的话呢还是跟它一样，我们输入的呃反馈的是它最终的一个结果，然后再去算它的这个。呃，这里呢，这个就是因为它分了不同的一个步骤，如果它是呃，圈的话呢，我们再去更新它的参数，那如果它是评估的话呢，我们只需要获取到它的一个呃，预测结果。然后去计算一下它的一个准确率就好了。然后这是它的一个呃迭代呃优化器。那在这里的话呢，我们用的这个呃那个的一个呃，然后呃把它加了进来，然后还有就是它的一个常用的一些这个配置函数的一个。呃，内容，然后去初始化，首先是我们的一个配置啊，它的一个呃类模型呢是我们刚刚的模型，然后这个啊就是我们的一个学习率，然后还有就是它的一个EPS。啊，我们的一个呃内容就是这个GPU啊，这个最多了跑五轮。然后是不是每轮都保存的，我们不是，然后建立了这个呃配置文件，然后就是这个london的一个配置文件呢？

翠仪

02:22:21

呃，我们的这个？呃，模型文件保存的一个路径名称叫这个，然后这个最多保存两个。啊，然后根据它的一个。呃表现呃就是最大的那两个，然后就是scanner的一个模型，因为我们只有两个这个呃。我们可以把它分到四个。里面去。然后把它实力画出来，用的12的一个杯去下。直接fix一下就可以看到它的一个。看目前呢，正在启动这个。我们在这里也可以去。

翠仪

02:23:05

嗯，这个节点怎么死？这个节点是不是我刚刚我刚刚开了？开了呀。

翠仪

02:23:42

然后就是他正在开始训练。哦，它它是活着的，那我们可以在这看到这呃四个GPU都被粘满了，基本上。

翠仪

02:24:10

然后嗯，每一个呢都有在做，那就是他的一个训练的一个结果。那最终的一个的话呢，它会在。这个里面呢，我们可以看到，就是这个。它会把这个数据呢给它保存下来。一些这个日志啊什么的，最后的一个权重呢，也会保存在这里。那就是这样的一个过程，就大概是这样的一个过程呢，就可以把我们正常的一个P touch的一个代码改成这个为分布的一个代码。那我们可以再看一下就是。它的一个更简单一点的一个例子。这里就不跑了？

翠仪

02:25:29

那在这里的话还是要加载我们后面用要用到的一个包啊，然后就是设定的一些参数。然后就是呃定义我们自己的一个神经网络啊，这个神经网络的话，你用这个呃，比如说里面去加载的话，也不需要自己定义，直接加载就好了。然后在这里的话呢，我们会定义一个这个训练函数。那这个训练函数就是每个epoch的一个训练函数啊，就实例它会在这个训练函数里面去实例化我们的一个网络，然后定义这个损失函数，然后定义这个优化器。然后在每个epoch里面呢，就是这个每个work的一个训练函数。然后这个呃逐个E呢去获取数据去进行这个训练，然后我们可以把训练呢，呃汇总过来，比如说我们的这个直接汇通过这个session report呢，去汇总我们这个最终的一个结果。

翠仪

02:26:35

那这里的话可以定义一个这个简单的一些数据。设定我们分布式的哈，就是我们又用这个，比如说用四个。他他这个已经释放了。然后后呢用一个touch的一个trainer，然后给它封装起来，然后它需要定义的。如果就是不用刚刚的一个light的话，用touch的一个trainer的话，我们可以就是啊定义一个training的呃，每一个worker的一个呃训练循环函数。然后它的一个分布式参数，然后还有我们的一个数据集。然后直接去礼盒这个模型。那在这里的话呢，就开始这这些都就是开始工作了。

翠仪

02:27:45

因为数据太简单了，很离合完了，那这是最终它的一个礼盒的一个结果。然后它的一个trap的话，我们也可以看到它保存的在这个路径下面。那稍微复杂一点的话，就可以看你不能说服怎么讲，这是一个例子，那大家可以就通过这两个例子呢，去看一下这个。哦，一个情况。呃，这个也还是用这个PY touch对对，我们数据集的话用的是这个呃那个那个fashion手写体就其实也是一个十分类。

翠仪

02:28:31

一个数据。我们这个数据呢呃还是放在了这。

翠仪

02:28:58

这还是一个这个呃神经网络，嗯，就卷积都不是。然后还是要定义一个就是呃，训练的一个epoch就呃，就是比较传统的那种epoch就输入这个data load model我们的一个损失函数要优化器的话，那它就可以去进进行一个epoch的一个训练。然后验证的话也可以去呃，定一个验证的一个函数，那么无非就是把数据输进去，去计算它们的一个准确率。

翠仪

02:29:29

然后就是要还是要定义一个训练函数？它这里训练函数的话呢？呃，其实也没有什么，就是获取我们的数据，然后就这个初始化模型，然后定义这个什么，然后就是开始训练了，在每个里面每个里面也去呃获取一下它的一个。呃，损失函数，然后把这个损失函数呢可以作为这个结果呢哦嗯，汇总过去。然后呢，就是定义我们的这个。呃china。然后呢呃，用这个胃呢去开始这个我们这个训练就可以了。

翠仪

02:30:22

呃，那大家对于这个训练这一部分有没有什么问题吗？那个贝贝看一下有没有同学要要提问的？

王蓓

02:30:38

嗯，同学们可以踊跃提问了。目前暂时还没有同学举手提问哦！

翠仪

02:30:48

哦，好的行，那这个时间也差不多，我们先休息十分钟，待会再继续吧！

王蓓

02:30:54

嗯，好的。

翠仪

02:30:55

嗯，好行。

王蓓

02:31:24

完了。哦哦。

王蓓

02:32:00

我们给他们。

王蓓

02:32:28

没上没上个台风的眼睛。

翠仪

02:40:50

啊，好的啊，时间够了，好，那我们继续，那刚刚讲了这个训练的一个呃模块，那继续讲的话就是一个调优的一个模块。那就是啊的话，它因为本身呢呃更多的最开始的是服务于这个机器学习。继续学习里面呢，就经常需要调优，所以这个啊在前期呢也是一个很重要的模块，但是到了现在的话，如果你要去做这个呃带圆模型的一个训练的话，基本上很少会涉及到这个超参数的一个问题，所以可能用的会比较少一些。嗯，它这个工具的话，其实用起来跟刚刚的这个其实是差不多啊，这个整个流程的话，它这个呃也是用了一些这个呃函数数据封装了。

翠仪

02:41:47

所以就是你用无论你原来用的什么算法去做的话，它的效果呢其实呃也是挺便捷的，那比如说在这里的，它有一个简单的例子，这里是一个这个呃，我们调优的。那我们首先呢，需要定义一个这个呃。我们的一个目标函数去啊，在里面的话呢，呃需要有这个获取到我们的一个data，然后去有一个模型，然后去算这个模型的一个结果，然后用我们的一个呃，比如说准确率去作为一个反馈。

翠仪

02:42:25

那在下面的话呢？去进行这个调优的时候呢，我们会设定这个参数的一个搜索空间，比如说我们的一个学习率啊，到底怎么样去设定呢？可能会好一些。呃，然后还有就是我们的一个呃动量的一个参数值。去设定的哪一个是好一些，那这个主要还是这个它呃搜索算法优化器的一个这个参数的一个条约问题上面。哼哼哼。啊，然后我们去把它这个呃，算数搜索呢，给它去做这个实例化，那最后就是用这个推呢去进行一个呃，实例化，那包括了我们去调优的时候去获取到啊，我们的这个呃，结果。的一个option。啊，就然后的话呢啊，就是我们的一个配置文件，我们的一个评估函数啊，这个然后搜索的一个算法啊，搜索的一些空间。然后就是搜索算法，然后搜索空间。呃，就是在这，然后就是运行的一些参数，然后直接就对一下灯，它就可以获取它具体的一个呃结果了。那因为这个时间关系的话，这一块呃，简单讲一下，就不会讲太多的一个类型。哼哼。

翠仪

02:43:53

那我们看一个这个类似于刚刚的一个例子，先把其他的给它换一下。目前基本上没有程序在占用。然后呃，因为这个是一个这个P的一个调用，所以会把需要用到的一些包呢给装呃加载进来。然后这个的话主要用到的是这个，还有这个S。然后就是我们的一个schedule。有连上我们的？

翠仪

02:44:38

那还是定义这个模型，这个模型定义的话就没什么好说的，就看自己的个人需求。然后就是我们的一个size和这个测试的一个size。然后定义了一个这个训练函数，其实跟刚刚的那个呃基本上是一样的，然后测试函数呢也跟刚刚的这个呃基本上是一样的。就每个epoch它里面的一个呃训练过程。然后就是我们的序列函数用的是这个fashion的一个呃MM IST。

翠仪

02:45:18

呃，data的话刚刚我们下载过了，我就不用重新下载了，要用回同样的一个路径就好了。

翠仪

02:45:43

嗯，这个训练函数呢也跟刚刚是一样的。然后我们把位启动起来，那在这里的话呢，我们需要设定我们一个训练参数哈，就是这个缩小这两个LR和这个动量的一个值。然后用这个tina去进行封装，然后把我们的一个训练函数啊，这个整一个过程呢，给它封装进去，然后把我们的一个搜索这个算法呢，给它这个，呃，搜索的一个，呃，参数范围，那如果是没有特别的搜索算法的话，它其实就是会遍地去搜索。

翠仪

02:46:28

嗯哦，前面还没有这一个哦弟弟。

翠仪

02:46:44

然后你可以看到它这个。已经搜索完了，因为我们的一个搜索的一个比较少，就是这个是0.1。到0.9然后这个是十个？那我们可以把整个过程呢给画出来。这是我们的这个？它的一个准确率。的一个变化情况。呃，我们也可以尝试的啊，去做更多的一个抽样，比如说抽20次做20次的一些试验。然后呢呃，把它的这个结果呢再汇总一下。那你怎么可以看到它现在是开了20个这个CPU呢？基本上呢是已经往上走了。就是最终的一个跑啊，结果都跑完了。这个准确率的一些变化。啊，可以看到这个最终结果。就是有一些的话呢，因为我们设置了一个这个。哦哼。搜索算法就是有一些它可能就是没办法再去收化了，就基本上最好的停止的这个啊，我们也可以用其它的一些搜索算法。那这个嗯，它这里面的话呢，它有这个呃，好几个的这个搜索算法。就是这是它的一个超参数，就是超参数的一个搜索空间的话，它可以有不同的一个分布。然后它的一个试验的话，也有可以有不同的一个搜索算法。啊，这是它支持的一些搜索算法，大家可以呃这个具体去看一下，我这里就不过多介绍了，因为这个啊时间比较有限啊，大概是这样子，大家如果有兴趣的话，欢迎大家去呃官网去去进行。

翠仪

02:48:59

查看更多详细的一些资料，那如果对这个呃有问题的话也可以，后续呢，我们在群上呢继续去沟通啊，这一块的一个内容。

翠仪

02:49:09

那下面的话呢，我们再讲这个啊，这这个比较重要的另一部分就是这个它的一个surfer。呃，它的一个搜索其实是一个模型的一个服务户，那我们经常呢，这个模型你做出来你要去，呃，应用的，那它其实就是去构建这个在线的一个API，那如果没，呃，这个像之前以前的话，就是传统的人来说，我们训练的一个模型不是。啊，不不说大模型啊，这个可能是啊，像一些bot模型啊，或者是一些这个计算机视觉的模型，OCR模型呢？或者是那个。呃，这个图片识别啊，这个图片分割啊等等这些模型啊，我们要把它不起来用的话，我们都要自己写这个呃API的一个部署。那如果它要做这个分布式的一个呃，在线部署的话呢？其实它比较麻烦的，那就是你必须要多开几个进程啊，这还是要在单机上面，那如果你要在多机上面去进行这个推理部署的话啊，就更麻烦，你必须每台机子上呢都去，嗯。啊，开启这个服务，然后并且呢额外写一个这个服务呢，去呃帮助我们去做这个呃分发排队的一些工作。

翠仪

02:50:32

呃，但是呢，如果你有微的话，你待会就会看到你这些工作呢，啊，其实都是一行命令就可以做完的，这个跟框架是没有关系的，所以你可以使用这个呃拍也好，就是cos也好，它是也好。呃差距不也好，等等这些啊可都行啊，甚至是普通的这个拍的一个呃函数啊，你都可以把它构建成一个这个呃API。就跟跟欢笑是没有关系的，只要你的这个内容代码是拍损的就OK了。你如果说你是这个呃java的，那就这个对不起啊，我目前只在这个只有拍版啊，所以说的话呢，它是非常的一个简便。啊，方便就尼布式就可以这个构建这个呃分布式的一个推理框架。而且它还有一些这个响应流啊，动态请求批助理啊，多节点多GPU等等的一个服务，只需要一行命令，所以说的话它特别适合于啊一些这个模型组合或者是多模型服务啊，让你可以去构建一些比较复杂。它的一个呃服务，然后用代码的就行了。呃，所以说的话呢？哦，非常的一个简单方便，嗯，比你自己写API的话肯定省时省力很多，而且的话因为它建在这个位置上，你只要写一份代码，你就可以拓展到很多的一个机器上面，而且提供灵活的一个调度支持啊，比如说。

翠仪

02:52:12

分数GPU就是呃，早上黄老师不是还在讲那个GPU的一个虚拟化嘛，呃，但其实问的话也支持这个分数的一个GPU，就比如说啊，你这个服务我要拓展呃好几份，然后嗯就就。见很多个worker，然后每个波口口可能只用0.5的一个GPU就ok了，那也可以啊，0.3的也可以，所以你就可以用这个更低的一个成本资源去部署更多的一个模型啊，去啊，这样你的一个并发之时了。更加好。

翠仪

02:52:51

那这是一个简单的一个例子，它其实整个surfer就是分三步走。啊，第一步就是定义一个这个surfer的一个呃应用，那这个其实就跟我们去写，你不用写flask。Fast ABI都是一样的，我们首先。啊，去订立一个这个。那里面主要包含什么内容呢？然后第一个是初始化的啊，比如说你要加载这个模型呢啊，也要加载就是top啊，你要加的什么数据你都要在初始化里面做，然后其实重点就是这个core的这个函数啊，就是当我们有请求过来之后，我们就会调用这个core的一个函数。然后我们对我们请求做什么样的处理啊，最终返回我们的一个给他就ok了，你就把这两部分给它做好了啊，然后我们就可以写好一个这个应用啊，并且这个应用呢就是用surfer的一个。

翠仪

02:53:49

呃，啊，去封装它这里呢？deployment的话它可以有更多的一个参数设计啊，这里是它的一个路由的一个设计，比如说就是这个主页面啊，你可以是有其他的一个分页面，你可以在这个页面里面定义很多不同的一个呃应用。都是OK的。然后把这个定用定义好了，第一步ok了之后了啊，我们去实例化它，那么这个实例化的话用的是bite这个函数。啊，就ok了啊，这个message写了不写都是一样的啊，然后就我们就有这个APA PP了，然后我们只要用surfer论一下它啊，我们就可以啊，把这个应用给不起来了。

翠仪

02:54:36

然后下面的话你就可以去调用这个应用了，比如说我去啊，获取就是调用给它发送这个请求，我们就可以获取到这个具体的这个结果啊，是不是非常简单啊？我们可以实力的这个来看一下。这个比你这这个可比你写什么都简单很多。就那些各种的API，你还要就这样那样，它就是完全的自动的一个分布式的一个处理。

翠仪

02:55:13

啊，就是刚刚所说的，我们先把这个相应的一个部分啊，当然这个应用就是非常简单，你输入什么我们就返回什么。那它这里的话一论起来的话，它就相当于做了一个呃，并并行的一个操作。它其实是copy了好几份到不同的一个work上面去，然后我们可以就是啊，都当你的一个必发量比较大的时候，你直接给他发送消息，他就会。啊，给你返回非常多的一些消息。

翠仪

02:55:55

Ok啊，这是我们啊，非常简单的一个例子，只是为了说明一下它整体的一个过程。那还有就是如果你平常习惯用这个。API的话呢，它也就有提供这个呃也有这个相应的例子，它其实就是这个APP呢就先用这个呃fast API呢去这个部署呃这个实例化。然后的话呢，我们封装的时候呢，呃直接呃需要多写一步，就是把这个APP呢给它封装进去。然后的话呢，在里面去写这个呃呃APP的函数。比如说在这里的我们hello这个界面的话，我们就会调用这个say hello这个呃函数。那他说为什么我们就给他哈喽谁？啊，这是我这个fast API的一个用法，然后的话后面其实是一样的，就是定义完了这个之后。啊，我们就这个还是论起来了。哦哦。弄起来直接吧，弄起来，然后这个。我们可以看到它已经有一个，然后还是发送过去，我们就可以获得这个结果了。唉，这个比如说你要改个名字啊，你就可以获取消灭的一个结果了。那在这里的话我们可以看到。

翠仪

02:57:28

现在。我们的这个？嗯。哎呀，看到卡了一下。重新一下这个页面。就我们也可以在这个所里面去看到我们的这个服务，包括我们的一个日子都可以在这里去看到。

翠仪

02:58:18

那这两种方式都是可以的，就是看你个人更习惯于用什么，就是它这个比较原生的一个方法，就是这个呃，fast a API的一个写法。

翠仪

02:58:35

那如果你要用这个呃的一个呃，数据，就它它不是有那个什么吗？上面这个其实就是这个这个包啊，大家如果有兴趣的话，其实后面我们也在考虑去开一个。课程去专门想这个transformance这个包啊，因为我看呃，这个包确实是现在是非常实用的一个包在在语言模型的一个前提下面，但是有很多同学可能都不怎么。呃，会用这个包，对后续的话可能还会拍一门课去讲一下这个怎么去调用啊，包括这个呃，这个它不是还有一个的一个呃，分布式训练的一个框架嘛，其实到时候会把H。它相关的一些生态呢，也会给大家详细的讲一下。啊，就是题题外话了。

翠仪

02:59:31

然后回到这的话，其实呃用这个模型的话呢，它其实用起来呢是这个方法是一样的，就是整个过程也是一样的，就是我们在这里呢。去啊，这个一个这个模型，那在这个模型这个模型的话，它用的它的一个管道，这是一个情感分析啊，就如果我们自己写的话，就待会啊，可以去看一下别的例子，自己写的话呢，可能是用。呃，这个呃，就比如说像T GM的话，那可能我们就是把那个呃。通过les和model都初始化，然后它括的话就直接把它的一个呃，请求里面的一个txt的数据输入到模型里面，就会返回这个相应的一个结果，那我们同样我们的把这个模型呢，给它部署起来。

翠仪

03:00:46

然后它需要下载这个模型，所以就可能时间会比较慢一些。然后同样的我还是快一下这个数据，然后给他出这个参参数的话，它就可以给你返回，那只是它的一个返回结果。

翠仪

03:01:12

这个就先不看了，那如果呢啊，就是有一些应用场景的话，可能我们整个嗯。流水线流程呢？它可能可能是啊，不止不仅仅是呢，只是用一个模型可以去做完的，它可能是啊，比如说像这个啊，我们可以。去把这个多个模型给它组合起来的话。比如说像这个哦，我们有一个组合的一个模型啊，用的是这个，它有一个，然后还有summarize。呃，我们呃构建了两个，这个之后的话呢，我们把它建立起来。是这个。在这里呢，把整一个summarize呢去给它呃建立起来，那这个summarize的话，它其实呃去做core的时候呢。它不仅仅去做了summarize，还去做了这个translation，就是它会先把你的这个输入的这个数据呢去总结一下，然后把它翻译成这个呃，别的语言。然后最终呢，把翻译的结果呢，然后给它返回过去。那其实这个这是一个异步的一个操作。那所以这里的话，我们同样的这其实是定义了两个这个模型，每一个模型的话它都是一个，呃，但是呢，我们在调用第二个模型的时候呢，就把这个流程呢给串起来了。就先把我们的一个英语文本去做一个呃，summary，然后再把它翻译成这个法语。最后呢去返回到我们的一个数据，那我就可以去得到最终的一个输出。但是呢那个？哦，可以，这个运行一下，就是这里的话。然后运行完之后的话，你就可以把这段话给输进去，我们可以返回这个结果。

翠仪

03:03:42

我看看这里你可以了吗？哎呀。这是它的一个情感分析的一个结果。因为这几个它掉的是同一个呃这个端口啊，都是没有没有改端口的，默认的是这个呃800的。呃，事实上的话呢，我们可以在这个。

翠仪

03:04:36

看看他对那个。配置的一个参数上面呢，我们可以去设定它的一个呃端口。

翠仪

03:05:06

嗯，它里面的一个内容呢，其实还是挺多的，大家可以去看一下，就是它官方的一些例子。啊，比如说大家如果想部署一个diffusion model的话呢，啊，也可以去看一下，就是它有提供这个啊，因为model的话就呃这个扩散模型的话，它本身上是挺大的。哦哦，在上一次那个这个AI GC的一个特训营里面，我讲过。它是呃部署起来确实也挺麻烦，它本身这个运行速度还挺慢的，但是如果你用位去呃资源足够多的情况下，你用位去做B型号的话，可以有效提升你的这个呃diffusion model它的一个推理的一个速度。那用呃，surfer的push模型的话，它就是呃有什么好处呢？第一个就是它可以直接构建一个端到端的一个应用，因为很多的这个我们模型它其实就是一个T的一个输入。啊，最终输出的可能也是一个探测啊，那大圆模型它它除外，它除直接输出的就是它的一个文本，可能就是有点不一样。但是的话呢，在其它的一个应用上面的话，端到端的一个方案，其实是会比较的一个。

翠仪

03:06:32

简单呃，更加的一个实用，那就是的话，它可以在单个框架里面去构造这个端到端的一个分布式应用。啊，也可以呃，使用这个fast ABI的一次集成啊，然后这个其实你只要你的代码是拍成写的话，它都可以帮你去做这个分布式的一个部署，就从这个，呃。这个投入成本来看的话呢，是这个代码开发量非常的一个少。

翠仪

03:07:03

第二个就是它可以这个将多个API的组合，这这这个多个模型组合在一起。最好不需要说你一个模型布一个API，我每次调查的时候还得写，因为还得发一个请求过去。然后而且它的一个呃拓展呢，资源分配呢是比较灵活的啊，它本身内部的一个算法呢，就是有在追求这个呃灵活的一个拓展和这个资源分配上面，你可以随时啊，按照你的一个需求啊去增加这个。嗯。

翠仪

03:07:40

呃，你的集群的一个节点，然后当你不需要的时候，你可以关闭一些节点去这个呃，节省你的一些资源，也可以避免跟这个框架或者是供应商锁定，就你不一定说你一定要用这个pouch啊，不一定说一定要用text for。反正这个呃它是这个没有框架限定的。

翠仪

03:08:07

然后的话呢，它的这个呃有一些这个C的参数，包括这个，呃，它就可以去部署了这个副本的一个数量，然后它可以呃去自动的去做一个负载均衡。

翠仪

03:08:24

然后呢，呃，还有就是说你的这个每一个worker它的一个，呃所用的一个CPU的一个数量，还有这个GPU的一个部，呃，数量，然后它这里的话都是一个浮点数的形式，你不一定要用一个完整的一个CPU去。作为一个worker就是当你的这个呃。程序所需要的一个CPU资源或GPU资源比较少啊，不至于占据你整个这个呃GPU或CPU的时候，你可以给它一个浮点数就可以了，就非常的一个简单方便。那它运行的话也是非常的一个简单，比如说刚刚的这个，我们也可以去运行一下，但我不太确定我这个镜镜像上面有没有。下载相应的一个模型。

翠仪

03:09:34

哦，我们这个。这个这个这个叫做。

翠仪

03:10:32

那怎么这个函数学的写作？

翠仪

03:11:14

啊，这个脚本可能有点问题啊，但它大致上是这样的一个操作啊，没事，然后后面的话呃。我们再去修改一下这个脚本在共享的大家上呃，共享给大家，那这大概就是这个它几个模块啊，当然还有一个这个呃，强化学习的模块，但是因为时间关系的话，就没有安排这部分的一个内容，就如果大家有兴趣去做这个。强化学习，用位去做一个分布式的一个强化学习的话，可以看一下这个模块，那上面的例子的话其实是。还是会比较几年前的一个强化学习的例子，就跟它为1.0的一个版本的话，这个模块其实没有太多的一个更新，因为近几年强化学习好像也没有太大的一个呃，突破性的一个进展。

翠仪

03:12:11

好像到了阿尔法基，我们那边就好像就是目前暂时没有更新的一个发展了，所以这强化学习现在呃做的人倒是不是很多？那最后的话就是这几个模块综合起来的话，我们可以构建一些这个实战应用啊，就比如说。我们要部署和微调自己的一个大语言模型的话呢，我们可以有这样的一个这个框架，其实就是啊，我们的模型呢，一般可以从这个上面去下载，比如说像这个。哦，穿JPT呀。呃，还有就是呃呃这个拉马啊等等这些可以在哼跟飞上面去下载。然后数据的话呢，可以是自己准备，也可以是在上面下来，然后我们可以用这个呃去帮助我们去呃，配置我们的一个训练，它本身就是一个变形训练的一个框架，但是呃它本身有一个呃问题，就是它。这个它又去做并行的话，它去处理到不同的这个呃，节点之间的一个通信，它需要用大量的SSH million去进行这个通，建立通信去沟通。其实这是一个非常麻烦的事情。

翠仪

03:13:34

而这个为呢，其实就是呃弥补了这个在这些通信上面的一些困难。对它们俩可以配合打配合一起去使用，然后P touch的话也是作为一个这个目前呃最呃相对来说。最常用的一个这个深度学习框架，我们可以用它啊，加上这个transformers的啊，去进行这个模型的一个训练啊，肯定呢，最底层的话其实是需要到有这个呃GPU的一个支持。啊，就因为大语言模型没有GPO训练实在是太慢了。那这种这个部署或者是自己微调带原模型的话，其实呃从成本来说呢，就这样的一个架构的成本来说可能会比较低一些，而且数据的一个安全性上面呢，可能会更好一些。啊，当然那个呃，因为GBD3.5它并不是一个开源的算法，就大家如果想要微调GBD呃系列的话呢，可能还是得这个自己付钱给open ai。但是如果你想微调别的语在语言模型像norma。这个之类的一个呃东西的话。

翠仪

03:14:51

本身的这个呃显卡资源可能就是充足的，你想要要去做一个这个分布式训练，去提高你的这个效率的话，呃，可以借助于整个这样的一个架构，就是为加上去做。那我们来看一个就是我们呃可以用一个这个。就它部署的话呢，其实我刚刚接着刚刚的一个例子上面去做，如果我们要。啊，比如说要不属于一个大语言模型？

翠仪

03:15:33

呃，比如说像这个TGOM。View的话，整个部署过程也是非常简单。在这里的话呢，呃，我们把它的一个。模型。这个用四个，这个每个每一个用两个这个GPU去做。然后。我不知道它那个有没有加载这个。

翠仪

03:16:24

哦，可以直接就是跟刚刚的这个的一个例子是一样的，我们去直接去编写一个这个类，然后在里面去加载我们的一个模型和这个。

翠仪

03:16:38

然后的话呢，呃，它这个呃，调用的时候呢，只需要去把我们的这个，还有就是这个的一个文本啊，还有我们的这个历史的一个记录给它输入进去，它就可以去做上下文的一个推断。

翠仪

03:17:00

哦，它这里报错了。啊，这个他他他他他。刚刚好像嗯！这个报错要具体看一下它为什么报错。哦，这里用到的一个这个文件。然后有一个这个work上面是没有的。还是我这里没有这个。

翠仪

03:18:14

这有问题啊，这是用了它官方的一个这个。哦，这个已经重返过了，就是呃因为我这个大家会看到这个显卡上面呃单11个显卡，它那个TGOM的话呢它是。呃，是它它，它要13G左右的一个显存才能加载进来，但是我这个服务器上面的一个显卡的话，大概就只有这个，呃，一个显卡大概是八G不到，所以它加载速。不全的，所以用了一个这个官方。所提供的一个分布式加载，就把它加到两个这个GPU上面，但它可能是在这一步出错了。那它找不到这个？啊，就这个具体得看到它这个哪里，就是它找不到这个这这这个模块啊，就有点奇怪。嗯anyway嗯，这个后面可能去调调整一下就是呃。这这是一个TGLM的一个部署，就是如果你的这个显卡足够，比如说你显卡是这个呃16G的啊，你可以去部署一个TGLM在上面。啊，就是没有问题的。然后啊，你有多个16G以上的显卡的话呢，你就可以把显卡啊，这个模型都加载啊，分发过去啊，在不同的一个啊模块上面去进行一个并行的一个呃计算。

翠仪

03:20:09

那下面给大家讲一个例子，就是说，呃，用这个呃微调我们的一个大语言模型。那这里呢用到的是这个GPT的一个G六B，因为这个GLMGPT没有开源，所以呢，我们可以用一个这个呃开源的一个版本，就是这个GP DJ啊是一个简化的一个版本，它大小模型跟那个GLM。这也是差不多，大家都是有鼻的一个弹塑料。哼哼。那就是它的一个呃，版本的一些要求，那在这里的话呢，呃，它用的是一个这个呃，这个上面的一个dataset，就是莎士比亚的一个呃，简单的一个数据集。然后把它分成了几个模块去进行这个训练，那这个数据集其实就是这个莎士比亚的一些这个作品。然后训练的时候了，无非就是。因为这个大语言模型。它的一个训练无非就是输入上一句，然后预测下一句的一个概率，所以它的一个数据呢就不，呃，基本上呢是，呃，只要把它分坏啊，这个把它这个划分一下。我们就可以得到它的一个最终的一个结果，然后把它划分的一个结果，呃，去做一个通过来时就不需要去做额外的一些处理了。在这里的话，我们去定义每个worker上面的一个训练呃就。

翠仪

03:21:47

在这里的话呢，它要用这个互打的一个32位啊！去进行一个更好的一个呃，结果，因为一般都是16位的，如果你需要用到32位的话，可以获得更好的结果，那如果你对这个显卡不支持的话，可以把这一个呢给它去除掉。然后的话呢，就是一些被size的一个设定，然后就是，呃，这里所用到的一个deep speed的一个设定，因为我们这这个就是用这个deep speed加上我们的一个位去做这个微调。啊，它的一个体现在可以直接写在我们的一个trainer的一个函数里面。那就是它的一些设定啊，这个设定的话，具体什么意思呢？我就不11解释了，就是大家可以参考的一个官方的一个网站。

翠仪

03:22:47

然后在这里的话去构建出了一个trainer的一个呃，就参数的相关的一个参数。然后我们的一个模型啊，通过哪些啊，还有我们的一个模型呢，都可以加载进来。然后呢它就可以去呃，去计算，呃，在里面也也定义了一个这个呃评估的一个函数，那我们整个trainer呢就可以给它封装起来了。就把这个圈等呢，给它返回回去。这个china呢，其实就是里面的一个。因为它是transformers上面所提供的。然后的话呢？呃，这上面这些呢其实都是。呃，这这这这几块其实就是那个dispute它的一个训练的相关代码，只不过我们把它写成了这个呃，trainer的一个形式，每个这个work上面的一个display的一个train。然后下面的话呢，我们就可以用一个transformer的一个trainer呢，去把刚刚我们的这个呃函数呢给它封装起来。啊，然后我们的一些参数，这是跟刚刚所写的一个参数呢，是类似的。然后这个F一下，我们就可以去把这个相应的一个模型去保存。然后有了这个模型之后的话呢，我们就可以去读取模型，去得到一个predata去获得我们的一个。预测的函数，那整个流程是这样，那有了这个模型之后，你也可以去回到就刚刚的这个呃。

翠仪

03:24:36

部署的一些环节去呃，部署一下我们这个呃GPTJ的一个模型啊，这个整个流程上面是这么去做的啊，但是这个脚本呢，我这边没有好，是因为，呃，它所需要的一个时间太多了。啊，就大家如果有兴趣的话可以去跑一下，它要求的是单个显卡。需要在16G以上才能跑得动。哦，这个。然后呃就是。在这里的话呢，它去做一个微调的话，主要还是集中在这个呃模型并行，但是模型没有切分数据并行啊，像这个解决方案。那这个大家可以回去再认真看一下这个。呃，代码。就是，呃，这位跟dispit它是可以一起去使用的啊，这个with的话呢，它其实其实是帮去做了这个调度的一个问题，就是因为它如果你单纯用disbit的话，你需要去定制一些呃，这个S。

翠仪

03:25:45

什么命令啊，有这个目标管理啊等等这些就很麻烦，但是我可以帮他做了，呃，然后训练的话还本质上呃还是这个disbed在训练，只不过了，我们给帮他去做了这个分布式的一个调度，那这就是这两者之间的一个关系。然后呃呃之前就是这个也有同学问啊，这个分布式这个呃，我在前两个特训营里面所讲到的这个向量数据库的一个问答机器人啊，他如果也要用位去做分布式的话。它能不能做，但是能的啊，这里给了一个大概的一个思路图啊。就是呃，我们因为整个这个呃问答机器人其实它分为了两步骤，第一步就是这个呃，我们需要去构建这个相当数据库，然后在构建项目数据库里面的话，拥有本身这个文本就是拆分的。那在拆分了这个文本之后的话，其实我们可以并行我们的这个向量化。我的一个工作，然后一起去写入到这个模型里面，那在这里的话可以提高我们的这个速度。那所以它整个流程的话，只需要在原来流程上面去改动一些方法，就是在这样的画的时候，去加一个这个的一个装饰器啊，就可以去啊，做这个并行的一个这样的画的一个写入了。

翠仪

03:27:17

那第二个呢，就是在这个问答的时候呢，啊，因为也有同学之前在问啊，就是我们要啊，应付一些大并发的一个需求的时候，我们肯定是要并行，多开几个不同的一个服务的。那在这里的话呢？哦，我们可以呢把我们的一个模型呢，呃，这个这个做一个并并行的一个并发的一个处理，就是多个不同的一个代语言模型。然后在呃这个呃提问的时候，它可以自动去分发到不同的一个模型上面去，然后查询同一个数据库，然后把结果呢最终的返回，那这样的话呢，我们可以呃处理一些告并发的一些情况，因为一个模型呢可能实在是。

翠仪

03:28:04

处理不了那么多的一个东西。那这个步骤的话，具体上呢也是写了在这里，那这个也可以去参考一下，给大家的一个数据样例。那这里的话只是写了一个大概的一个思路哈，这个可更多的一个IT过程的话，可可能要靠自己去封装了。啊，这是一个？它的一个象征化的一个并行，也是借用了这个L的一个架构啊，就是呃，数据库的一个部分啊，这个象征数据库的一个部分，这是我们的一个呃，切分器的里面切分器的一个部分。那我们可以呃设定这个呃路径，然后这个把这个PDF给它读进来，当然你可以包括其他的内容，然后在这里的话呢，我们去定义这个呃PDF的一个loader。然后去做处理的时候呢，我们就可以写一个这个处理的函数啊。在这里的话呢？就是其实就是一个in的一个函数。那这个embody的一个函数的话啊，可以根据自己的一个需求去选择一个函数。比如说像医学的，你可能用你选上的一个函数啊，普通的可能用普通的标的函数。

翠仪

03:29:26

甚至你可能觉得自己需要用这个呃，GPT提供的一个函数都是可以你去修改一下，然后把它写到这个数据库里面，然后返回一下。那在这里的话呢，我们只需要把我们并行得到的这个trust啊，说这个共建这个函数。啊，我们就可以去做这个变形化的一个处理。啊，就是这个相当的一个并行。那这是过问答的变形的话呢，其实啊，重点也还是在这儿，我们去这，这是这个GM的一个调用方式啊。然后我们这个CG MN的话，我们就是有一个模型啊，包括我们的LOM和这个P。我们运行的时候呢，就是把我们的一个呃，查询，呃，和这个内容呢，给它输入进去，它自动构建我们的一个泡，然后在这里的话就是定义一个这个向量。搜索的一个。那我们的一个向量化的同样的还是跟刚刚是一样的，我们需要把这个呃，数据呢，呃去做向量化处理，那它在搜向量搜索的时候呢，就是这一步。

翠仪

03:30:42

像搜索的时候的话呢，我需要去把这个呃数据，这是我们原来的这个数据的需求下的话，要输入到这个数据库里面去找到这个搜索结果，然后把这个搜索结果呢，呃，给它这个合并起来。

翠仪

03:30:59

一起返回到我们的里面，然后让它去进行这个查询，然后就可以得到最终的一个结果。

翠仪

03:31:08

那这是我们最后的一个C的函数，直接搜是用这个搜索这个函数去做就好了。那在这里就结合了两个过程，一个是我们的这个呃，向量化这个搜索，然后还有就是我们的问答。然后我们就可以把它这个模块呢，用这个rain呢，给它这个并发构建起来，那这个它能并发多少的话，就看我们的一个资源池里面有多少个GPU了。那大概是这样的一个过程。就呃，大家有兴趣的话可以把我们之前所讲的这个向量户基于向量户呃这个搜索呃问的问答机器人呢改造成这个分布式的啊，可以适用于更高并发的一个需求。那最后一个话题的话就是这个呃，大语言模型的一个分布式架构，因为前面说了这么多，呃，其实大家可所看到的例子都是我们所有的这个模型。其实它整个都没有切片的哦，它都是整一个放到这个GPU GPU里面，那像我们刚刚遇到的，我们这个GLM啊，它要十六，十三G的一个显存，但是我一张显卡呢，就只有七G。

翠仪

03:32:32

啊，这个很明显是不够放一个模型的，那这时候需要怎么办呢？哦，需要我们需要啊，把这个模型去进行这个切分。但是这个切分方案的话，其实没有一个这个，呃，标准，就是怎么去切分，你是从中间这个拦腰斩断，把它一分为二呢？还是说，呃，就要根据这个模型它本身的结构，那比如说有些模块它是可以并行的，我就要把。按到不同的一个呃work里面去啊，然后有一些就串行的，它就是啊，要要串起来去这么走的啊，这个切分的话呢，是需要去人们去定义的。

翠仪

03:33:17

而且的话呢呃这个。瑞其实它没有去做这个大语言模型的切分，所有的切分呢，都是我们另外去做的，所以说我们需要更多的一些呃，别的一个框架的去帮助我们去做这个模型的一个分布式。那这里的话可以给大家介绍一个这个app这个框架，这个框架的话，其实它就是解决了这个模型的一个并行化的一个问题。就大家问这个CG PT啊，GPT是啊，它这么大的模型，我们这个显卡。显显存才，那么这个呃16G32G能有个100G都很贵，很了不起的，那它这个模型呃显存肯定是几百G级的。他怎么去做呢？它其实就是要有一个模型的一个切分。啊，就我们这个切分的一个并行的需求的话，其实其实要要求就是说他们这个并行之间通信成本呢要低一点。啊，不然的话呢，呃，它这个通信成本太高的话，像我们一开始举的那个open CV那个并行的例子，我单纯我不做并行，我串行，我在单机上面运行零点零几秒啊，我一做了这个分布式，而且这个最快也要。

翠仪

03:34:44

五秒多，那这个差距还是挺大的，然后就是说这个算呃，这个算子内部的话，那个病情呢，呃。呃，通信成本呢？会更高，它就是有不同的避。那它整个这个逻辑上面的alpha这个做的是什么呢？阿pa它的底层呢，其实还是位，但是它在R上面去做了一些更多的东西，就是帮我们去做这个模型的一个切分，就是它把。呃，它其实不能说叫模型切分它，因为说我的模型呢在这个。内部里面都可以看作是一个计算图。那我们就是要做一个计算图的一个切分，把它这个图分成不同的一个子图，然后不同子图之间呢，我们是希望他们可以就是尽可能的并行。那如果有串行的需要的话呢，我们可能是有彼此之间串行的一些通信。那阿帕这个模模块的话呢？它就是要去做这一个，呃，寻找一个更合适的一个。并行的策略，它追求的是一个全局最优，而不是一个局部最优啊。然后就是比如说它这个有ABCD啊，这个图要经过这四块，那我们要把它布置成四个模型。

翠仪

03:36:13

那它是这个呃ADC D。呃，这个呃这样分开呢还是这个呃我们要把它放到两个不同的显卡上面去，那我们是要把它放成这个AB啊，一个显卡CD一个显卡还是a一个显卡？BCD一个显卡呢？还是说ABC啊，一个显卡。

翠仪

03:36:34

第一个显卡还是说ABCD啊，但是它显卡内部这个划分啊，可能内部的一个算法也有点不一样。呃，内部是一个呃，串行的呢，还是一个并行的呢？就是这个策略呢，可能是有非常多的，那我们怎么去找到它的？

翠仪

03:36:53

就是阿帕呢，就是解决这个问题。它就会去首先去算一下，就是每每个算子之间的一个计算的一个呃，速度啊。因为来说的话，就你这个神经网络啊，这个ABCD啊，就是是个模块相对比较独立，但是不是每个模块它都是这个计算速度是一样的。它可能它最终的一个最高的一个算法，可能是这个呃，a是一个，然后BCD是一个，那ABA里面的话可能是可以把这个数据呢去做一个这个并行的一个操作，然后BCD的话可能有并行，有串行。

翠仪

03:37:33

这样的一个操作，它是最坏的。那它就会可以把这个数据呢去做，那这样切分了去做这个并行操作有什么好处呢？就原本大家想一下，就原本一个模型没有切分成块的时候呢，我们要去预测一个模型，结果的话。

翠仪

03:37:53

啊，我们肯定是串串行去做的，就是一个数据来了，我们就a算完a算。B那是，那其实这时候就会有一个闲置的问题，就是说啊，我们这个呃，下一个数据要算的时候，但是其实这个a是闲置的。A在算的时候，BCD都是闲置的，就是无论我在算哪个部分的时候啊，其他部分都是闲置的，那这样我们的一个效率就不高，就大家会自己去看一下，就是自己呃，去布这个C UIM的时候，你是不是觉得它？人多的时候就回的特别慢啊，就没有考虑到这个并行的需求，但如果我们把这个，呃，模型大模型分成的几个指块，那我们就是在算的时候呢，呃，我们可以做一个异步的一个计算，那首先我们有一个这个数据来了，我们再算。A。

翠仪

03:38:46

然后a算完了给B，然后a现在空了，我们可以继续去算下一个数据，然后等它算完了再给B。那这样的话其实它每个模块它都是同时在工作，而不是在闲置的等别人啊的工作完了我才能进行开始下一个工作。那这才是一个更合理的一个并行的一个设计。哼哼。所以就是大语言模型，它也能比呃，普通的一个模型啊，是这个速度啊，也也也可以提的差不多的话，其实它也有这样的一个并行的一个设计，那阿帕的话呢，专门去找一个最佳的一个策略。可以使到这个划分就是划分这个不同的一个块啊，把它这个，比如说它这里把a啊分给这个显卡一，把BCD分给显卡二，这可能是这样去啊，最快的。

翠仪

03:39:41

那最终的话会有一个运行时间，作为它的一个这东西的一个结果。那这个的话呢，呃。它的一个安装呢，相对于比较复杂。呃，而且它跟那个rain呢是有一点点的一个版本冲突的。呃为我们现在用的是二点六点一，但是alpha的话它依赖于这个ray的一个版本呢是二点一点零的版本。所以一开始我这个。

翠仪

03:40:13

可能装过这个阿帕，然后它就是在二点一点零的一个版本上面。呃，后面需要重装一下，它才更新回这个二点六点一的一个版本，所以建议大家这个安装这个aa的时候呢，呃呃去分开一下，然后它有对这个酷打和这个CU DNN呢也有这个版本的一个需求，它需要在酷打11。点一以上。然后这个CU DNN的话是这个八点零点五以上。然后这个安装过程的话，我之前也录了视频啊，给大家这个稍后给大家发群里面，我这里就不再重复了啊。然后的话呢，主要是讲一下就是a它本身也是支持这个呃分布式训练的，它这个呃aa的一个使用的话是跟那个R也是一起的，但可能没有那个。没有那个那个那个？呃，就是的那个例子上面一样啊，它可能是以味为主的，只是里面的一个封装器，如果是用a的话，因为它底层就是这个V，它可能呢，呃，就是这个整个语言呢都是以这个为准的。

翠仪

03:41:35

所以它的一个初始化呢，都是这个阿帕的一个段落，也要会用，是用这个R呢去做这个呃，分布式。就这个训练的一个代码，可能大家也可以参考一下，它写起来其实。嗯，这个字可能大一点，它写起来的话呢，它这个整个框架跟那个rain是差不多，毕竟它底层就是为，就是在上面去做了这个分块，呃，模型切分的一个算法。模型切分呢，你不能在训练呢，还是在这个呃嗯在在在这个呃推理的时候就部署的时候呢，都可以得到更高的一个效率。那它跟位的一个用法其实都是一样的啊，我们会有一个这个。就是每一个这个它的一个训练函数，然后在上面去封装我们的一个阿帕的一个变形化的一个装饰器就可以了，然后就可以启动这个呃，阿帕他的一个训练。然后就呃训练完了之后就刷单就可以了，那这个时间关系确实是讲不了那么多，那这这里这个呃这个训练的话，大家可以去看一下官方的，然后还有就是这个。呃，部署的本来想给大家演示一个例子，但是这个OPT的模型它这个。

翠仪

03:43:12

呃，实在太大了。它整个权重呢呃，需呃它它需要350G的一个GPU才能把整个这个OPT175B这个模型的一个参数呢呃给放起来。呃，然后。呃，三十，三百五十G的话呢？呃大概就是大家想想一个显卡。呃，就这个一个显卡就是就不如我是这个好一点。呃24G吧，就就十几20个显卡，然后我要布一个十几20个的一个集群都好像划不来。呃，所以就是呃，没办法给大家演示，但是它里面的一个思想的话，就先看它的一个demo给出来的，好像好简单，就是用一个get model就可以把它这个模型给它部署起来。

翠仪

03:44:08

那这这个它所有的金税都是在这个该model的一个函数里面。那它这个model的话，其实呃，还是它这个阿塔去进行这个权重经过这个权重很坏的一个处理的。呃，其实阿帕他目前的话呢，呃，官方只给出了这个OPT系列的一个模型知识，然后还有就是呃。还有就是这个呃，还有这个burn这几个模型的一个知识。就其他的模型的话呢，可能就是需要大家去呃，去改写一下它的一个代码，因为，呃，它，它的一个权重的一个形式呢，其实是还是跟普通的这个。哼，跟肺上面的一个权重呢是不一样的啊，虽然它这个呃，阿帕上面呢，它是，呃，提供的这个demo上面呢，它都是做了这个权重的一个转换的，那如果是其他的一个模型权重的话，可能要参考它这个。呃，175B的一个权重转换的一个方式才能转换成这个阿帕的一个方式。就是有利有弊吧，就是用起来会麻烦一点，但是它的整个策略上来说的话呢，可能会比你自己去想会好一些，就这呃，就去把它的这个呃策略。

翠仪

03:45:42

就是这个切分的一个策略会好一些。啊，当然这话就如果大家想需要把这个呃CG LM啊，这种给它切分的话呢，其实如果是自己要切分的话，你我建议你看一下这个CG LM它的官方里面它有一个这个多卡加载的时候。它是怎么切分这个模型，因为这个模型它可能有些部分它是注定要一起。有些部分它是可以分开的啊，那这个逻辑的话，可能就是更需要多一点的一个人为思考，那你思考了这个啊，你得到了这个模型的一个。

翠仪

03:46:27

切分的一个方案的话，那你就可以呃，把你整个模型呢给它呃分成不同的一个小块，然后放给不同放到不同的一个work上面去部署。然后呢，给它串起来就好像类似于这个我们的这个呃，多个服务给它串起来就好了，大概是这样的一个做法。呃，今天的内容呢，可能就是到这里差不多剩下还有这个大概十分钟的时间看一下大家，我们什么问题或者什么有什么想要交流的，我们这个呃一起交流一下。

王蓓

03:47:04

好啦，同学们可以举手提问啦！

许涛

03:47:11

何老师。

翠仪

03:47:13

哦，你说。

许涛

03:47:14

就是那个。呃，就是那个PH里边儿不是有那种分什么GPU版本儿的，还有CPU版本。

翠仪

03:47:23

嗯。

许涛

03:47:23

我的理解是不是这样的，就是咱们现在用了ray之后是不是这个touch的这个GPU版本是不是可以不使用了呀？

翠仪

03:47:33

呃，他们不冲突啊，你用为你需要也需要用它的GPU版本的呀！

翠仪

03:47:39

只不过你的这个呃土库的这个叫呃这个函数由这个魏帮你去实现了。

许涛

03:47:51

嗯，那他怎么理解它这个这个R他帮着干，那他既然既然它已经实现了这个这个这个，那这个pouch的这个GPU版本儿他们就是。那就可以可以替替代替，就是代替他去做这个东西了吗？我我我是这么理解的，可能不知道哪可能是个。

翠仪

03:48:07

啊，不是的，不是的，所以其实rain它只是这个呃，相当于在不同的一个呃，进程开了不同的一个pouch的一个训练进程，那你这个进程里面的一个P里面的一个运算，还是要依赖于这个pouch的一个。呃GPU版本的，因为它需要用到酷。如果你这个只装了一个CPU版本，你要放到它那个GPU上面，它其实是用不起来的。你虽然给它分配了GPU，但是如果它本身是不支持GPU的话，它是不会把它放到GPU上面的。

许涛

03:48:47

啊，就是说这个瑞如果。想要充分利用它那个GPU的能力的话。他还得要去介入？就是因为他最终还是通过去调用那个拨打的一些东西。

翠仪

03:49:00

对对对对对！

许涛

03:49:03

这个瑞他自己本身他不去用就用扩大那些东西嘛。

翠仪

03:49:07

呃，不调用它本身不调用它其实就是，呃，更简单来说，你就把它看成一个这个多进程管理，然后它就是帮我们去开进程，那进程里面所有的工作就是我们自己代码要写的东西嘛，那这个代码你在这个进程里面，这个环境。呃，依赖的它它它是要依赖于一个GPU的一个呃P touch的话，那它可。它它才能用GPU的一个个这个相关的功能啊。那如果你本身装的是CPU的pouch的话，即使你给它分配了GPU的资源，它也不会用的。

许涛

03:49:46

也也就是说这个。呃，这个瑞它它就是目标是就是比较单，它就是为了实现一个分布式并行计算，也就是这么个目的。

翠仪

03:49:53

对对对对，它就是一个分布式并行计算的一个框架，它不能取代任何的深度学习框架。

许涛

03:50:05

那那那这个app它它不是这个app这个东西。呃，是不是可以切分这个大语言模型，它它这个切分大语言模型本身跟这个GPU也没啥关系是吧？

翠仪

03:50:16

哦，没有关系。

许涛

03:50:16

他也就签。呃，然后就是他切完了最后把切分出来的那个结果，然后又交给了单独去调，是这个意思吗？

翠仪

03:50:26

啊，内饰差不多就是它，它其实切分的是这个计算图，就拍touch里面的计算图。就把它分成了纸图，然后他把这个他他做的就是分子图这个工作，那具体计算的话还是拍套去去做的。

许涛

03:50:43

行，然后嗯，最后你刚才说的那个叫什么？OPT那个东西我我没听过那个是什么东西。

翠仪

03:50:51

哦，这也是一个大语言模型。

许涛

03:50:55

这个有什么特点，因为之前一直没听过这个东西，因为一上来就175B这个。

翠仪

03:50:59

这个其实就是那个玛塔，就是facebook它去这个之前所推出的一个代言模型，后面才吹的这个拉玛。

许涛

03:51:02

才有动力。

许涛

03:51:13

它相较于拉马它它两个哪个更先先进一些这个。

翠仪

03:51:18

嗯，先进一些。呃，其实差不多吧，就是大家的参数量不太一样。

许涛

03:51:27

嗯，它使它底层使用的那个？嗯，算法的东西都还是穿方面的那那套东西是吧？

翠仪

03:51:34

啊对对对对对！对。就拉马是它调优得来的嘛，你可以把它看作是一个更原始的一个拉马。

许涛

03:51:47

行。明白。

翠仪

03:52:04

嗯，还有其他同学有问题吗？

翠仪

03:52:08

嗯，你说。

王波

03:52:09

那个我想问一下，就是刚才那个每个节点就是每个linux节点的文件和目录结构是不是要完全一致？

翠仪

03:52:21

其实这个的话就比如说像你涉及一些这个。呃，比如说模型读取的一个操作。呃，你这里给它输入的是一个路径。那它肯定是要求你的路径是一致的。但是如果你这个模型啊，你用呃这个你的数据事先是在这个组的那个节点上面啊，你把它放到了这个V data里面的话，这个就没有必要了，因为它会自动去呃共享过去。

翠仪

03:52:54

就不需要从本地读取。就是你所有要从本地读取的东西的话，你就是必须要这个路径是一致的，放的东西是一致的。

王波

03:53:06

哦，明白就是说。假如我有一个文件的文件。在这个节点的另外一个位置，我就改相应这个。这个节点的那个路径明就可以嘛？

翠仪

03:53:24

不太行，但它它它要放在同一个位置上面，就你只要要从节点它自己的一个呃，本地读取的话，它都是要统11个路径的。

许涛

03:53:42

何老师这个这个模型切分的这个策略啊就是。

翠仪

03:53:43

哦，你说。

许涛

03:53:48

呃，是是另外一块儿被另外一块儿东西了是吧？这个可能讲就麻烦。

翠仪

03:53:52

哦，是。是，是另外一块东西了。

许涛

03:53:54

啊。嗯，这个后边儿会会会涉及到这这这方面儿东西吗？这个。

翠仪

03:54:03

后面。呃，后面是指？

许涛

03:54:08

就比方说后边还还还有没有类似的东西课什么的，或者是或者是。

翠仪

03:54:08

后面的人。呃，这个要看一下大家的一个反馈，就是后续与我有需要的话可以再去讲，呃，开一个类似于这样的一个课程，就是讲模型切分的。

许涛

03:54:37

啊，对，我还有个问题，就是说现在就是说。那个呃，咱们现在说的这个模型都是说就是说这个。呃，我现在有很多的这个，比如说拆的PPT吧啊，它肯定是一个模型训练出来的，拿来去用啊，嗯，现在是不是有有有有这种？

翠仪

03:54:47

嗯嗯。

许涛

03:54:53

这种场景就是说，呃，它这个模型吧，它是个实时在更新的这种。这种应用就是。嗯。就是不是说训练出一个静态模型来，然后去使用了，然后就是就是就是它一直在，在，在动态的它自我迭代，自我更新的就这种。

翠仪

03:55:15

哦，现在应该没有这样的一个应用。你可能因为它的那个训练跟它的一个推理一般是分开的，就你可以不断的训练，但一般会训练到一定程度上再去更新这个推理模型。一般是采取这样的策略，就不可能是说你在用这个模型的时候，它就还在训练，那，那这种是没有办法去保证这个模型的一个效果的。

许涛

03:55:42

就是拿着那个就是就是组织组织好11批离线的数据，到了一定量级之后，然后再去训训练一下，然后呃再再拿到一些批数据，再去训练一下，是这样的，我他应该不是那种，比方说我是不是来一条数据，我训练一下，来一条数据，然后。

翠仪

03:55:58

一般一般不会，一般不会，一般都是累积到一定的数据量再去进行训练，然后再更新模型的。

许涛

03:55:59

就是批量的这种出就出。

翠仪

03:56:09

对对对对。就太少量的数据去进行这个训练，其实意义并不是很大。

george

03:56:27

呃，我问一下我们那个就是那个集群是自己用那个服务器搭的吗？还是说？用那个驱动云用他们那个租的服务器？

翠仪

03:56:37

啊，都是随便租的服务器啊，就只要你能确保这个服务之间是通信的，然后还有就是它是要这个开的是。

george

03:56:37

就是他。哦。

翠仪

03:56:47

呃，这个端口吗？

george

03:56:51

哦哦。

翠仪

03:56:51

就是6379这个端口是开的，它是人相互通信的，无论你是用这个腾讯云，阿里云还是驱动云。啊都可以，你甚至用自己电脑做一个had这节点，然后连接其他的云服务器都是OK的。

george

03:57:09

嗯，我说那个驱动云他不是有那个多机多卡吗？好像这个。进行一下那个那种环境测试，因为我接下来你想跑一个比较大的模型那个。就是要可能要出那种多机多卡的。

翠仪

03:57:26

但驱动云它本身就可以做一个那个显卡，虚拟化，它可以把多机多卡就虚拟成一个比较大的显卡。其实我觉得。跟这种分布式都差不多了。

george

03:57:41

呃，是你比方说。要想做一个论文上的改进要需要。大概三到四张AA100，然后就可能就需要两两个或者三个那个。

翠仪

03:57:49

哦。

george

03:57:53

机械然后每个上面有一到两个。A百度其实就这种。

翠仪

03:57:56

嗯。嗯，都可以啊，就是只要他们网络是通的。都可以加到这个集群里面去。

george

03:58:08

就是他他们这个。云租完之后需要自己装那些框架吗？还是一般都装好了？

翠仪

03:58:15

呃，他们的那个应该要自己装框架吧，所有的这个云服务器一般都是要去嗯，自己装框架的呀。

george

03:58:23

哦，就问一下那个驱动云一般多少钱111小时啊，一般这个。

翠仪

03:58:28

哦，这个我不太清楚，你要不就是去问一下，就是在驱动云上面看一下吧这个。

george

03:58:35

哦哦。

翠仪

03:58:37

呃，就就就可以找一下我们的客服，然后去跟或者是直接找一下驱动云那边的客服吧。

george

03:58:37

嗯，行。嗯，行，谢谢。

翠仪

03:58:58

Ok还有其他同学有问题吗？

王蓓

03:59:07

目前没有其他的同学举手提问。

翠仪

03:59:10

啊，好的，那我们的课程就呃到此结束了啊，就是说后续有问题的话，我们可以继续在群里面长沟通啊！

王蓓

03:59:21

好的，那同学们听一天课辛苦了，然后有问题在群里沟通，大家再见。

翠仪

03:59:27

嗯嗯嗯，好，谢谢大家，拜拜！再见！拜拜！