# Day1a
关键词

大语言模型

向量数据库

图神经网络

拉普拉斯矩阵

自然语言处理

知识图谱

图机器学习

问答机器人

随机游走

联通分量

PPT视角

09:52

喂喂喂喂听到吗？

风铃之音

10:02

嗯，可以听见可以可以。

PPT视角

10:02

喂。哎，好好行好！

PPT视角

11:02

喂，听了吗。啊，好的。呃，小慧你试一下录像吧，看看能不能顺利的录像。

风铃之音

11:13

嗯，现在是在录制中。

PPT视角

11:16

哦，行。这个录制现在只有一路吗？这个摄像机不录制是吧？哦那。

PPT视角

11:35

呃，小慧你看他整个视野是正常的吧，就没有什么障碍物啊！或者拍不清楚。

PPT视角

11:43

好。

PPT视角

12:55

可能做内侧比较好。坐在那一侧比较好。不是因为我站在这边，我就把你挡住了。

PPT视角

13:17

好9:30了，我们开始了吗小？

风铃之音

13:33

嗯，行，时间到了，忙的时候我们开始上课吧！

PPT视角

13:37

诶好，我们开始上课了啊！呃，非常欢迎大家参加我们今天这个特训营啊，那我们呃两个月前吧，大概就办过一次AIGC的特训营，那这一次的内容呢，呃，可以说是那一次的内容的延续，但是呢它本身呃在整个体系上面呢，也可以说跟。上一次的内容呢？是完全独立的。呃，在这几天里面呢，大概会给大家。呃，讲一些什么事情呢？那我把这个列在屏幕上面呢？

PPT视角

14:16

呃，第一个呢，我首先会给大家讲一讲关于这个图机学习啊，我们整个这个特训营的核心是要做怎么做这个垂直领域的专家机器人，我们要做产品的，我们要做具体的东西，但是呢，围绕着这个目的的话呢？我们需要有一些基础的知识。那这个第一个基础知识呢，就是图机器学习，那这个之前在念书曾经上面呢，曾经开过一个图机器学习的课程，但因为一些个人的原因呢，这个课程一直都没有能够延续下去，就这个停止更新很久了，那正好呢，借这个特训营的机会呢，可以把。一部分可以把它完全的讲完啊，因为这几年呢，这个图机器学习呢，也有很多的这个新的发展。有一些我们过去认为可能很有意思的东西，现在已经不再重要了。那有一些呢，可能是新的工具，比较巧妙的啊，那在这里的身边可以给大家讲一讲，那其实现在看起来的话呢，只要花一天时间啊，就可以给大家讲完了。那这个图记忆学习的话呢，自己看书。可能是一个不容易看懂的内容，因为基本上这个看完这个书以后都不知道他讲了什么，那我在这里呢，把这个整个体系给大家讲一遍的话呢，也可以给大家省去很多学习的时间啊，那这个是我们第一个方面的内容。

PPT视角

15:28

然后。呃，第一天的内容啊，第二天内容呢，会给大家讲这个知图谱。那在过去，在2018年前后吧，那这个一直都在办这个自然语言处理与基这个知识图谱的特训营啊，我们也讲过很多的知图谱，但是呢，在那些课程里面呢，基本上都是以讲这个自然语言处理为主。呃，这个知识图谱呢为辅。就知图谱的那个分量呢，可能有点不够，完了，我在这里呢，想花一天时间来把这个知识图谱从头到尾给大家讲扎实了。那我们在过去的课程里面呢，也很少讲这个知图谱怎么应用啊，比如说怎么用在这个实际里面啊，怎么去做这个问答。

PPT视角

16:09

这个机器人啊，那我们这一次都可以把它讲出啊，然后这一个图机器学习呢，其实有一个最主要的用途，最主要的目的就是用在知图谱里面。那这个经常有人问啊，这个图机学习学完以后我们有什么用处呢？那这个正好就一支图谱可以作为一个例子。那所以第一天跟第二天的内容呢，就可以贯穿起来了啊。

PPT视角

16:31

然后到了第三天的话呢，我们就讲一个比较新潮的事情了，就大语言模型，那这个我们在呃，上一次的特训营里面呢，也讲了不少，我们开了一个头吧，那这一次呢会讲的更仔细一些，比如说怎么微调啊，呃，这个。围绕它可以展开来做什么产品呢？那这个我们会讲的更加仔细，然后呢，有了大语言模型以后呢，我们在自然语言处理里面本来认为很难做的一些东西。呃，像这个知图谱啊，我提取这个实体或者关系或者把它补全，或者判断它里面这个实体有没有重复。啊，就我们称之为叫实体，对企业，这种东西本来都很难做的。

PPT视角

17:12

这个用了各种这个图机器学习的方法来做，但是呢，在大语言模型的帮助底下呢，我们可以很容易就可以把这些事情把它做好啊，那这个是我们课程的呃，另外一个要讲的内容，那当然讲这个大语言模型呢，我们除了做这个知图谱以外。那我们最终的应用呢，就是第四天的内容，那我们希望能够用我们之前讲的一系列的工具就知图谱。呃，大语言模型，还有这个图机学习，把这一堆东西这类技术混在一块儿，那我们可以开发出所谓的这个垂直领域的专家机器人出来啊，那我们这几个课程，这几天的课程呢，基本上就是这么一些内容。啊，那这个内容的分配呢，大家也比较清楚了。那我们最后的目的呢就是呃大家。至少每个人都能够拿着一个。这个垂直领域的专家机器人啊，只要你想做。那你是有能力做出来的，那当然我也不能说你这个在这四天里面就能做出来，但至少事后你是能做出来的，那你如果做不出来的话呢，你来找我，那我帮你这个告诉你用什么办法可以把它做出来啊。那当然如果你要求很高。比如说你要求这个精准度很高啊，那可能做起来呢就比较费时间了啊，那这个不容易的这个事情呢，是不容易的啊。呵呵。

PPT视角

18:35

那这个是我们一贯的这个惯例呢，就是讲讲这课程的上下游。那我们这个课程的上游课程的话呢，大概需要一些什么东西呢？大概需要一些什么东西呢？呃，第一个呢，首先你要懂python那这个是我一个基本的假设，那大家如果不懂python的话呢，听我这部分的内容应该没什么关系，因为我这部分内容基本上不涉及代码，那我只是会讲一些理论呢，或者一些讲解的东西。但下午何老师讲那个部分的话，基本上都是讲代码，那所以你要懂python，那不懂python的话呢，听起下午那个课来呢，就可能会比较辛苦。

PPT视角

19:11

啊，然后呢？我们这个呃。这个。深度学习框架了，用的是pytouch，那你应该还要懂一些pouch。那无论是python也好，或者这个pytouch也好了。啊，这个我们在念书曾经里面呢，都有一些呃，基本的课程，那如果大家在听完课以后觉得没听懂这些代码的话呢，那你可以向这个客服人员啊，申请一下，听这个念书曾经上面的python的基础课。

PPT视角

19:42

拍touch的基础课啊，那我们都有很完备的这个知识体系了，那你可以在听完以后听完这些基础课以后再回头再看我们这个课程的回放，那我们这个课程跟上次一样呢，也是有完整的录像。呃，完整的回放啊，包括这个腾讯官方的回放，还有我们自己本身在现场做的一个额外的一个备份的录像啊，都有，到时候都会给大家，那大家可以在没跟上节奏的情况之下呢，那你可以重新看这个回放。结合我们的基础课程啊，一起来看啊，那这样这个呃应该理解起来就不会有什么很大的问题了。

PPT视角

20:20

那至于这个理论部分的话呢，呃，那可能就是这个有一些深度学习的知识了，那在课前不是给了大家一些这个预习的材料嘛，对吧？有关于深度学习的，也有关于这个自然语言处理的，那两个预习的材料呢，都比较老了。那其实我也挺想更新这个材料，但是呢，一直都没有时间。那这个最近刚好这个出版社也有一个约在催，我这个约都约了好多年，可能约了有45年时间，黄老师你能不能出一本深度学习的教材？但是我一直都没有时间来做这个事情，那我想后面如果有一个合适的时间，其实我在这个认识曾经的学员群里面呢，也征求过大家的意见啊，我想讲一门呃，非常清晰的这个深度学习的课程。那我觉得怎么怎么怎么才叫能够清晰的学习这个深度学习呢？大概有四条线路吧。那第一个呢是数学的线路，然后第二个呢是网络结构的线路，第三个呢是代码的线路，第四个呢是应用案例的线路，那应用案例的话呢，就包括了。语言处理还有计算机视觉这两个东西。那我想把这些东西呢写成一套书。那能够媲美这个现在市面上比较流行的，比如说像这个呃，复旦大学的这个邱西鹏教授那个教材。

PPT视角

21:36

呃，那我认为那个教材本身呢还是有一些弱点。那我举个例子呢，比如说啊，我们用这个梯度下降算法来训练一个神经网络。然后呢，这个一开始的时候是全。全数据的这个梯度下降算法。他突然间就过渡到单个数据的。这个随机梯度下降算法，那这个随机梯度下降算法是什么原理呢？呃，为什么我们能这么做呢？那其实有很多东西都没有展开来讲。那还有一些这种训练的手段呢，像这个dropout啊啊，还有这个baclomo啊，这些啊，那我们在日常的训练里面呢，也是很惯用的。那其实他也没有怎么展开来讲这些事情啊，那我想把这些细节呢，把它补充一下，再完成一。

PPT视角

22:19

一个完整的一个呃课程。啊，以及这个写出相应的教材。那大家有兴趣的话呢，也可以欢迎大家来参与这个事情，就是大家不光是听课了，那也可以一起来参加这个教材的编写，那我可以把把大家列入到那个作者的名单里面，那当然这个主编只能写最多只能写四个人，就贡献最大的四个人能写上去。那其他的可以出现在这个前沿呢，等等的地方啊，那这个是我们目前想做的一件事情。

PPT视角

22:49

然后上完这门课以后呢，这个后面还会有一些后续的课程在这里可以预告一下，那比如说我们紧接着应该就在。这个月底吧，就7月底，8月初这个时候吧，有另外一门课程就锐分布式训练。那这个课程已经开了，这个欢迎大家报名，那这个主要是做怎么回事呢？那我们现在这个大语言模型都不是很大嘛，对吧！很消耗这个显卡动不动就可能20多G的显存或者几十G的显存，那其实你一张显卡呢是很难呃。达到有那么多的这个显存的。那你那这个就成为你的门槛了，那我们可以用这个锐来重写这个大语言模型。呃，它是一个一个基础设施。就是你装在这个机器上以后呢，你在你的机器上除了能够看到你自己的显卡以外。你还可以看到别的？在同一网络的这个服务器上的其他的显卡。然后你可以通过这个python里面调用ray这个库里面的一些功能，那你除了利用自己机上那个显卡以外，还可以利用别人的显卡，就说你只要对那个python的代码做稍微一点点的改写以后，那你就可以。

PPT视角

24:01

把这个模型。多机训练了啊，那这个呃你就可以用比较便宜的在这个呃，我们在电脑城都能买到的显卡，比如说像呃3090啊4090啊，这种1万块钱出头的显卡就能够完成那种78万块钱才能买到的这个。服务器能做的事情，那你就不需要去买什么a100啊H100啊啊这一类的这种服务器了。

PPT视角

24:28

那这个是一个课程啊，然后呢？后面呢，我们还会开关于这个aiops的特训营。呃，还有这个？L就是大圆模型的一些周边工具的使用，就是说你怎么用GPT啊，用什么什么包啊，等等啊，那这些课程呢，我们陆续都会开起来啊，欢迎大家到时候参加我们这些课程，那这个是上下游的事情。他不翻页了。

PPT视角

25:06

呃，给大家准备了一些礼物。那这个跟上次的特训营一样了，也也是一些书。那专门是给大家这个问问题的时候。呃，作为礼品来使用的，那因为我们是一个直播课程，那直播课程跟录播课程相比呢，有一个好处呢，就是老师能够直接听到你的疑问，然后呢并且加以回答。那这个为了奖励大家这个多互动啊，因为这个互动才能够反映出直播的价值，那不然的话呢，我还不如弄一个录像给大家看，像电视曾经以前做那就好了。

PPT视角

25:39

啊，那这个我在这里准备了一堆奖品，那这些书呢，有一些书好像之前送过，像这个智能运维，还有图深度学习，我记得好像上次送过，呃，那这个这一次送了一个是这个自己动手做聊天机器人。那这个聊天机器人跟我们这个课上讲的聊天机器人有点不一样，它还是基于比较老一点的技术的，没有我们现在讲的那么新，但是呢也可以给大家参考一下了，比如说这个基于知识图谱来做这个呃，问答机器人，那他在里面有讲啊。那你可以参考一下里面的内容。那还有这边这一本书呢，是关于这个智能化运维的。那刚才不是讲过我们下一个特训营将会讲那个AI O PS什么。那这个这两本智能运维的书呢，可以给大家参考啊，可以让你知道一下我们下一门。

PPT视角

26:25

特训营课程呢？大概会呃讲一些什么内容？啊，那这里是四本书，但实际上也不限于这四本呢，那理论上呢，你只要看中一本书。呃，这个价格大概跟这些书差不多的，那你就告诉一下客服，我需要那本书那也行，都无所谓啊，反正我们跟出版社都很熟啊，这个什么出版社的书你都可以告诉我啊，反正我买买了我就寄过去给你啊。

PPT视角

26:53

呃，这里顺便插两段小广告吧哈，那第一个呢，我们这些图书呢，有这个电子工业出版社赞助。啊，这个电子工业出版社的话呢，是我们念书曾经的一个合作伙伴。那同时我自己的一个身份呢，就是电子工业出版社的这个专家委员会的成员啊，那这个出版社的话呢，它都有一些出书的需求，那这个编辑呢，总是问我，唉，黄老师。那你最近有没有一些这个熟悉的朋友想出书的，呃，这个过来我们出版社出版，那在以往的话呢，我们也曾经向这个出版社推荐过很多我们的网友啊，比如说当年这个IT P的会员等等啊，也出过很多这种畅销书。呃，像这个梁庆斌老师给我们讲过那个四口优化的课程的梁斌老师，还有这个谭花园老师讲了我们很多这个oracle课程，那他们都写了很多关于这个数据库的书啊，比如说这个让oracle跑得更快啊，什么收获不止oracle啊。等等这样一堆书，那在出版社里面都是很有名的，都是非常畅销的书籍。那大家如果你个人有兴趣出书的话，那你可以找我，就是你觉得自己不是很好联系这个出版社，那我可以作为一个桥梁啊，那这个没有什么。

PPT视角

28:07

中介费啊，这类的概念就是我只是把你介绍给这个出版社的编辑认识，然后呢，后面的事情就是你们之间去交流了，包括搞费理也不用分给我，这个你你们你你们，你自己拿就行了啊，那只要只要出一本质量好的书。那这个就是我们共同的愿望。好，那这个是我们的第一个小广告吧？

PPT视角

28:29

那第二个呢？所以说今天我们这个直播活动呢，呃是由这个。呃，广东鼎泰企业管理有限公司赞助我们这个场地的支持，呃，还给了我们很多这种啊设备的支持，那在这里呢表示感谢！那这个鼎泰企业管理有限公司那底下有一个品牌叫弯顶会，那这个弯顶会在什么地方呢？就在这个，我们在群里面好像发了一个我们那个直播室的这个地址。啊，就是给这个同学可以到这里来听课的那个地址，就在广州的黄埔区的于租地铁站附近，有一个叫于租ai智慧港这个地方。那这个鱼珠这个地方呢，在地图上看呢，有点稍微有点偏僻啊，但是呢，其实这个。以后可能是广州的一个C。

PPT视角

29:19

呃，因为大家都知道了，这个超一线城市的，现在这个地皮就就跟黄金一样贵。那这种这个城市都在往外扩展，那这个余族呢是广州的一个发展的一个方向，那这个我这个经营这块场地的朋友呢，他就因为，呃，看到了有这么一个规划，那所以呢，预先在这里占了一块地啊，那他把这个整栋楼收回来了。

PPT视角

29:42

那这一栋楼的话呢，专门就是给这个ai公司，呃，这个***方面的企业进驻的，那我们这个名字呢叫做呃，于租ai智慧港啊，那我们念书曾经啊，就是我的公司，后来那也在参与到这个事情里面去。那大家中间如果有这个创业者，有企业想在这个广州找一块经营的场地，那可以落户到我们这个地方，那我们这个地方呢，可以提供这个租金的优惠啊，还有很多国家政策的优惠啊等等。那这个如果是有兴趣的朋友，这个欢迎来咨询我啊，那这个啊，我可以给你牵线解决这个事情。

PPT视角

30:22

好。那下面呢，我们就开始我们今天的这个内容啊，正式开始我们今天的内容。那我们首先呢，这个直击核心吧，就是大家可能比较关心的一些啊，面上的事情，因为呃我我负责的不是细节，细节是何老师负责，他在下午给大家讲，那我在上午的话呢，主要是讲一些理论知识或者一些面上的一些。呃，个人的感受，个人的看法，那大家可能呃，很想听到一些业界里面的这种，呃，比较资深的人，就或者说经验比较丰富，人对是现在发展的一些看法，那我在这里呢，可以跟大家探讨一下，那稍后呢也可以跟大家进行这个讨论啊。

PPT视角

31:01

那第一个呢，最近有一些这个说法。这个我忘了那个叫什么名字，好像叫朱啸虎。就是也是业界的一个名人吧，他在他的这个呃，微信的这个公众号里面啊，发了一些文章，他说这个check gp呢，对于这个创业者是不大友好的。因为它在不断的更新，功能太强大了，就变得那个创业者没什么可做的，而且呢，这个创业者。他可做的事情就是他，他创业者做的事情很可能会逐渐的被这个所蚕食。

PPT视角

31:35

那举个例子，比如说啊，会不会我们做那个？不是向大家介绍了那个方问的那个中药的那个呃软件嘛，对吧？那这个是我们自己研发的一个产品跟车GPT。没有关系，那也有同学问我，唉，这个黄老师，那那如果HP也能回答这个中药方面的问题，那是不是你这个产品就没用了？那比如说我在TPT里面输入这个病症是什么啊？然后呢要求他回答一个药方给我，那大概率他也能够回答出一个药方，这个药方看起来。对于不懂中药的人来说呢，看起来是像不像样。那这个。

PPT视角

32:09

还有一种畅想的，就是他今天回答的这个呃，可能不大好，可能不大符合我们的要求，但是这个缺GPT也在不断的进步当中，那你会不会他明年的一个新的版本就能够把这个事情搞定，它变成一个中医专家。然后你这个方问系统就完全没用，可以扔到一边去了，完全没有价值，那会不会发生这种事情？那说实话呢，这个问题呢，这个有点不好回答。

PPT视角

32:35

那这个后面这个事情呢，就不说了，谁能预测将来发生什么事情，对吧？那这个那如果说突然间有一个TPP升级到一个程度，所有人类能够回答的问题，他都能够精准的回答，那我们一下子所有的人都失去了价值。就就是好像一下子什么这个知识分子啊，这些老师啊，啊，这个专门负责解答别人问题的人啊，这个什么专家啊，那都都不如去了，那那你你这个时候就所有人都失业了，那这个时候我们唯一做的能做的一件事情就我们就去玩就好了。就是吃吃喝完了什么都不用做了，然后我们靠什么生存呢？那我们就就靠给这个ai打碎。就是说谁干的ai的，我们就收他的税，用这个税来养活我们啊，那就行了。那那时候我们就实现共产主义了，啥都不用干了，那给ai打税这个靠谱吗？我觉得很靠谱啊，那最简单的这个税机就是这个显卡。就说你买的什么显卡，这个算力是达到什么要求，那我就按你这个算力来进行这个收税。啊，那你的显卡越多，你的CPU越多GPU越多，那我收的是越多啊，因为你产生的这个AI功能呢，取代了很多人力的需求，而这些人要生存，这个国家是要给他们费用的，支持他们的生存的需要。

PPT视角

33:48

那所以像AI打碎这个是很正常的事情。那我相信那些掌握了AI能力的大脑呢，那也是乐于被打碎的啊，因为他们挣的钱是远远超过了这个税收的钱啊，那那这个以后的生活方式，可能就变成这样子了，就是少部分人。是生产ai调整ai发明ai的人啊，那这些人呢就拼命的干。然后呢，这个国家向他们打碎，那其他人的话呢？你只要乐呵呵的生活就行了，那你就过你的日子，就不用去忙什么事了，这个是我的一个看法。呃。

PPT视角

34:23

那对于前面一个问题，就是说这个怎么回答呢？是不是？

PPT视角

34:27

这个缺gpt啊，做的已经挺好了。呃，然后呢，这个创业者就没有什么机会了，那我就先回答现在这个问题吧啊。那从现在这个角度来看呢，这个checkgbt。并没有那么好啊，其实并没有那么好，那大家其实用过以后。啊，都可以发现了，那我们不是建立了很多这个AIGC爱好者群嘛，那也让这个一些这个网友呢，去测试的这个CG PT啊，包括GPT四G3.5在里面都有啊，那也给大家测试了。那大家测试完以后呢，发现这个CG PT在有一些事情上。还是挺弱智的，就是你如果正儿八经的问他一个知识性的问题啊，比如说啊，这个某某病啊，这个新冠啊，这个是怎么回事？这个用什么可以治疗，那这种在网络上？包括在维基百科上都能找到的这种资料的这种东西呢，他能够回答的很好。

PPT视角

35:19

但是一些这个。这个。呃，不是那么容易找到了资料，他就回答了不好了，那我就回答一下，刚才我说的有有一些同学问我啊，这个我们搞这个中医药的这种呃软件还有没有意义这么一个问题，那你可以在这个上面试一下。就是你问他一个病症，然后要求他开药，那你发现他开的药了，很多时候都是不靠谱的，包括他的断症，有很多时候呢都是不对的，就医生基本上看完这个这PPT出来的东西，看了两眼以后。那基本上就摇摇头了，这东西用不了，这个没啥用处，那目前呢是这么一个情况。那所以呢，在这里呢，就留给了我们一个空间啊，就check G。

PPT视角

36:04

他能够做得比较成功的事情呢是？呃，他能够很好的理解用户提问的意图，并且呢能够找到一些近似的材料。并且能够把它组织好一个你能够接受，你能够感叹，哎呀，这个人太厉害了，这个机器都那么厉害，能回答的那么利索了，他把这些资料把它组织起来，一个你很感叹的方式回送给你。但是这些资料是不是精准的呢？那这个就真的未必了。那比如说我问他最简单的问题，我说你告诉我梁山坡。

PPT视角

36:38

108好汉有哪一些？基本上就没有答对的，没有一个大语言模型，能答对的，不信大家现在你马上可以试一下。就包括GP四也好GT3.5也好啊，这个包括我们的文艺言啊，各种大模型也好，唉，就回答出来就是让人感叹一下这个机器就是机器，这真的就是一个弱智，就回答得乱七八糟，这个是永远的棒。那这个有一天如果能够有一个大模型。

PPT视角

37:03

大语言模型能够回答好，梁山坡108好汉有哪些人这个问题的时候，那我在担心方问都不迟啊，现在还不用担心<笑声>，那这个由于他有这样的弱点呢，就给了我们一个创业的机会。那大家都知道了，在一些领域里面呢？比如说医疗级的领域啊，像我们那个方问这个产品。这个包括。这个方问产品所涉及的领域。还有这个金融级的领域啊，比如说你是在银行里面要出售一些基金的产品，要出现出售一些理财的产品，但用户呢总是会有一些呃，关于这些理财产品风险啊，这个理财产品的收益啊等等的一些问题来问你。呃，这个银行现在都很少人了，你看你进到一家银行里面，通常就是有一个这个呃大堂经理把你带到一台自动柜员机面前，让你自己去操作，而不是到柜台里面去操作，你让银行养那么多的人来替你做这个理财的服务呢？也不大现实。

PPT视角

38:04

那有一个很基本的一个呃。需求的就是，那我能不能用一个机器人来回答你这些机械性的不断重复的问题呢？那无论是医疗级也好，这个金融级也好，那这个要求都是很高的，那这个医疗级你一旦回答错了问题，那可能会。误导这个病人或者误导这个医生，那最严重的后果就可能会死人。那这个死人会不会追究你的责任呢？那这个就不敢说了啊，那所以这个敢于开一家这种，呃，医疗方面的公司，或者开一家医院，并且承当承担这个相应的责任，比如说你当他的股东。当他的法人代表，那这个是很需要勇气的。那一旦出了什么问题的话呢？自己可能会承担不少的责任啊，那还有金融级的问题，那你的这个回答可能被别人保留下来的截了图了。那当当以后出现这个什么这个这个这个债券违约啊，呃，这个基金的这个收入缩水啊，并不像你说那么高啊，那那种情况呢，就有可能会拿出来来刁难你，比如说你以前回答错了一个问题，呃，把一个这个收益。比较低的基金啊，这个回答错了，这个这个语气没注意啊，一下子回答说这个收入很高，然后呢这个呃说话的方式也不对路，那这个以后呢，很可能就会被别人抓住，作为把柄啊，那这个时候就很麻烦。

PPT视角

39:21

那另外其他的一些，比如说涉及到这个军事的用途啊，这个精密工业的用途。比如说我们维修这个飞机啊，维修一些这个精密的很昂贵的一些机械，一些仪器。那我需要有一个专家机器人帮我查这些维修的手册，回答我的问题，那你想象一下，如果维修飞机的时候，这个回答是错误的。这个专家回答给你的这个事情是错误的，导致你发生了一个维修上的一个瑕疵，然后这个后来就。可能会造成很严重的后果。那这种后果呢，很明显呢，也没有人能承担，那这个军事上呢，就更加不得了了。那如果你这个判断完全错误的话呢，那完这个军事上的失败，是可以改变一个国家的命运。那所以在这些场景底下呢？我们对这个精准率的要求都是很高的。那现在这个<外文>呢，它只能够达到闲聊级的水平，你要让他做到做到那么高的精准度，那是根本没有可能是根本你就不要奢望了，他连量身或108好汉都没答对。你还能奢望其他什么东西？

PPT视角

40:27

那你一涉及到列表的问题，就是说，呃，你要列出888什么啊？比如说这个用户问你能不能列出你这个银行现在卖的所有基金以及它的这个呃，收益率给我看一下啊，你发现881开始列的这个十个还是对的，那后面。出来的那些都是错的，而且这个错误还还会不断的循环跟重复，那这个是大语言模型固有的一个办法，是吧？那所以遇到这种问题的话呢，这个。这个GPT就有点不灵光。那这里呢，就给我们一个创业的机会，就是我们可以做精准度更高的这个模型，那我们可以对这个gpt做一做一些改进啊，来提高它的在行业级里面的水平啊。嗯。那我认为呢是这样的。呃。

PPT视角

41:15

对于浅层的应用。就简单的，比如说像这个陪聊天啊，或者这个翻译一下文档啊，这个修饰一下文稿啊，这一种情况的话呢，那我只需要通常真正浅层的应用呢，只需要通过这个。对大语言做一个简单的微调啊，就可以搞定了。那在这种场景里面呢，我们最好是能够做出规模，而不是只是做一个小产品，那一个小产品呢，因为你这个门槛很低，你可以做，其实别人也可以做，很快很快，你这个地方就会充满了这个竞争者。变成一片红海，因为你挣了大钱的消息呢，它一下就会被传播出去，然后所有人马上就知道了，那大家都一起来做这些事情啊，那就变成你就没有。没有钱，挣了就没有什么门槛。

PPT视角

42:05

那你要做出规模，那这个规模怎么做呢？一个靠你的销售能力了，那就说啊，我能找到很多很多类似的需求，比如说黄老师想现在想做一个浅层的产品啊，我专门就是给这个公司啊，这个开会的时候生成这个。一摘要或者可能写一些这个日常的汇报文档等等，我就做这么一个产品。那我现在通过这个培训呢，把这个消息告诉给大家所有的这个。呃，听众所有的选啊，那大家这个100号人那都知道，马上知道黄老师能做这个事情，那以后你遇到这样的需求，那你自己又觉得挣这个钱太累了，你也不想养一个IT，或者说做一些自己并不擅长的事情，那你可以把这个机会介绍给黄老师。那那我这个这个规模不就一下做上去了，对吧？啊，那这个是一条路径，那还有另外一条路径呢，就是你最好能够把这个产品做成一个通用的东西，就不是，不是说每一个这个需求者过来找你，你都要给他。

PPT视角

43:03

定制做一个项目，你做成一个通用的一个东西，那这个用户。只需要把这个通用的东西输入一些自己个性化的材料，就能够满足它个性化的要求。那这个举个例子啊，那以前很久很久以前那个连带呢是并没有数据库这个东西的。每一个这个应用系统都有自己的一个数据库，那这个数据库呢，很可能是把数据写在文本文件里面，或者写在一些别的格式的文件里面。然后呢，它，呃，也有一些代码来去访问这些简单的数据文件，来读取它里面的数据。并且获得结果很久以前的时候是这样，但是后来这个oracle公司。就掌握了这种。就觉得这个方向是一个可以努力的方向，它把这种数据就是储存数据。

PPT视角

43:52

这个查询数据这个东西呢，做成了一种通用性的一个产品，那就是oracle数据库了。那结果呢？这个oracle数据库无论你是什么行业的，你是银行也好，医院也好，呃，这个军事单位也好，这个政府也好，那你总而言之，你只要把这个数据都放在我这个数据库里面，你可以用一种标准的操作界面跟方法。

PPT视角

44:13

就是C语言你可以对它来进行查询。啊，这，那那于是呢，我本来一个很个性化的，要针对一个场景才能实现的这种数据管理的产品呢？那我就通过这个oracle数据库，这么一个通用的东西就能够实现。的规模性呢啊，那这个也是产品的通用性呢，也是规模化的一个呃，一个需要。那这个是。

PPT视角

44:37

呃，另外一个对于这种浅层改良的一个机会啊，那这个是一个方向，就是往规模化产品化的方向来发展，那还有一个方向呢，就是。呃。这个。对于这个精准度比较高的这种场景。那我们可能会要有一些这种相应的手段，因为这些场景呢，往往他们都是付得起价钱，那比如说像医院。这大家都知道，现在医院呢，这个。经费肯定。是比较充裕的，这当然这个有很多医院都很穷啊，说这个这个生意不太好了，什么看病的病人越来越少啊，等等，但医院肯定也比在现在这个经济环境底下呢，肯定也比其他的企业呃，会有更多的这种更充裕的经费。啊，还有呢，这个金融？啊，这个银行啊，这个基金啊，这些也不用说了啊，那他们都很会乐于。去做这么一些这种基于hgp的这个回答精准度比较高的项目，呃，也愿意花大价钱来做，那为什么他们愿意花这个价钱呢？因为你做了这些东西以后，可以改善它的效率，减少他的人手，比如说他可能不需要那么多客服了。这个医生看病看的更快了，不需要那么多医生了啊，他他其实付完这些代价以后呢，他可以获得这个更低的成本啊，那所以对于他来说呢，其实这个也是一个可以接受的事情。啊，那所以这个精准化呢是另外一个方方向啊，就如果你是做浅层的东西的话呢，你是做这个规模跟通用性。

PPT视角

46:14

那还有另外一个发展的方向呢，是精准化。那我们这个课程呢，主要就是针对精准化这个方向啊，来展开我们的叙述。好，那我们就直接我们这个课程的主题啊！那我们先说一说，我们要做一个垂直领域的专家机器人，大概有哪一些常用的路径？那我这里总结了这个几几条路径，大家看一看啊！或者说你心目中认为还有？其他的一些路径，那我们也可以讨论一下，等一下就可以讨论。

PPT视角

46:47

那第一个呢？我可以拿车GPT进行微调。那这个tpp本身就带有接口啊，等一下我再展开来叙述。那第二个呢，我用了，不是我拿一个开源的，像这个清华glam之类的大语言模型拿它来进行微调。呃，或者定制化。啊这个。我们也可以做成这个垂直领域机器人。那这个另外呢，我们还可以用这个landtrain，还有向量数据库。那向量数据库呢？今天下午何老师就会给大家讲。呃，然后呢，这个<外文>的话呢，我们在第三天的下午，何老师也会给大家讲啊，在这个课程里面都会涉及到，那这个是什么意思？等一下我再给大家解释啊，那另外呢就是我们这个课程主打的一个这个主题就是知识图谱加这个大语言模型。那这个当然也是一个新的探索方向了，好像我也有其他的一些朋友提了出来，但是呢，我相信他们都没有做过具体的探索，那这个我们是做过具体的探索。啊，那这个呃应该说我在这里介绍给大家的这些知识跟经验呢，都是可以供大家借鉴。好，那我这里就刚才所讲的几条路径呢，给大家展开一下啊，那我们也可以等一下进行讨论。

PPT视角

48:00

那第一个呢是这个GPT微调？那大家可能呃都用过T GPT那有一些呢，可能是通过我们念书曾经提供那个钉钉群。那个聊天机器人那个接口来聊天。啊，有一些呢，可能是直接上这个官网，就说你如果有能力翻墙，有能力去教。用一个信用卡去注册用户啊，让他扣费的话呢？那你可以直接访问这个官网。那官网上面呢，大家其实都可以看到了。它是有一个训练的入口。那这个训练的入口呢，你可以上传一些所谓的。个人材料。那个人材料就是指你这个专业领域，你这个垂直领域里面的一些。

PPT视角

48:46

东西啊，比如说一些文本，你通常要整理成问答对的形式。然后呢，你上传到他那个？呃，训练接口里面它就会对这个checkg进行微调。然后呢，这个事情也是要收费的，但这个费用其实并不是太高。啊，并不是太高，那大家可以上去查一查它这个费用好像那个anton的费用呢，还没有那个问GP问题那么高。那这里详细的关于这个价格，这个收费标准呢，大家可以看一下知乎上面的这个。

PPT视角

49:18

这个链接。那这个链接里面有一些介绍啊，那这个我这里就不细讲了。那这种方法的话呢，你上传完语料上去训练以后呢，那这个GPT大概就可以成为你的一个专属领域的机器人，就这么简单，你把这个材料送送上去就行了。

PPT视角

49:38

啊，那这种方法有什么优缺点呢？那第一个我首先讲一讲它的优点吧，啊，那第一个。它不是一个本地模型，你只是调用了别人的接口。所以呢，这个部署特别简单，这没什么部署，那你就不调用一个接口不就完了吗？你连服务器都不需要，服务器都别人。

PPT视角

49:55

然后呢，这个GPT四是目前最优秀的这个大语言模型。那没有一个模型能够跟它相比，那其他模型呢？像这个文心一言啊。还有这个清华这些的在gd四面前呢，就都显得像个弱智一样。啊，那这个gp四是目前最优秀的。目前还看不出有什么大语言模型，有能力追上他。就是说你如果我把这个GPT是冻结在今天的水平，过一年以后有大语言模型追上它，那这个我不奇怪。但是在大家一起发展的情况底下。那你想想人家的资源，那是多丰富，那第一个他有人，这些人一直都在做这个事情，那他们继续做这个事情，肯定他的经验要比你丰富很多，那你现在才开始做，你怎么追得上人家已经做了几年时间，那个人，对吧！那第二个呢，他有大量的资金。有大量的这种设备硬件的支持。那这些呢都是我们国内的公司缺乏。

PPT视角

50:56

那这个第一个资金？啊，现在国内这个经济形势的话呢，大概这个企业也没有本领，拿出多少的这个什么几亿美金的这类的这个研发的经费的这个太难了，这个啊，那第二个呢？由于这个中美关系不大好，这个我们有很多设备可能不一定能买到。那你看那个老黄啊，那天一下子捧出一台这个超级显卡服务器。那个吓死人，那个显存的是144T？那这个是什么概念呢？我都很难想象这个是什么概念。那那你说我们弄不出这么一个服务器出来，我们怎么跟别人玩呢？那这个确实有点难呢。所以呢，你你要说我们这个国家里面这个，呃，要整体上追上这个。这个gttc的这个水平呢？这个估计是有点难度，但是你在一个特殊的领域，比如说在中文这一块儿。呃，或者说在中文的一个特别的领域里面能够超过它，那我觉得这个是有可能的，你如果要在通用领域里面来超过它呢，那这个实在是太艰难了啊，那这个是我的一个判断吧。

PPT视角

51:57

然后呢，这个现在唯一能看到的能够超过这个GPT四的一个可能啊，我认为从技术路线来看。很可能那个google的那个。呃，现在他们也在做大语言模型嘛，他用alpha购物那个强化学习的方法。希望这个能追上这个gp四，我觉得这个是有可能。大家可以注意一下这条线路的那个发展，那个这技术的发展。就说他的这个强化学习慢玩，强化学习是玩的非常溜的。那以前这个openai跟电相比的就跟稳腿跟牛腿相比一样，就openai只能做一些很搞笑的，很简单的小小软件叫做openai dream gym。然后呢在里面做一下这个什么这个小车动态平衡啊之类的。一些很搞笑的一些应用。我们那时候都可能看不起这个，觉得它跟那个阿尔法呢，简直就是没有办法相比啊，根本就没有办法相比，但但现在呢，它做出GPT四以后呢，我们对这个印象就改观了。那看一看这个定的反击啊，能不能够这个。把它又重新压回去啊，那这个google跟？这个openai跟微软的竞争呢，这个是长期性，那我们以后还会陆续看到的这样一个事情啊。好，那这个是它的优点，然后缺点是什么呢？

PPT视角

53:17

那第一个首先它使用比较困难。那由于这个。呃。中国。不在openai的服务区域里面。那所以这个中国的ip是没有办法能够访问这个。Openai的服务的这个不是什么防火墙的问题，就就算我们国家的防火墙你是通了，你能访问过去，但人家都不给你通。那所以你只能够在美国搞一个服务器，呃，做一下所谓翻墙的工作，那这个翻墙搞笑的可能你不一定是翻中国的墙，你可能连美国的墙也要翻啊，那你要做这么一个事情，那这里已经拦倒很多这个创业者了，那你说让你去这个。呃，搞一个账号在美国租一个服务器，你要找一个美国人的身份，要找一张美国的信用卡啊，那这个对于很多人来说呢，这个事情就已经不容易了。

PPT视角

54:08

然后呢，你如果用一些什么虚拟的信用卡啊这种，那现在有很多在淘宝上卖这个虚拟信用卡的，就说，呃，他会给你一个卡号，可以伪装成一个像美国的卡，那你只要给这个淘宝人交钱的话呢，他也能够像美国的信用卡一样扣费。但是呢，这些虚拟信用卡呢，很容易被查出来。那这个openai呢？这一段时间一直都在查，那一查到的话呢，他马上就把你封号了。所以你也很难说得清楚你这个虚拟信用卡了能不能一直这么使用下去啊？好，那这个是它的第一个问题，然后第二个问题呢，这个GPT是很贵的。

PPT视角

54:44

那我我们的那个不是做了一个钉钉机器人来回答问题嘛，那上面我统计过那个gpd四的那个费用。呃，大概每回答一条问题的话呢，平均成本是大概三毛多左右。那现在外面有很多这种做这种翻墙生意的这个人。他会向别人收啊，五毛钱一条问题等等，那大家不要以为这个五毛钱很贵，他真的没挣你多少钱，因为像我们这一种随便玩一玩的，就已经三毛一条问题就要三毛多了，他挣你的也就挣了一条问题一毛多，那如果遇到有一个狠人。他长篇大论的输入几10万字去问问题，那你也不能阻止他说他一出来又又是几10万字，那你这个成本一下就高了啊。那所以这里面的这个成本的话呢，虽然短时间来看这个几毛钱什么。不是很大的问题，但是你长时间下来，比如说你可能有成千上万的人在那里问问题的话呢，那这个还是很可观。

PPT视角

55:43

那因为我们现在一直都在免费支持大家去测试这个GPT四还有缺GPT，所以我对这个事情呢，是深有感受，就是说你从一个人的角度来看呢，不算什么就不就一条问题，几毛钱的事情，但是一旦你变成成千上万的人的时候呢？那这个就会对你变成一个很大的负担。而且这个负担还是不可预测的，因为你不知道这个用户突然间什么时候他就会问你一堆问题啊哈，那这个不好说。

PPT视角

56:11

好，然后呢，这个GPT的话呢，还有一个问题呢，就是对于垂直领域它的精准性呢是不够的，那这个模型本身也不透明，也不可控。因为这个是放在人家服务器上，你根本都不知道它是什么构造，也没有任何论文透透透露出任何信息，即使你知道它的结构的信息也没有这个接口允许你去调整。所以你只能够说我这个资料上传上去训练了。那它是什么效果？那就是什么效果了，我还能干嘛呢？我什么都不能干是吧？那所以呢，你其实发现你微调了以后呢，并不容易。调整到一个合适的火候，就是说它不能够不能大在很大的概率底下，大概可能有90%的概率，它并不能达到你的要求。比如说我希望说我上传了一堆这个几百本的中医书籍。那他就能够很利索的回答我中医的问题，我希望的是这么一个效果，对吧？但事实上呢，你看到的结果呢，可能会让你非常的失望啊，其实他并不能做到这一点，而且呢，有时候你这个训练呢，还很可能是过火了。

PPT视角

57:19

就是这个gpt是被你这么一训练完以后，它反而变得有点不正常。啊，就是就是就是他连医学的问题回答不好，原先能回答的好的问题都变得回答不好了。那这个就不符合我们的预要求了，但然而这一切呢，你又不能控制。你没有办法控制，那所以这个这个事情呢，目前来看还不是那么靠谱的一件事情啊，那这个是第一个。

PPT视角

57:46

呃，这里有一些同学呢，曾经问过我这个ppt本身是按照那个多少多少个toc来收费的。我好难明白这个<外文>到底是啥意思呢？那我一个汉字到底是多少个talk呢？那我这里前面啊，这里下面这里给了一个链接，大家可以访问这个链接。那这个链接呢就解释了token是到底是什么东西，它跟汉字有什么关系？那这里要看不同的接口，不同接口呢，所采用的那个token。这个数量是不一样的。那如果你用的是那个达文西这个接口，那这个接口呢，大概是一个比较高端的接口吧。那他一个talk呢大概？呃，一个汉字呢，大概相当于2.23个token。然后在这个GPT四？后来cp3.5这个tpp这个情况底下就普通的接口的情况底下api的情况底下，那大概消耗了是1.12个token啊，那这个大概大概是这么一个数字。好，那这个我就不展开讲了，大家可以自己。

PPT视角

58:51

啊，去看啊，所以去看我就不展开讲。

PPT视角

58:54

好，那这个是第一条路线。那第二条技术路线呢？就是我找一个开源的大语言模型下载回来放在我自家的服务器上面慢慢调。那这个现在用的比较常见的就是那个清华的那个大语言模型就清华gom。那现在最新的版本呢是gom第二版六B。啊，据说现在已经很厉害了，在某些方面甚至已经超过了GPT四。我听完以后呢，我就笑了，我上去一测试就问他一个老问题，梁山破108好汉是谁？结果他不出预料的又回答错了。<笑声>那所以从这个角度来看呢，他其实没有什么进步，那大家不相信的话，你可以上去问问他，在我们那个钉钉群里面不是。之前有一个清华GOM机器人嘛，对吧，那个机器人现在已经升级到这个版本，升级到V26B版，那大家可以上去自己来试一试啊。那这个。

PPT视角

59:50

这个模型呢是开源的，那我们在后面的这个下午那个课程里面。啊，这个何老师会引导大家去下载这个模型下来部署，并且对它进行微调。那它的微调呢？跟那个？呃，缺GPT差不多，它本身直接就提供了一个。程序就它本身那个下载回来的东西里面就包含了一些代码，那其中就有微调代码。那你可以把你的问题把你的这个语料就说你这个领域知识都整理输入整理成输入输出队。就说啊，你问什么我答什么，然后你把这堆材料扒一下，塞给他。那他就大概也能回答出。

PPT视角

01:00:30

呃，一些这个垂直领域的问题。好，那这个是一种做法。那这种做法呢？它的优缺点是什么呢？它的优点是第一个，它是本地部署部署连接的。那这个对于有一些这个单单位企业很重要，那我举个例子吧，就是医院。那这个医院呢，它对这个整个信息安全的管控呢，是非常严格的，你不能指望一个医院去连到外面，甚至去翻墙，连去一个美国的服务器，你想都不要想啊。就没有人，这个信息科里面是没有人敢干这个事情的，一旦发生了什么黑客入侵事件，或者什么病人吸引信息泄露事件的话呢？没有人能扛得起这个责任。你别说给他什么tgbt呢，你给他黄金钻石他都不敢要。啊，那这个就别想了啊，那所以呢那个。呃。

PPT视角

01:01:24

能够本地部署了，这个是很重要的，就是我这个服务器放在医院里面，不要放在外面啊，它本地可以连接啊，那这个是很重要的。然后呢，它也没有这个点击成本。那因为是你自己的服务器，你自己安装了软件，那你点击多少次，这个都不用。呃，这个收什么费用？而且呢，你还可以对这个模型还有它相关的软件进行大幅度的模改。那比如说我对这个模型彻彻底底的不满意，那我可以用一大堆语料把这个模型彻底的训练一下，就是就是像它这个原始训练的样子，不是那个微调训练是原始训练的样子，把它重新再整一遍。那那这个你都是可以做到了，你爱干什么就干什么啊，那这个是它的优点，然后缺点是什么呢？那第一个你如果需要本地部署的话，那你就不用付那个点击成本了，但是你也需要算力成本，那这个算力成本呢？包括你部署了服务器的成本，还有你放置的一个机房的成本，还有维护人员的成本，还有这个电力开销的成本。那这些公司得算？那如果这个这么一算下来的话呢，恐怕你也不会比这个hpd便宜多少啊。

PPT视角

01:02:38

还有呢，就是这个微调呢也是一样的。你虽然对他的可控性更强。但是呢，这个大语言模型本身的复杂性呢，决定了你并不能够呃非常简单的去可以把它控制好。那所以呢，你如果想达到好的效果的话呢，也是这个非常消耗时间的一件事情，那大部分非专业人士，就除非你是专门做这个行业的。

PPT视角

01:03:06

那非专业人士去做完这个事情以后呢？那大部分都这个结果都不是很好，那我在朋友圈里面呢，也看到很多朋友的公司，他们都纷纷去做了一些自己垂直领域的。这个产品啊，比如说有公司做这个环保专家机器人啊，专门回答一些环保类的法规的问题啊，还有一些关于这个环保设备的问题。那我看到他这个做生，虽然宣称他做完以后就没有下文了，好像也没有说有多少人用啊，这个也没有说他后续怎么发展这个产品。那我想我猜测可能大概这个效果可能不是很好吧啊！啊，那这里呢，顺便也。他一段这个我们自己做了一些东西。那我们现在呢？准备做一个叫做胎基地的产品啊，大家可以听一听我们的这个创业计划，那也算创业吧，虽然我们这个公司已经成立了差不多有20年的时间也算创业吧，那这个大家可以听听我们的创业计划，然后呢？来看看你有没有。这个。

PPT视角

01:04:12

类似的计划，或者说我们有没有可以合作的地方啊？呃，第一个第一个思路是这样的，就是。呃，比如说。现在呢？呃，我们可能要做一个。Idc做一个house？那我要我这个<外文>呢，有点像这个<外文>差不多。啊，就跟那个差不多。那就是。大家比如说每一个企业abcd啊，每一个企业都有一个大语言模型，这大语言模型微调好以后变成一个垂直领域的专家机器人模型。那就放在我的服务器里面。那这个这里边的结构是什么样子了？那这里。那是不是我们有很多服务器，每个服务器我都装一个大语言模型给它？然后呢，给这些企业来使用。分分别这个训练好以后给abc企业分别来使用。然后呢，我们就。呃，有一个机房，那这个机房的话呢，会提供这个服务器提供显卡。提供这个电力啊，这个维护人员等等支持。那这个就是普通的idc服务了，这个一点都没有什么稀奇。

PPT视角

01:05:21

但是呢，这个我们这里有一个疑问就是。每每一台服务器装一个大语言模型，是否值得这么一个问题？那这里也顺便解答大家一个疑问啊，刚才黄老师你不是说？这个gpt可以微调嘛，对吧，那我这里我就奇了怪了。这个缺gpt很很明显，是一个非常庞大的模型，对吧？那你微调好以后，你那么多用户，全世界有几个亿的用户？那我就不说几个亿吧，我说里面比如说有100万人，每个人都想把自己的那个tgbt微调一下。那我是不是就会微调出？100万个CP出来，那我要得要多少服务器才能放这么多C。一个很明显，一台服务器放不下，我假设你五台服务器能放得下吧，那100万人是不是就需要500万台服务器呢？那你收我的那一丁点的这个一个token零点零几分钱的这么一个费用。你你能够承受你这个服务器的成本吗？那这个是一个？

PPT视角

01:06:29

灵魂拷问了，那这个TPP T很明显呢，不大像采用这种架构。那我们呢？打算做这么一个服务啊，打算做这么一个服务，那我们就用一台。或者若干台？部署了这个分布式训练以及发布系统。或者这个deepspeed啊，这类的这种服务器。作为我们的这个算力的一个基座。

PPT视角

01:06:53

然后呢，我们在上面部署一个大语言模型，那举个例子啊，我部署一个清华的这个呃什么GOM六B之类的。那这个微调的话呢？那说明是微，首先它是微调了，那对这个模型的改动是很少的。那我们在这里在采用一种技术。那这个微调呢，并不改变这个大语言模型本身的那些全职的一些系数的数据，我并不改变这些。我改变的仅仅只是这个微调层上的东西，就说我在这个大语言模型里面呢。冻结它里面训练好的绝大部分的参数。我仅仅只是留一小块一点点是可以变动的。然后呢，我们的微调训练的就是这一小块的参数。那每个人每个企业我都可以给你一小块参数，可以改的。啊，每个人可能都不一样，但是我都可以给你。那这一，这一小块所占用的资源是很少的。那举个例子，比如说它可能只有几十兆到几百兆这么大，它并没有达到这个几十个G几百个G这么一个级别啊。那所以我一个模型上面呢，可以贴上很多个微条层。那每一个微调层呢，就可以为不同的这个企业来服务了，那我的这个资源的底座呢？是比如说啊，我们拥有一个机房，那我们在这里的话，比如说像。哦哦。现在这个创业园。啊那这里。呃，将来的话呢，呃有这么一种合作的，可能就是跟这个电力公司。

PPT视角

01:08:31

合作。那现在国家对电力公司呢，都要求有一定的这种环保店的要求，就是说。呃，它要有多少的电力是来源于那种清洁能源？啊，就是不产生污染的，不是什么核电厂啊，或者说这个火力发电厂生产的这种，那特别是比如说一些太阳能发电啊，或者风力发电啊等等啊，这样的一些资源，而且呢，它对这个。呃，这个算力使用这些资源是有一些要求，有一些kpi的。那比如说国家有一个政策就说啊，你这个机房，你如果在这个呃这个电力公司里面使用了它多少的清洁的能源。那我国家就可以把你这部部分的电费把它给补贴了，你就可以不用支付这个电费了，那我们大概也能谈成这么一个事情，那就变成了我们这个机房的话呢，我们的电力成本呢，可能是比较低的。

PPT视角

01:09:23

那我们在上面呢，可以放一堆服务器。然后呢，就可以支撑起这个服务。那大家将来还可以畅想有这么一个合作啊，大家不是现在呃，学习我们这个课程感到比较吃力的，就是啊，我不光要买显卡，还要买一台很强的服务器，还要很耗电啊，那这个成本很高。那以后呢，你很可能只需要买显卡就可以了，那你把你的显卡寄给黄老师。那黄老师就把把把你的显卡插到我们的服务器里面，那你首先第一个你可以远程去使用这些显卡，那这个成本呢，肯定比你自己去呃，买一台服务器，自己交电费，那这个肯定要低很多。而且还有一个好处呢，就是当你的这个显卡自己不用的时候，你还可以租给别人用，我们可以把大家的算力把它综合起来。

PPT视角

01:10:08

呃，利用这个后面我们讲的一个软件就是那个猎户座X，大家现在不是？租那个驱驱动云的那个计算那个算力资源嘛，对吧，那个算力资源的这个提供者就驱动云本身呢就是这个猎户做X这个软件的这个发明者。那我们也用他们类似的这个手段装上这个软件以后呢，那我们也可以向别人卖算力了。那虽然这个算力听起来比较便宜啊，这个一个gpu多少多少钱一小时才一块钱都不到，还有几毛钱，但是呢，这个少数怕长算呢，这广东人有这么一句话。啊，就是你如果这个日积月累，一天有24小时，一年有365天，这么算下来的话呢，其实这个费用还是很可观的，应该你这个显卡的成本呢，大概在。不短不长的时间，可能很短的时间里面就能够收获来大部分，那我们将来呢，可以一起合伙来做这个生意，那大家只需要买显卡就够了，然后你把你的显卡寄给我，其他我帮你搞定<笑声>那现在先做个铺垫吧啊。那大家可以先听听我们这个模式。哦。

PPT视角

01:11:17

那这个是第一个，我们可能要做的一些事情啊！然后呢。呃。

PPT视角

01:11:25

我这里又可以给大家讲一讲这个一些垂直领域的机器人可以怎么做啊，刚才讲了我们的前一种方法，第一种方法。就是第一，第二种方法就是分别是微调这个trygbt。还有微调这个清华？Gom六B这类的大圆模型，那我们再看第三种办法啊！

PPT视角

01:11:44

那第三种办法呢就是。用这个向量数据库以及embedding的这个方法。那这个向量数据库是什么东西呢？那我们听说过有图数据库，关系型数据库，但好像从来没有听说过向量数据库到底是啥东西呢？啊，其实也没有什么别的，这向量数据库呢，它的主要用途呢，就是存放海量的向量。比如说。你不是有很多这个语料吗？有很多问答对吗？对吧？我把你的每一条语料，每一个问答对里面的问题或者答案都变成一个向量。那这个变成向量的过程了，我们叫embed。那叫嵌入，中文叫嵌入，那这个嵌入怎么做的，那等一下再给大家讲啊。

PPT视角

01:12:33

好，我们变成向量以后呢，就把这种可能有几10万条甚至到几11条的这样的向量。放到一个数据库里面。然后我们做什么事情呢？当我们遇到一个问题的时候呢，我们也把它变成向量。然后呢，我就算在我这个向量数据库里面有哪一条向量？跟这个问题对应的向量是最接近。那这个最接近的话呢，无非就是计算距离或者夹角于弦，看它们相不相似计算这两个东西。

PPT视角

01:13:09

好，然后呢，这个我们我们这个向量数据库能够解决的核心问题就是它进行这个比较速度特别快。那因为如果你用关系型数据库来做这个事情的话呢，那一看就知道这个是一个全表扫描。你必须要把全部的这个向量全部拿出来，跟你给的这个向量做一次计算，那你才可以知道哪一个是最接近的，对吧！那这个全表扫描的速度呢是很慢。

PPT视角

01:13:38

但是向量数据库呢，就有一个技巧呢，就是。它通过某一些算法。可以使到这个向量。相似性的计算特别快。那特别是这个批量的这个向量。相似性的计算。那我们其实以前呢，呃，我们公司也研发过一个类似的产品，当时用的技术呢是用那个显卡来介入。就用显卡来实现这个批量比较更快，这个事情。啊，但后来呢，我们也没有往这个方向里面再深入的研究下去。那但是呢，这个有一些人是搞了这个事情啊，那就做错了，现在的这个向量数据库了啊。

PPT视角

01:14:19

好，那这里顺便给大家解释一下embedding embedding什么意思呢？就是把场景里面客观存在的东西，那举个例子，比如说像文字，音频，这个图片，视频等等。那通过一些这个？呃，算法。把它映射为一个低为欧式空间里面的向量。那因为文字是制服是吧，音频是一些信号。这些东西你是不能够直接进行计算的，你说对字符串计算，那你能算什么东西呢？那你必须要把它变成这个数学上的这个向量以后才可以计算。那所以<外文>第一个实现的目标呢，是可计算化。那第二个呢？原先你的相似性。我只要指定，那他在<外文>以后还可以保留。那我举个例子，比如说啊，中国首都还有北京这两个本来是同义词，或者说近义词，对吧，那你从文字上呢，要判断他们的近似性是很难做到的。就判断两个词的近似性，这个是文字上是很难判断。那你只能够通过一些硬性定义的这种对应关系，那你才能够这个，呃，发现这些同义词，但这个工作量太大了。

PPT视角

01:15:33

那有一个方法呢，就是我们首先用一种合适的<外文>的方法呢，把这个词汇映射到这个欧式空间里面。那中国首都？我后面这里有一张图啊！比如说我们这里有四个词啊，北京，悉尼，中国首都，还有这个英国首都。那这里有一个欧式空间？那我有一个<外文>的映射，分别把这些词映射到空间的一个点或者一个向量那里去。那我们可以看到呢，在合适的映射底下呢，北京跟中国首都都会映射到一个点，或者至少是距离很近的点。那于是呢，我就知道这两个词的意义非常接近。然后悉尼还有英国首都呢，被映射到比较远的点，那我们就知道它们之间是不相似的。那这个嵌入呢，可以保持这种相似性。啊，就使到我们可以把这个一些很难计算相似性的东西，那我们能够计算出它的相似性出来啊。那我们等一下呢，会讲到。那这个就是专门把这个文字。呃，做到。好，那下面呢，我们看一看怎么通过这个。

PPT视角

01:16:48

向量数据库来实现这个问答机器人啊，首先给一个简单的版本。那这个图呢，稍微有点潦草，但是应该还是能。啊，看得清楚。那比如说我们现在呢，已经准备好了很多标准的问题跟标准的答案。就准备好一堆啊，那假设这个你是一个金融服务的，这个想做一个金融服务的机器人，那这个顾客常问的问题你是知道的，就高频率问的问题你是知道的，然后这个答案怎么回答你也是知道的。那你可以整理出语料出来啊！

PPT视角

01:17:24

那现在有一个问题呢，就是用户有时候问这些问题的时候呢？不像你想象中那么乖。他不一定十十足十的，按照你的这个想法来问你。那它可能在次序上或者用词上会发生一些改变。那这个有我们想到了一个办法，就是说啊，这个用户问的一条问题，我看看跟那些问题里面哪个是最相似的，我就把这个答案推送给他，那不就完事了嘛，对吧。但由于这个用户并不是你想象中那么乖。你判断他问的问题跟你存的问题是不是相似？这个就是一个很大的难题了？你如果用字符串匹配啊，这种啊，比如说这个什么最大字符串匹配啊，这种东西呢，一般来说都会得到很差的结果，这个效果不大好。好，那在有了这个向量数据库这个情况底下呢，我们可以用来做一个呃，这个更好用的一个问答机器人。那大概是这样子，我们首先把这个问题通过某种embedding算法，比如说我们等一下讲的那个叫<外文>的算法。把它映射到这个向量数据库里面。成为它里面的向量。那这个向量数据库的内容就是11条一条的向量啊，11个一个的向量。

PPT视角

01:18:41

好，然后呢，当有人来问问我们问题的时候啊，我有一个问题。那这个时候我们会怎么做呢？那我们首先把它的问题也用embedding把它变成一个向量。那我再再把这个向量呢，在这个向量数据库里面，叭叭叭叭叭进行查询。看看它跟哪个比较相似，那相似，就是距离近。或者这个夹角很小。好，那我查到了，假如我查到了。那我就把相关的答案推给对方啊，那就解决了这个问题了，对吧？好，那如果我查不到，比如说它每一个的效果都不大理想，那这个时候怎么办呢？那这个一个办法，比如说你可以让人工接管。就说哎呀，这个问题，这个机器人回答不了了，就转给人工处理吧。那这个机器人其实也解决了，因为因为有百分之八九十的问题的，大概率都是能够找到答案的。那所以这个他也能解放人力，但是你还是需要有一个人进行兜底，就说这个实在回报不了了，我就转给人去处理。那还有一个方法呢，就是啊，这个回答不了的问题呢，这个机器人就告诉对方说，哎呀，说你我不知道这个问题怎么回答。然后呢，他把这个问题记下来。那记下来以后呢，我们？定期可以在后台里面翻阅一下有哪些问题用户问的？

PPT视角

01:20:02

那我这个机器人实在回答不了了，那你就给它重新填上答案。然后呢？丰富我们的这个数据库的内容就行。那这些问题呢，有一些呢是可能是确实是我们的。我们没有没有这个安排这个有关的数据，那另外有一些呢，很可能是呃这个。这个客户开玩笑了，他乱问你一些问题，就问问一些跟你这个场景根本不相干的问题，那这个机这个时候这个机器人回答说这个问题回答不了了，这个也是很正常，这个客户本来也不想你回答他只是考验一下你而已啊。好。那这个何老师今天下午来会给大家讲这个事情就是怎么样用这个向量数据库实现最简单的这个机器人啊。

PPT视角

01:20:48

好，下面呢，我们再讲一个复杂的版本。那这个就有点意思了，大家可以先看看这个图啊！先看这个图。

PPT视角

01:21:09

好，那这个情况呢是这样的，现在比如说我要做一个金融银行金融服务的集权回答各种我的理财产品跟基金。的一些呃，这种有关的问题。

PPT视角

01:21:23

那我怎么做这个事情呢？那第一个。首先，关于我这些基金还有理财产品的信息呢，往往都是一些pdf文件的存在，对吧？那只有这里我就能拿到一叠，可能有几百份的这个pdf文件。那我现在的目的就是我要根据这些pdf文件呢，做一个问答机器人，这个问答机器人能代替我。利用这个pdf里面提供的知识跟这个客户进行交互，那于是就取代了我的一些人力的成本。

PPT视角

01:21:56

好，那我怎么做这个事情呢？第一件事情首先把pdf文件里面的文本抽取出来，变成文本文件。那这个事情就不用解释了，因为比如说在linux系统里面呢，它就有一个。功能或者说一个应用程序叫PDF to TXT。然后呢，在很多这种各种软件的库里面呢，也有类似的库可以找到啊，那这个怎么变我就不说了，就PDF文件变成文本文件，那这个不是问题的。好，那接下来这个事情呢，就是由何老师准备给大家讲的那个landtrain。

PPT视角

01:22:31

来做的事情。那第一步呢，我首先把这个文本。拆分。那我们有一个库，里面有一个函数，它自动可以把这个文本的内容拆分成一小段一小段的。就每一段叫做一个。那大概可能从几十个字到几百个字之间啊，就一个一个创。好，然后呢，我把每一个创。通过这个线路，通过<外文>的方式。把它映射为向量。把它放到向量数据库里面。那这里的embedding呢？很可能就不能用那种简单的embedding，可能要用复杂一点的embedding。那比如说像那个？就利用穿来实现的那种。就是在他的预训练任务里面，是包括了问答。是一种预训练内容的那种任务。他做的<外文>呢，在我们这个场景里面就比较有效，那像这个<外文>这一种比较简单的这种<外文>，在我们这个场景里面的有效性呢，就不大强啊。

PPT视角

01:23:36

好。那我们先做好了这么一个限量数据库，那大家记住呢，这个就是把pdf里面的一段一段的内容放到我们的这个限量数据库里面了啊。好，那现在来了啊啊！有一个人来了啊，我有一个问题。

PPT视角

01:23:51

那你回答我吧！那这个我们怎么做呢？

PPT视角

01:23:54

我们首先把这个人的问题也做一个embedding。做相同的<外文>变成一个向量。那同样的，那我也在这个向量数据库里面进行搜索。那这次搜索的话呢，按照相似性，我们找出最相似的前N条内容。就是比如说我要搜索五条内容啊，那你每一个都算一下距离或者夹角余弦，我找出五个出来。然后做什么事情呢？那剩下的事情我不能直接回答他，因为我不知道五个里面有哪个是答案，甚至这些是不是答案我其实也不知道，但是呢，大概率这个人提的问题的答案就在这五个窗口里面。

PPT视角

01:24:37

那我下面做一个事情呢？我把那这个五个创。这个。拿出来。送去给大语言模型。我做一个prom是这样的。我把这个人的问题的原文。跟我这五个创的原文。全部包裹起来，送给大语言模型，那大概就是。呃，根据下面五条资料啊，8885个创列出来。请你帮我回答下列问题，再把用户的问题写出来。好，那这个大语言收到你这个信息以后呢，它就会按照你的指示，根据你给的五个创也结合这个大语言模型本身自由的能力来去回答用户的这个问题。那这个就是返回的，就是一个回答，质量很高。跟这个用户的意图非常匹配，而且回答的框架也非常符合我们的预期啊，这么一个答案，那这个答案呢，我们就送给用户。回送给用。这个是更复杂的一种。这个应用的方式啊，那我们后面呢也可以来试一试这种方式。那这里这个方法关键之处是什么呢？那第一个呢？首先你这个场景里面用的embedding算法的话呢？呃。那这个必须要核实，刚才说过了，你不能够用那种太简单，你最好是用那种用问答对来训练过的。这种系统比如说像bgpd之类啊，那应该都是用问答，是曾经作为这个预训练任务之一来训练过的。

PPT视角

01:26:21

好，然后呢。

PPT视角

01:26:23

呃，还有另外一个关键的问题呢，就是。比较合适的比较函数的设计。那所谓比较函数，刚才我说了算距离或者算这个夹角于弦，那这些都是简单的比较。那还有一些可能是更复杂的，比较算法啊，那我这里就不11展开了，我把这个内容留给留给何老师给大家讲。那关键是你这些比较函数能不能够在这个向量数据库本身能够实现呢？那大部分的这种稍微有一定复杂程度的这种向量数据库呢，本身既提供了这种<外文>的算法，也提供了大量的比较的方法啊，那你选择合适的就行。

PPT视角

01:27:10

然后。呃。

PPT视角

01:27:13

还有一种方法呢，就是我们在这堂课，我们在这门课里面要讲。就是以支持图谱作为核心。然后呢，这个再结合大语言模型来做这个问答，那这个大家也可以想象了，那大概就是你提了一条问题给我。我从你这个问题里面呢，抽取出一些关键的实体。我看看这个知识图谱里面有哪些实体是跟你这个问题里面的实体？是相匹配的，比如说啊，你问。这个。呃，你能不能给一些关于姚明的资讯给我？那我举个例子，这样子啊，那当然这个LAOM本身也能回答这个问题，但用了知识图谱呢会更加。这个呃精准一些，比如说我们提取出姚明这个实体，那我就在知识图谱里面找一找有没有姚明这个节点。那有这个节点里面的话呢，我可以把它周围相关的资料收集一圈。而且还可以不止收集一圈，还可以收集两圈。就说我把相邻的那些实体或者是跟你那个。最短路径的距离是二以内的那些实体啊，甚至可能是325以内的一些实体有关的，所有的资料全部都拿出来。我所有这些资料打成一个包，我就送给这个L啊，我说。你收到这些资料以后啊，这个人问这个问题，你看怎么回答？那他就会自动的把这些资料整理成一个答案，然后回送给这个用户。

PPT视角

01:28:42

啊，那这个用这个这个这个这种方式整理出来的答案呢，肯定就会比你用向量数据库去用那种呃，夹角余弦啊，或者距离匹配出来那些资料肯定要靠谱。因为它的那个金币程度会更高，这个第一个它是知识图谱是经过人工整理的。那第二个呢，它排除了那种嵌入算法的不靠谱。第三个呢，它也排除了比较算法的不靠谱。那最后剩下的这只是这个这个LLM的封装而已啊。那所以这个呃，从这个原理上来看呢，它大概率可以做出一个。

PPT视角

01:29:19

更精准的一个问答机器人啊！好，那我们今天上午的这个。呃，第一部分内容啊！我就讲述到这里啊，下面大家看看有没有什么问题啊？这个。好的。呃，我的问题是这样的，刚才说的那个复杂的聊天机器人的算法里面要把那个问题的embedding跟答案的embedding做一个比较。那问题的答案的？按道理来说，它应该很不类似的，应该是比较不出来任何结果才对，因为它本身就不同的东西。行，我回答一下这个问题啊，这个是现场朋友问的问题。呃，刚才我说过了，这里选择了这个嵌入算法呢就很重要了，就说你大概率你不能够用<外文>这种简单的嵌入算法。呃，你必须要用那种用问答？形式预训练过的模型的那种embedding那在那种embedding底下呢，它问题跟答案呢，就会基本上映射在一起。是这样啊。啊。对。就是就是这个预训练模型本身就是用问答作为它的任务来训练过的。啊，要用这种方法来点才行。有有就像我们之前讲过那个<外文>那些就可以做到这一点啊。

PPT视角

01:30:53

好，这个在线的。这个听众看看有没有什么问题啊，小慧整理一下。

王蓓

01:30:59

大家可以举手提问了。

PPT视角

01:31:01

啊，有有奖品啊，大家可以问啊！

王蓓

01:31:01

点击软件中的举手。

PPT视角

01:31:04

有奖品。

王蓓

01:31:08

点击举手后，我们会赋予权限。

PPT视角

01:31:26

喂。听到了道理啦喂！是不是没点开麦呀？喂喂。

许涛

01:31:44

喂喂。

PPT视角

01:31:45

哎，听到了，听到了。

许涛

01:31:47

那刚才那个？

金文娟

01:31:48

好。

许涛

01:31:49

我想问一个问题就是。嗯，刚才我呃提到一个就是说。

金文娟

01:31:51

我的。

许涛

01:31:54

呃。

许涛

01:31:55

要保持一个一句话或者是保持它一个向量之间的关系。

金文娟

01:31:57

高。

许涛

01:32:01

那个。

PPT视角

01:32:03

能听得清楚吗？

许涛

01:32:03

这个。

PPT视角

01:32:06

听不清楚，不知道是什么原因。

许涛

01:32:06

喂。

王蓓

01:32:09

对许涛同学的声音那边有点空旷，您要不大声一点？

许涛

01:32:09

哦。

PPT视角

01:32:17

喂。

许涛

01:32:18

错了错了。

PPT视角

01:32:25

王贝的发言我是能听得清楚的呀！哦哦哦，对方的麦有问题哦哦！啊那打字转？

王蓓

01:32:33

呃，许涛同学您稍等一下，然后让金文娟同学先说，然后您调试一下麦克再说啊！

PPT视角

01:32:36

嗯。

PPT视角

01:32:45

喂喂。

张彤

01:32:46

喂，黄老师。

PPT视角

01:32:48

诶，你好诶，很清楚。

张彤

01:32:49

呃呃，然后我问一下，就是我们现在做机器人问答的话，一般是根据词的相似性。

PPT视角

01:32:50

好。

张彤

01:32:56

那假如我们会出现这种口语问答的话，比如很多它是有疑问句啊，反问句啊，像这种的话它会怎么判定？然后我们之前在做的时候经常会把一些摸呀或者一些问号的那些。

张彤

01:33:11

剔除掉。会有歧义。

PPT视角

01:33:13

对。就是停停用词可以把它剔除掉，免得它影响这个语义。

张彤

01:33:14

对。

PPT视角

01:33:18

这个在一些比较简单的这种<外文>的方法的时候呢，经常都要做这样的事情，比如说用那个<外文>的时候呢，很很可能要把一些代词啊，这种就没有什么实际意义的，不影响这个语句意识的这些词，先把它排除掉，然后再算它的这个。

张彤

01:33:18

对。

PPT视角

01:33:34

呃，这个句子的这个embedding啊，那确实要做这样一件事情，那我们觉得现在觉得更好的方法呢，就是。呃，有了这个大圆模型以后呢，我们可以把这个比较的范围拓宽一些。就说我们原先是需要拿出这个<外文>以后，并且匹配效果最好的那个作为答案嘛，对吧，但实际上是不是最好那个就是答案呢？我们也没有什么信心，那现在有了大语言模型以后呢，我可以拿出匹配最好的五个或者十个甚至几十个。我都列出来。我我就告诉这个大圆模型，我说啊，这个人有这样的问题，你就根据我给你的这么一些资料，你看看怎么回答他<笑声>，那他就会回答了，那他就帮我做完了那个我很头痛的事情。那所以这个大语言模型呢，对于我们的帮助还是蛮大的。是吧。

张彤

01:34:22

呃，也就是说还是依靠你后边的大语文模型，不断的去猜它的答案。

PPT视角

01:34:24

嗯。对对对，没错，就是这个聊天机器人这个事情。

PPT视角

01:34:31

有了大语言模型以后呢，这个难度的这个呃下降了很很很多，这个下降了可能有两个量级。

张彤

01:34:32

啊好，谢谢黄老师！

PPT视角

01:34:41

好，不客气嗯，好的。

张彤

01:34:41

嗯。

PPT视角

01:34:43

啊，下一位听众。

PPT视角

01:35:01

呃，也可以做到，因为这个大模型本身也有这样的服务，它本身也提供了这样的接口。呃，想想必也是可以的。

王蓓

01:35:18

同学可以提问了。

金文娟

01:35:21

哎，老师你好！

PPT视角

01:35:22

这个他怎么训练过了，这个我们都不知道，其实就是我们只能够用这个效果来说话了，就这个这个，比如说你GPT四怎么训练的，我们都不知道他有没有用问答任务来训练过呢？我猜想应该是有的。

王蓓

01:35:38

好了，黄老师，金文娟同学要提问了。

PPT视角

01:35:41

啊，我这里再回答一个问题呢。啊，为什么喂喂？喂，请您再说一遍。

金文娟

01:35:49

啊老师你好！

王蓓

01:35:50

嗯。

PPT视角

01:35:53

诶，听到。

金文娟

01:35:55

嗯，就是我想问就是我有<外文>很多数据转到这个，嗯，向量库里面之后，然后我我像我想问一个问题的时候，它是需要把这个向量库里所有的数据都取出来，然后做一个。

PPT视角

01:36:02

对。

金文娟

01:36:10

比较进行一个输出嘛？

PPT视角

01:36:13

对，没错，他他也也有某种缩影。就说这个销量数据库有自己一个特别的缩影，那你建立了这个缩影以后呢，它匹配的速度是特别快。它不一定是每一个都算一次，就是在有特定缩影的情况底下呢，它可以快速能查找到，那这个要看具体的向量数据库跟你建的缩影是什么了。啊，就就好像在关系数据库里面，你建了那个B加速缩影以后你就不需要全表扫描，跟这个道理是一样啊。

金文娟

01:36:42

嗯。我们现在有个困惑，就是可能还还是用这个向量库，还是定位和之前传统的这种数据库一样的这种使用方式是不是这种？市里不对呀！

PPT视角

01:36:55

呃呃。呃，什么来着？我没没听清楚这个前面那个那个那那半句话。

金文娟

01:37:00

就是，就是我们现在用这个向量数据库，它其实是可能会更侧重于做一些隐形的一些计计算一些推理，但是我们可能没有这种相应的这种想法或者思维。

PPT视角

01:37:05

嗯。

金文娟

01:37:16

在用它的时候还是用这种呃，用传统数据库的这种方式去用它，就是这种方式是不是不对呀？

PPT视角

01:37:23

呃，向量向量数据库能做的事情呢？其实传统数据库它也能做，那只是做的比较慢而已。做的比较慢而已啊！

金文娟

01:37:34

嗯，就是感觉用起来可能笑感就是和自己预期的那种效果不是很好。

PPT视角

01:37:37

嗯。呃，就比我打个比方啊，比如说你上图数据库，那其实图数据库能做的事情呢，这个关系型数据库也是一样可以做的，它只是没那么直观，没那么直接，它不能够直接支持一些这个图上常见的算法，比如说算最短距离啊，它这个它算不出来。它它这个算起来也不是不叫算不出来，算起来很费劲，那不像你那里调用一个函数就能搞定就不方便。哦。

金文娟

01:38:10

有没有一些优化的算法？或者说一些思路，让我们更好的去。只用这个向？

PPT视角

01:38:20

呃，有的这个向量数据库现在还是一个比较粗糙的东西。很多时候做一些这个精密的产品的时候呢，很可能要赤膊上阵，对这个向量数据库一些底层的算法进行改造。那这个也是我们搞技术人的那个技术含量所在的啊？

金文娟

01:38:37

就是咱咱咱这次课会讲这一块嘛？

PPT视角

01:38:41

呃，可以向何老师要求一下他负责这一个部部分的内容。

金文娟

01:38:46

哦，好的。

PPT视角

01:38:49

还有没有下一个问题啊？

王蓓

01:38:54

有的徐文峰同学可以提问。

PPT视角

01:38:56

哎，好，谢谢。

徐文峰

01:38:57

哦，黄老师你好！

PPT视角

01:38:58

啊，你好。

徐文峰

01:38:59

诶，就是我们现在遇到一个问题，就是我们的语料篇幅都基本上比较长，是那种大几千字甚至上万字的，就是我们想要把语料切分成那个文本块，然后构建只是向量库，但是如果用那种就是。几百字，一切几百字，一切的方式就会发现。他这个就是在做这个向量知识搜索的时候吧，他这个语料的上下文的逻辑就丢失了，还有一些语义也丢失了。就是搜索结果反而不好。

徐文峰

01:39:30

就是如果用模型，就是现在有一些算法去做呃文本分割的话吧。发现这个嗯，就是模型的话，它这个输入的偷根长度基本上都在512。就没有很长，就这种问题，我想问问你应该怎么解决？

PPT视角

01:39:48

呃，那你心目中有没有一个合适的长度呢？就是你认为这个长度是比较合适的有吗？那你可以把这个切割工具稍微改一下，有一些可能是设定参数，有一些呢，可能是改源代码。那你看看哪种方式改了以后呢，可以使到你得到。呃，符合你心目中要求那个长度。

徐文峰

01:40:08

嗯，因为找的模型它基本上都是继续的做的。它的那个向量维度就是512。

徐文峰

01:40:16

然后就比较小。

PPT视角

01:40:16

这些都应该可以调参的，我觉得就是这个嵌入向量的维度，还有你的刚才说那个创的那个大小那个CS的都可以调调整一些参数。

PPT视角

01:40:27

你可以试一下什么参数效果是最好的。

徐文峰

01:40:31

哦，明白。

PPT视角

01:40:33

还有问题吗啊？那没有问题呢，我们先休息十分钟，十分钟之后我们。

许涛

01:40:45

啊。

PPT视角

01:40:48

再继续啊。

王蓓

01:40:48

有的有有。

许涛

01:40:51

为什么。

王蓓

01:40:51

再接受一下王波同学的提问吧，还是许啊，许涛同学的调试好了。

PPT视角

01:40:53

哦，好行。

王波

01:40:56

喂。

王蓓

01:40:56

嗯。

PPT视角

01:40:56

哎，好的好的，那继续吧！

许涛

01:40:59

帽子。

王波

01:41:00

喂，你好。

许涛

01:41:00

黄老师。

PPT视角

01:41:01

诶，你好。

许涛

01:41:02

啊，就是刚才我信号不好，就是我想问一下是不是有这种可能就是刚才说的那个向量化的这个过程。比如可以用这个或者是什么大语言模型训练过的这种方式，它是不是？

PPT视角

01:41:09

对。对，没错是是。

许涛

01:41:16

啊，是不是有这种可能性？就是比如说它本来两个现实世界当中的两个这个东西吧，它其实。呃，在咱们正常认为中，它是两个比较相似或者接近的结果，经过向量化以后，最后。就最后可能就是他俩反而就。不是那么的相似了，是不有有这种可能性？

PPT视角

01:41:35

呃，这个完全是有可能的，我举个例子，比如说两个同义词啊，像刚才说中国首都跟北京。如果你从这个字符串。

许涛

01:41:42

啊。

PPT视角

01:41:44

匹配这个角度来看，那它们完全就是两码事，这就不叫相似啊，对吧。但是你从那个<外文>这个算法的角度来看呢，它们就是比较相似的。那这个<外文>是怎么判断的呢？那它是通过这个。呃，判断有没有相似的，上下文就说这两个词出现的那个语言环境是不是经经常会上下文相同来判断这两个词的这个相似的程度，那从这个角度来看呢，它们就是很相似的。那所以呢，不排除。有一些东西。从一种算法的角度来看，他们是很相似的，但是换了另外一种算法的话呢，他们就很不相似了。

PPT视角

01:42:22

那这里另外一个典型的例子就是刚才那个问答对。那我说的是我们要挑选一种合适的embedding使到这个问题跟它对应的答案。要落在同一点或者同一点附近，他们是相似的。但是呢，如果你挑的这个算法不对呢，那很明显这个算这个问题算出来的，<外文>跟这个答案算出来<外文>很可能差异就很大了。啊，那这个是有可能。

许涛

01:42:50

嗯，所以呢，我我就想就是，呃，在具体一种这种项链化的过程，那肯定还是要考虑，可能咱们怎么去定义这个想要达到的一个目标，它这个语境里边是一个怎么怎么才算相似的一个定义。

PPT视角

01:42:50

嗯，好的。

王波

01:42:55

嗯。

PPT视角

01:43:06

呃，能不能再再说一遍一下，刚才没突然间中间停一下。

许涛

01:43:10

就是比方说怎么才算真正的相似，是不是得得要有这么一个定义，才才才能去后边这种他那个相似性的算法呀，或者是去处理。

PPT视角

01:43:18

就说应该选择这个什么embedding，有没有一个通用的标准是吧？呃，我觉得这里可能没有什么很常用的标准的，那第一个可能是看别人是怎么做过这个事情，就学习别人，那第二个呢，就靠自己的经验呢。

PPT视角

01:43:34

就你做做做的多了，那自然就是知道在这个场景里面用什么的方法呢？会比较合适，嗯。

许涛

01:43:35

但是从这个方式本身的话，咱们可能处理这手段上可能也就是呃，就比如说你，你说的那个。长度啊，或者夹角的那种也就这种方式了是吧？

PPT视角

01:43:51

对对，这这个是比较容易理解的，这个方法就算距离跟算夹角是大家听起来最容易理解的，那还有一些复杂的算法，那看看何老师下午会不会给大家讲？

许涛

01:43:51

或者是其他的有什么？

许涛

01:44:04

啊行行行好！那那没问题了？

PPT视角

01:44:07

嗯，好的，谢谢！还有问题吗啊？大家还有问题吗？

王波

01:44:11

谁。

PPT视角

01:44:13

诶，你好听到。

王波

01:44:13

哎，你好，有个问题啊，有两个问题，我第一个问题就是刚才您讲那个rain就是那个分布式计算那个。

PPT视角

01:44:16

哎。嗯。

PPT视角

01:44:22

呃。

王波

01:44:22

那个框架。

PPT视角

01:44:23

是哪个计算？

王波

01:44:25

就是那个有多个多几多机多机GPU那个对对。

PPT视角

01:44:28

啊瑞瑞啊，我们有一个课程，大概11个月后开课。

王波

01:44:31

对对，我就有个问题啊，就是说如果用K八S是不是也能实实现相同的东西啊？

PPT视角

01:44:33

嗯。呃，理论上应该是可以的，但是。好像有有怎么说呢，我我感觉这个架构有点太复杂了，就有点。有有点稍微贬义的词，就有点矫情，感觉就K八S的它不是一个必需的东西。

王波

01:44:50

哦哦哦。

PPT视角

01:44:53

啊，因为在大圆模型这个场景底下呢，通常一台机的硬件的潜力就已经被挖光了，就没有必要再用K八S了来挖它的潜力了。

PPT视角

01:45:04

啊，就我觉得不大，需要，好像把这两个工具结合在一块儿，但理论上呢，他们是可以结合在一块儿。

王波

01:45:10

啊，我的意思就是说用K八S把我因为我这个一个机器可能就一个11张卡嘛，我就是普通消费级的台式机，然后K八S把它连成一个集群，利用这个。

PPT视角

01:45:17

对。不。对。

王波

01:45:21

每一个GPU这样的话和那个rain的效率是不是rain的会高一点？

PPT视角

01:45:23

对。这里面还可能需要用到我讲的那个猎户做X就那个on X那个软件。它可以把显卡虚拟化。就说把一张大的显卡可以画系成若干张小的显卡，那这个时候用K八死了，可能效率会更高一些。那不然的话，你有一个这个进这个这个线程独占的那个显卡，那其他线程就用不了了，那个kbs也是没啥用的啊。

王波

01:45:51

哦哦。

王波

01:45:52

我其实啊好，那好我。就是我我就想着，因为我我我我假如有八台机器，八个GPU，我就想用一个容器能把这八个GPU都都看到这样。

PPT视角

01:46:02

嗯。

王波

01:46:03

原理上可以可行吗？

PPT视角

01:46:05

这个是可行的呀，这个本本身就能看到，本身就能看到。

王波

01:46:07

哦，就是都能用到是吧？好，我第二个问题就是跟这个嵌嵌入式这个语料有有有点<外文>关系，就是说您刚才说那个就是那个问答。

PPT视角

01:46:10

对对。嗯。

王波

01:46:19

就是问问题的嵌入和那个回答的嵌入是不是这个语语料库越大，这个训练的越好啊？

PPT视角

01:46:19

对。对。对。嗯，这个可能都得要看场景了，有时候未必是这样的，我举个例子，比如说我们那个中医的产品，那我们一开始呢，用很大的这个中医的语料来训练它。

PPT视角

01:46:39

结果直接把他学傻了，就是各种门派的中医都有，那他中中取这个博取众家之长以后，就变成不会开药了，就开出来，就是乱七八糟的东西，完全失去了这个风格。所以说他这个语料也不能说是越大越好啊。

王波

01:46:49

啊，我是这样理解的，就是说那比如说传统的<外文>是吧，它那个或者是<外文>就是我的大语言模型，大语言模型，如果和我这个就是问答，就是你这个这个流程图上面前面这个语料的，如果是和。

PPT视角

01:46:54

嗯。

王波

01:47:10

大语言模型比如说我问题和我这个。用的语料是完全一样的，那么我这个回答是更好一点。就是问答的它那个嵌入式和我这个<外文>预训练的那些语料是一样的，这样问答是不是更好一点？

PPT视角

01:47:27

这个就是那个<外文>的问题了，就说你如果你提出的这个问题，刚好跟一个<外文>比较相信呢，他的回答质量就会比较高啊，跟这个应该有点类似吧。

王波

01:47:38

对，就是说因为我大语言模型也是也是需要学学术嵌入式这些，呃，就是这些这个embedding嘛。然后如果我这个embedding和我这个问答对的这个embedding。

PPT视角

01:47:41

嗯。

王波

01:47:51

这哈就是是出出出于同样的embedding这样的。这样的回答会不会更好一点？

PPT视角

01:47:59

他们本来就应该是同一种人，不然这个比较就失去了意义啊。

王波

01:48:02

啊啊。哦，就是说你这个<外文>能把我这个，呃，比如说这个<外文>里面这个embedding用用上是吧，是这意思吗？还是说要重新训练我，我用的时候。

PPT视角

01:48:16

<外文>的话是你自己去选的，你不一定要用这个别人指定给你的，就是你可以用最简单的像<外文>之类的那里复杂一点的，可以用<外文>用什么？就有很多这种大语言模型啊，都已经提供了这种embedding的功能，包括gp本身也有这个功能啊。

王波

01:48:34

哦。

王波

01:48:35

那。也就是说在我那个问题。呃，问题这个embedding和回答问题这个这个这边红的这边embedding的话。呃，相当于是在我的这个呃LG RMI这个imbedding的基础上再再再训练一次，产生新的嵌入吗？

PPT视角

01:48:54

不用训练了呀，这里都已经全部训练好了，就问题通过一个<外文>函数变成一个向量啊，那我就在这个向量数据库里面进行这种距离的计算或者相似性的计算。

王波

01:48:57

啊。哦哦。

PPT视角

01:49:06

那计算出来以后，我就可以挑出有几个向量是比较匹配的，对吧？那我就把这个向量对应的语料拿出来。

王波

01:49:11

啊。

PPT视角

01:49:14

结合这个问题，一起扔给那个大语言模型，让大语言模型来整理出一个合适的回答框架，再去回答这个用户。

PPT视角

01:49:23

啊是做这么一件事情？

王波

01:49:23

噢，那那那我那大概大概我明白为什么刚才说那个语料库越大不一定越好，就是因为我用的这个嵌入这个向量本身是某种开源的这个大大圆模型本身的向量是是这意思吗？

PPT视角

01:49:25

嗯。嗯。

PPT视角

01:49:39

有可能是这个样子呃呃这语料呢要要适应自己的这个场景的那个需要也不一定是越大越好嗯。

王波

01:49:41

哦哦。就不用带训练？啊，因为那个我觉得大学模型它语料库比较大嘛，训练那个场景的那个嵌入式向量可能相似度啊什么的可能更好一点，有可能是这样。

PPT视角

01:49:54

对。对的对的嗯，好的。

王波

01:49:59

哦，好好好谢谢嗯！

PPT视角

01:50:00

行，好，谢谢好！行，那我们先休息十分钟啊，回来再继续。

PPT视角

01:59:29

喂喂，听到吗？喂喂。

王蓓

01:59:36

声音正常，可以开始授课。

PPT视角

01:59:36

能听到不。啊，录录像正常吧，录像正常吧啊！

王蓓

01:59:41

对，录像正常。

PPT视角

01:59:43

啊行，那我们继续吧！啊，那这个下面呢，我们继续刚才的话题啊，我们刚才讲到怎么去做一个？呃，万达机器人。呃。刚才提到有一个词汇呢是embedding，那我不知道这个听众里面呢，是不是每个人都熟悉这个自然语言处理？那我这里顺便把embedding的一些玩法来给大家讲一讲，主要是讲那个voa。因为等一下讲那图技学习的时候呢，其实也有很多。呃，用到的技巧了，跟那个是一样的，那所以我们这里值得花一些时间啊。

PPT视角

02:00:24

那这里呢，我给大家介绍一下呢，你是怎么样去把一个？单词变成一个向量啊，那我举个例子，比如说有个单词叫gpu。那它怎么变成一个零点一，零点三，零点五啊？这个什么一长串的数字怎么变成一个向量呢？呃，这里。第一个方法呢，是首先是最简单的方法，就是所谓词典向量。那我们也称之为叫毒热就one hot那为什么叫one hot呢？这个原因等一下大家就知道了。

PPT视角

02:00:54

那我假设我们有一个很大的词典啊，右侧这里是一个词典，我用每一条横线代表一个单词，那这个词典呢，其实是很长的，它里面可能包含了十几万个单词，甚至数10万数100万个单词都有可能。那我现在要把gpu这个单词变成一个向量，那这个特简单。那我就查一下这个词典里面有没有gpu这个单词。有gpu这个地方我们就标一，没有gpu这个地方我们就标零啊！那我们这里的每一个元素，这个向量里面的第一个元素。对应的是词典里面的第一个单词。那gpu是不是第一个单词，它不是，那我就把它标为零？那其他的零也是如此类推？好，一直到下面，到了第六行的时候，我们有gpu了，那这个地方对应的这个单对应的这个元素呢，就是一，其他都是零。

PPT视角

02:01:50

那我们可以想象呢，用这个方法我们可以得到一个很长的向量。它这个向量的维度可能跟这个字典这个词典的这个词数是一样的，比如说可能有十几万个元元素，那其中这个单词对应的那个位置就是一，其他的那个位置就是零。

PPT视角

02:02:10

那我们就得到这么一个向量。那这个向量呢？一般情况底下的就只有一个一，其他都是零，所以我们称之为叫做one。那就是这个意思啊，就是这个意思，那我们也给它给它一个名字呢，叫做字典向量，那这个就是最简单，最朴实的embody了。那我相信这个即使你不是专业搞这个自然语言处理的，都很容易，可以理解是什么意思？那这个<外文>虽然。很简单，但是它有效吗？那很明显它的效果呢？应该是不大好的。那我举个例子，比如说有两个词啊，一个是中国首都。呃，一个是北京。那你你这两个词你很难想象他们的one hot限量是一模一样，那很明显，他们就绝对不会是一模一样。那所以你光是靠这个问号呢，来判定这个词的意义。那这个其实是没办法做到的，它起不了这样的作用啊。那所以这里面呢，我们需要有一些更加高效的，这个更加好的一些方法。

PPT视角

02:03:13

那这里更加好的方法是什么呢？那这里在很多年前呢，不是最近的工作了，那起码都是十几年前的工作了。那有一个工作呢，叫做what to。它可以把一个单词把它变成一个向量。但是呢，它又比<外文>更能体现出这个词的语义。那这个voa的工作呢，是由一个杰克裔的这个nlp科学家，他的名字叫米可洛夫。开创的一种方法，他发明这个方法的时候呢？他还在捷克的一所大学里面在任教，那后来这个google呢就把它挖了过去，那他在google工作的期间就开发了一个软件，这个软件的名字呢就叫what。他不光是发论文，他还写了软件，那这个软件呢，呃，下午何老师会给大家介绍一个类似的，就<外文>从今天这个角度来看呢，就已经落后了，大概可能只适合用来讲课。

PPT视角

02:04:11

那这个有一个更新的版本呢，叫<外文>，它更适合把一篇长的文章变成<外文>，那它也是基于<外文>发展起来的一种技术。那一看名字就知道了，就把单词变成<外文>嘛，对吧？那我一个句子你能不能变成<外文>了？那它可能就是把你那个那个句子里面出现过的单词都标为一，其他都标为零，但是你这个这个就没有考虑次序的问题，因为单词的次序不一样。那这个很明显这个句子的意思就会有点不一样啊，但是这个呃，用这个<外文>编码的话，用这个刚才所说的那种字典向量的话呢，是达不到这个要求的啊。那所以。那这里。我们可以看到了，把词变成向量跟把句子变成向量呢是两码事。那我们在这里讲的是下午何老师给大家讲的呢是test test。那关于这里原始的文章呢，大家可以看这一篇。这篇论文那这篇论文呢，我没有放在给大家的那个论文集里面，那这篇文章也没有什么。呃，很深的研究了意义，因为现在所做的研究工作呢，已经比这个深入很多了，那这个是米可洛夫关于这个怎么把单词变成向量的最原始的文章啊，当时他还在这个。走。

PPT视角

02:05:35

部部落大学不知道中文怎么翻译啊？就在这个学校，这个是在捷克的一个学校啊！他是捷克人。捷克人，但是后来是去了google工作以后才发明了这个<外文>这个软件啊。好，那我这里可以把他的思想简单的给大家讲一讲啊，给大家讲简单的讲一讲。那他的思想呢？非常简单，那大概是这样子，用左边这个图就可以说明。

PPT视角

02:06:04

我们怎么样把一个词变成向量呢？那假设我们有很多语料。那语料无非就是长篇大论的文章，对吧，那通过这个语料呢，我可以知道上一个单上下文。我当前这个单词的下一个单词是什么？我是可以知道的，或者上一个单词是什么？上两个单词什么？这下三个单词什么？下四个单词是什么？那这个我是可以知道的。那我就可以通过来。

PPT视角

02:06:32

这个来构造？那举个例子，比如说我做一个很简单的神经网络。那这个神经网络简单到什么程度呢？它全部都是线性的，它连激活函数都没有，它全部都是线性，就从这个向量到这个这个中间这个嵌入层。就相当于乘上一个矩阵，然后从这个嵌入层到输出也相当于乘上一个矩阵，那我们所要决定的就是这些矩阵的元素就行了。啊，那我们只需要做这么一个事情哈，那这个神经网络。我们怎么训练它呢？那我们刚才说了，不是有大量的语料嘛，对吧，那我就塞入一个单词，那这个单词塞进去的是一个onehot向量。

PPT视角

02:07:14

就是这个01？那个很长的那个十几万这么长的那个向量啊！好，中间这里呢，这个这个线路层，这个隐藏层的那个维度。是我们定义的一个超参数，你可以取成512为这个1024为2048维等等，那一般我们都喜欢取成是。那个二的N次方啊，这么一种。这么一种维度，但是是是不是一定要这样呢？其实也未必你可以取成别的维度，好像也没有什么大碍啊。

PPT视角

02:07:43

好，然后输出这里要求是什么呢？要求就是预测下一个单词，它这里也是一个。跟那个字典一样长的向量，但是它就不是毒热了，它每一个位置上面呢，其实都是有一个softmas。就是它每一个输出可能都是0.1啊，这个0.05啊0.3啊这样子。代表的是。出现那个位置的单词的可能性。

PPT视角

02:08:09

但是呢，我们在训练的时候呢？是要用毒热向量来训练它的，比如说啊，我爱北京天安门。

PPT视角

02:08:17

那这边的输入我？那这边呢，我们就用爱来夹住它，对它进行这个神经网络的训练。呃，我输入爱，那我们下一次就北京啊等等，那我们就可以用这些语料用这种成千上万的大量的语料可以把这个神经网络把它给训练好了。

PPT视角

02:08:39

呃，跟GPT有点像啊，但还是有点不像的，<笑声>等一下就知道了，对对是好，那我们训练好这个网络以后呢？那我们需要做的<外文>是哪个呢？那很明显不是输出这个，因为输出这个，它的那个长度并没有缩短，是我们其实要的是中间这个隐藏层，就是说我输入进去一个<外文>，一个单词。那这个输出是什么？我就先不管它，我就要它中间这个就够了。那中间这个512维或者21024维的这个向量就是我们所需要的。就是我们所需要的embedding那这个就是我的原理了。啊，就这么简单？那我们要做的就是用大量的语料来训练这个神经网络就够。那真正的这个<外文>大概是什么样子呢？

PPT视角

02:09:33

呃，它的原始文章呢是这一篇啊，那这篇呢，我也没有给大家。也没有给大家，但是大家很容易可以在aka这种网站里面呢，能找到。那我建议大家看的文章呢，是这一篇。这一篇啊。Parameter learning explain. 那这篇文章是一篇综述文章，大概可能是一个讲课的一个讲稿。就讲讲述这个vtovta是怎么工作的，那这个写的人呢叫荣心。那这个应该是一个华裔的科学家。呃，但是很不幸的，我在大概一两年前呢，看到有一个消息。啊，这个人好像自杀了，不知道什么原因。就好像，而且这个自杀还是什么，是一个很离奇的事情。就比如说好像是从从这个飞机上跳下来还是什么，大概是反正一个匪夷所思的一个一个是一个一个一个自杀的方法。

PPT视角

02:10:29

啊，那这个非常的遗憾，但据说他有抑郁症啊，不过我看他写这个文章呢是。确实写的挺好的。那这个也是我看过的，学习的材料里面写的最好的一篇。那今天给了大家有大概45篇文章，那这个是其中的一篇啊。那大家如果想仔细的研究这个moba的话呢，可以慢慢看一看它。那主要是这个<外文>里面有两个很重要的知识点。一个是层次<外文>，还有一个呢是副样本。那在很多地方都讲的不清楚了，那比如说你看那什么知乎啊，有很多博客啊！基本上就没见过，有哪一个是讲的清楚的，那唯独这一篇文章呢，他讲的非常的清晰。啊，那这个看一看就明白。

PPT视角

02:11:16

那我这里就不给大家读这篇文章，我就说一说那个<外文>真正的样子是什么样子了。那我们刚才给大家看的这个图啊，只是用来解释原理的，实际呢，没有那么简单啊，实际没有简单，那实际的<外文>应该是这样的一个模型。应该是这样的一个模型就是。我有很多语料。那我我现在有这个神经网络，这个神经网络什么样子了，我输入一个one。中间是一个比较小的，比较窄的，比如说512维的这个隐藏层。然后这里输出的是什么呢？这里输出的就是它上下文的单词。那这里看到了，这个窗口宽度是二？那就是说我们预测上文倒推两个单词就W是当呃不是那个。T是当前的单词，然后我们倒推T减一这个位置T减二这个位置的单词，还有推这个T加一位置T加二位置的这个单词。啊，我们我们推这些位置的单词是什么？那在这里的话呢，从这里到这个单词的这个矩阵。跟从这个隐藏层到这个单词的矩阵，这两个矩阵是一模一样的，不光是两个矩阵是一模一样，这四个矩阵都是一模一样，是联合训练，联合在一起来训练。那我们就是。给出一个单词，然后呢，我们从语料里面呢，可以知道它上文的单词，也知道它下文的单词。那这样夹住它以后，我们就可以把这个神经网络把它训练好。

PPT视角

02:12:50

那最后用的时候？我们输入一个单词，那我们通过把它的隐藏层拿出来，那我就可以知道它的embedding应该是什么样子。那这一种方法呢？叫做？啊，那这个是<外文>引入的一种技巧。那这种技巧呢，是让人如此印象深刻，那以至于我们基本上这个现在有很多都会用到这种技巧，像等一下要讲的图神经网络这个图嵌入这些啊，那就会用到这个技巧。

PPT视角

02:13:23

那除了这个skipgram以外呢，还有一种叫cb。这个是磁带模型，那就倒过来。就说我们知道上下文的单词来预测中间夹住那个词是什么？那这个很明显就不大适合用来做。这个<外文>了，因为<外文>是输入一个单词，你得到隐藏层，这个隐藏层是<外文>嘛，是吧？那这个这个<外文>的话，你要输入好多个单词，它才可以得到隐藏层。那这里就不是做<外文>所需要的。那当然我们只是出于这个模型比较的需要，那所以就顺手讲一讲这个真正在这个<外文>里面用到的呢，是这一个模型就skip。

PPT视角

02:14:05

啊是这个模型？好，那关于这个。

PPT视角

02:14:11

呃，<外文>我们就简单介绍到这里，那我记得在给大家的那个学习材料里面呢，应该好像也有讲啊，那希望大家都看过吧。那这里有一些听众可能会问。那如果有一些词是在我词典里面没出现过，那咋办呢？那我怎么去给它做呢？那这个无论你的词典考虑的怎么周传周全都好，那肯定有一些词呢，是在这个词典里面呢，是没？呃，不一定会存在的。那关于做O？就是out of vocabulary就词典以外的这些单词。他的embedding怎么做呢？那大家可以参考这篇文章。那这个在我们过去的这个自然语言处理课程里面呢，反复的都会讲到。那这个人叫做王林？叫王玲那这个人呢是？<外文>现在去了？哦，那当时他是在葡萄牙的一个。学校里面工作，一个研究所里面工作啊，那后来因为他比较出色的这种自然语言处理里面的工作呢，就被丁曼招揽走。那这篇文章呢？讲了对于英文单词来说。如果这个单词在词典里面没有出现的话，我们怎么对它进行？embedding那我把这篇文章呢也发给了大家啊，那大家可以自行阅读。那如果有问题的话呢，我们也可以在这个相应的这个微信群里面来进行讨论。那我们就不在这里费时间了啊！

PPT视角

02:15:44

好，下面呢，我们开始图机器学习。那这里面。主要是讲一讲这个核心思想。以及一些核心的工具，我们可以很快的就把这个事情把它讲完啊！那什么是图呢？那图里面呢，说白了就是两个东西。一个是顶点，一个是边？那就是由顶点跟边所所组成的集合。那我们图一般用G就graph这个符号来记录？那这个国号里面写的V跟一的意思什么呢？那这个V就是顶点集啊，有把这个顶点全部写下来，顶点的集和V那一般我们还会在下面呢，加上一个V等于什么什么啊，这样的一个。

PPT视角

02:16:25

呃描述，然后一呢就编辑有哪些编是连成？连在一起的啊，这个是编辑？那这个图有哪些应用呢？那最简单的应用有很多了。比如说我们有一个社交网络图。那这里的每一个节点代表的是一个人。然后呢，它们之间连接一条线呢，代表它们之间的这种社交关系。那举个例子，比如说在社交网站上面。那每一个节点呢，可能是一个。这个网站用户，然后呢，这个他们之间连了一条边的话呢，那表示的是他们之间的那种好友的关系啊等等。

PPT视角

02:17:19

还有这个网页连接图。比如说我们这里每一个点代表的是一个网页。那这个我们这个网页到另外一个网页有一个有相编。那这里代表的是我们这个网页里面呢，有一个链接指向另外一个网页啊，那这个是网页链接图。那还有这个是？这个文献，科技文献。那每一篇文献的最后啊，都有一个index就一个论文引用表，那我们引用了什么论文在这里就有说。那这里这个箭头呢，就代表这篇论文引用了另外一篇论文啊，代表了这个意思。

PPT视角

02:17:56

那还有这个是压图，那这个明天再详细的会给大家讲。因为这个关系型数据库呢是先设计压图。这个叫概念设计，然后呢，我们再设计这个表的格式，那这个叫逻辑设计，那其实知识图谱也差不多，它先要画一个概念设计。那就有点像压图这个样子，然后呢，再来把它实例化，这个过程是很像，那我们明天再说这个具体是什么意思啊？

PPT视角

02:18:26

好，然后这里的话呢？呃是一个知识图谱。那大概是一个有点像那个天眼茶或者什么爱启茶这样的工具吧。就有一些公司啊，有一些人物，那这些公司之间可能有一些控股关系啊，这人物跟公司之间呢，可能是有法人代表啊，或者这个高管啊，或者控股啊，啊，这种关系啊，那这个就形成了一张知图谱，那知识图谱呢就是这个。

PPT视角

02:18:50

啊图的一个最最简单的一个应用啊！那这个比较常见的，基于图的一些分析任务有哪一些呢？那比如说第一个节点分类。那好像在这个twitter上面？那我根据这个用户发表的这个文章以及用户这个点赞呐，或者转发啊，呃，这样的一些行为。我把每一个用户分下类，比如说分成他是共和党的支持者呢？还是民主党的支持者呢？那这个是能分析出来的。那又例如说啊，在这个银行里面。

PPT视角

02:19:27

啊可能。可能有这个优质用户跟非优质用户。那他们之间可能也有一些联系啊，比如说有有一些用户跟另外一些用户有一些转账的关系。那我们现在要利用这个用户本身的个人信息以及互相转账或者消费这种信息。来判定一个用户是不是一个优质用户或者是一个非优质用户啊，那这个就是一个节点分类的问题。

PPT视角

02:19:56

那还有其他的一些图分析任务了，比如说啊，我们要在一大堆人里面识别这个群体啊，比如说公司的员工啊，它里面可能有很多的小圈子。那我们根据他们之间的这种工作的联系，或者这个联系之间的密切性。那我们可以判断出一些小圈子出来。那这个小圈子判定出来以后呢，自然就会有一些下游任务了。那举个例子，比如说啊，有哪些人适合当领导了？啊，有哪些人适合当这个沟通啊？啊等等。那比如说有一些人，他是既属于这个圈子，又属于那个圈子，那他就是两个圈子之间的，这个我们称之为叫做守门员。它是这两个圈子交互的一条路径。

PPT视角

02:20:42

呃，我之前看过有一篇比较搞笑的一个博客。啊。之前有一个电视剧叫做冰与火之歌，就是权力的游戏很。很很流行嘛，对吧？那我们经常会推测，在下一集的这个电视节目里面呢，有哪个角色会领这个便当？这种人<笑声>下一季的剧情里面，那我看到有人预测，就大概这个电视剧上映到60%的时候。有人就会预测啊，有哪个人会领领便当，有哪个人会活到最后，那这个真的是很准的，比如说他预测那个小恶魔一定会活到最后。那为什么他会活到最后的原因很简单？因为他是那个狮子家族的人，同时他又是龙妈这边的人。呃，同时呢，它又跟这个中slow关系很密切，那所以呢，它同时联系了很多个圈子，联系了很多个集团。那如果这个人死了，那这两个集团都不知道怎么发生联系了，那这个剧情就没办法开展下去了。所以这个小恶魔是一定不能死的，那这个已经在那个时候就已经被分析出来了。

PPT视角

02:21:51

然后谁会死了？那最容易死的就是龙妈<笑声>，所以这个我对这个结果的印象非常深刻的原因是什么？因为龙妈仅仅只是属于这个龙妈集团，它那个集团，它跟其他的这个集团的联系都不紧密，所以它死了，对整个剧情的影响。其实并不是决定性，就说这个填坑的时候它死了，对填坑没有影响。那如果这个小恶魔死的话呢，对填坑的影响呢，可就大了。那这个最后这个剧情的发展，那果然如其所料啊，那这个大概可以视为是。

PPT视角

02:22:25

这个图数据分析的一个简单的应用吧啊，那我对这个文章的印象太深刻了，那当我鼓励这个学生去学习这个。图继续学习的时候呢，我总是把这篇文章塞给他看。那另外呢，对于这个知识图谱来说呢？啊，这个有一些特定的任务了。那比如说我们会预测两个节点。那一般来说，在知识图谱里面呢，这个节点就代表实体。那我们预测这两个实体之间有没有联系？那这个我们称之为叫做补全工作啊，或者说这两个实体呢，其实是不是同一个意思，那我们称为叫融合工作。那如果是同一个意思的话呢，我们就把它放在一块啊，那这些也是图机器学习经常对付的任务。

PPT视角

02:23:09

那还有一些这个比较奇葩的一些。呃工作，比如说我们判断两个节点，它周边的组织是不是很相似？那之前呢，我曾经在北航参加过那个北航大数据中心的一个电机里。呃，被应邀出席那个电机里？那他的那个研究中心的主任呢？是从美国回来的。那他在美国的时候呢，就替这个联邦调查局做一个工作。就是分析这个黑社会成员。就他通过这个facebook或者其他的社交网站。

PPT视角

02:23:43

的那个联系的模式，那大家可能都有，应该有很多人看过教父这部电影吧，他描述的是这种意大利黑手党的那种组织的形式。那最上层有一个教父，他是他是做最后决策的。但是他一般不会直接接触到。这个最基层的那个去犯罪的，就是真正去杀人啊，或者打劫啊那个人。他一般底下有一个军师，这个这个军师呢，通常是他最信任的关系最密切的这个人，然后一旦出了什么问题的话呢，这个军师会给他扛下所有所有的罪名。那这个教父会把他杀人，或者做一些什么犯罪行为的这个指令下达给军师，然后这个军师底下呢，再有若干个。这个中层领导，这个中层领导就相当于是打手的头。那这个军师在把这个命令告诉这些打手的头，那打手的头呢在安。

PPT视角

02:24:41

排一个具体的打手去进行这个犯罪的活动，就是他们的这个发号施令的这个过程，整个决策过程呢是有一个。一个比较固定的模式的。那这个我刚才说了这位朋友了，那个北航大数据中心主任。那他就是专门干这个事情，就在一个社交网络图里面呢，要挖掘出这种黑手党组织。的一个可能潜在的这么一个结构。啊，就说看看有哪一些社交的结构呢，跟这个犯罪组织是很相似的，然后我们就对它来进行这个追踪啊，那大概是完成这么一个工作吧啊。嗯。

PPT视角

02:25:27

那这里解释了一下为什么要在那个图数据上面？来使用机器学习。那这个当然目标就是使到这个分析更加精准了啊，大家自己看吧，我这里就不练了啊，这里就不练了。那这个我们等一下讲了这个gnn呢，这个图神经网络呢？是一种。复杂，但是又很巧妙的神经网络。由于它是这个复杂程度比较高。所以一般人现在看外面的这个书都不大容易能看得懂，我们这门课里面讲的两个事情，一个是。图神经网络还有一个是知图谱，都是单独看书，不一定能那么容易看懂，那我们正好这个课里面覆盖的，那这个也是我喜欢的一种。

PPT视角

02:26:16

这个上课的方式就是通过一条主线索。那我们的主线索呢？就是做垂直领域。这个专家机器人。然后把一系列的不容易懂的东西，或者说我很有兴趣，想把它讲明白的东西串起来，让大家能够明白啊，这个也是我喜欢的一种。

PPT视角

02:26:34

啊，上课的方式吧？好，那我们下面看一看这个图怎么表示？那这个图最常见的这个表示的方法呢？就是临街矩阵。那比如说我们这里现在有左边这个是无相图，右边这个是有相图。那比如说我们这里啊！有12344个节点？那我们在这个临街矩阵里面的每一行代表一个节点啊，第一行是一号节点，第二行是二号节点，第三行是三号节点，第四行是四号节点。

PPT视角

02:27:09

然后每一列呢也代表是一个节点啊，第一列是第11号节点，第二列呢是二号节点等等。那如果他们之间存在一个边的连接呢，那我们就在比如说啊，一跟20连接的。那我们就在第一行第二次列这里标个一。或者呢在。第二行第一列这里标个一，因为对于无相图来说，一年二跟二年一是同一个意思啊，那于是呢，我们就得到一个可以描述它的举证。

PPT视角

02:27:40

那对于右边这个有像图呢？那我们这里是二指向一，那所以我们只能够在第二行第一列这里标一。而在第一列啊，第一行第二列这里是没有标一的，因为一并没有编指向二。那我们就很容易可以得到一个连接图方式的表示。啊那这个。呃，这个是最常用的表达方式了。然后。呃，这里特别是要说一句的，如果1234这种标号改变了，这个顺序改变。我可以把二标成一把四标成三。呃，颠倒过来，那这个连接矩阵呢就会相应发生变化。尽管这个图的内容没有发生变化，仅仅只是把它的描述的那个名称变化了一下。但这个连接矩阵呢，都会发生非常大的一个变动。那大道理基本上看不出它原先是什么样子啊，那这个也大概是用邻接矩阵描述图的一个缺点。就它不稳定，这种描述是不稳定的，当你的节点发生变化的时候，这个连接图会改头换面，让你都看不到它原先是什么样子的啊。

PPT视角

02:28:59

那关于这个连接矩阵的研究呢？呃，有一些很有意思的结论，那我这里顺带讲一讲嘛。因为你可以把这些呢作为图分析的一个基本的内容。在深度学习时代以前呢，我们对这个图的研究就是做这种分析。那举个例子，比如说有一个向量，这个向量叫一啊叫一。那这个我们一般在数学上称之为叫特征向量。连接矩阵a乘上这个一以后。如果得到的结果呢，相当于一乘上一个标量L。那我们就从一？是。这个a的一个特征向量。然后这个拉姆达拉称之为叫特征值。那由于这个a本身是对称矩阵嘛，就对于无像图来说，是对线对称矩阵。那所以这个它这个一定是能够求解出这个拉姆导出来的。那我们可以找出这个特征值landa里面最大的那个这个不止一个特征值。一般a的矩阵a矩阵的这个呃维度是多少，比如说是N乘N的。那这个luma就会有N个，我们找出里面最大的那个luma。然后呢，这个landa所对应这个一，我们称之为叫中心性向量。那什么意思呢？那大概是这个意思啊？我没有画出那个哦，在这里画了，这个是那个中心性向量，比如说啊。那里面呢？对应于一这里呢？那代表的是它的中心性质。对，因为二这里呢，这个也是二的中心性质，那这个中心性质大概是什么意思呢？

PPT视角

02:30:41

就是你在那个图里面的重要性。那如果你处于一个连接比较多的那个位置。那就代表你这个节点就越重要。啊，那就比如说，对于人群来说。你如果是中心性为这个中心性。呃，这个呃指标很高，那就代表你是这个群体里面比较重要的一个人，因为你跟很多人都有社交联系。那如果你代这个图代表的是一个网络互相链接的这么一个结构的话呢？那这个中心性指标代表的就是你这个网页的重要性。那反映出来大概是一个什么情况呢？就是。如果我顺着你这个图不断的漫游下去，随机的点击下去啊，你比如说我在一个节点上面看一看我有多少个链接出去。我就随机选择一个啊，然后到了下一个节点再做同样的事情，你在这个图上面随机漫游。那这个中心性指标相当于什么呢？相当于是你最终停留在那个节点的概率有多高？相当于这个意思。

PPT视角

02:31:49

那大家是不是听起来很熟悉？那这个不就其实就是google的？Google里面呢，就是用这个方法，通过对网页链接图进行这个特征值的计算特征向量的计算，得出每一个网页的page。从而可以知道网页的重要性。

PPT视角

02:32:10

因此呢，当你搜索什么东西的时候呢，这个搜索结果来排序，它就按照<外文>来输出给你，那你自然就看到一些质量比较高的这个中心性比较高的网页了啊，那这个就是图的一个很简单的应用。好，那这个图里面呢，还有一些这个指标，比如说。除了度那度代表是什么呢？就是你。连接的边的这个数目？就一个一个节点，你连接多少，它边那就是这个度，就是你连接边的数目。然后呢？还有初度跟入度，那度是对于。这个无像图来说的，那对于有像图呢，就分成粗度跟入度。初度什么意思呢？就是你从你那个节点？开始你指向外面的边有多少条？那入度是什么意思呢？就是有多少条边是指向你这个节点啊？呃。

PPT视角

02:33:14

刚才说过了，这个连接矩阵呢，会随着你这个节点的调整。而发生剧烈的变化。那其实在很久以前呢，我们就有一种想法，就是能不能把图数据。就以图形式来组织的数据。呃，作为一个神经网络的输入。然后呢，我们可以得到这个相应的一些图分析任务的结果。比如说，我们可以通过一个深度的网络来对一张图进行分类，或者对某个节点来进行分类。啊，那我们可以得到这些结果。呃。但是呢，实际上这个是很难做到的。那原因是这个？当这个连接当这个节点发生变化的时候，它的连接矩阵会发生非常大的变化。那导致于。你要训练这个神经网络，你基本上要把那个节点都轮替一次，就是不要把这个节点轮换一次。

PPT视角

02:34:13

那它才会产生全体的所有的这个连接矩阵，你每个连接矩阵都要送进去训练一次。那但这样这个数字太高了，那这个这个大概多高呢？你要做a的阶层背的训练，就说你原先的这个。呃，图是N个节点，那它的排列顺序有N的阶层种方式，对吧？那N的阶层的切成种方式的话呢，你就有N的阶层种那么多的这个连接矩阵。那你再输入到这个神经网络图里面，训练的话呢，大概没有人能承受得起这个计算量。那简单一点的N等于1000吧，那你算算1000的阶层是多少？那这个这个是算不了的啊，那所以这种思想呢，是想不通，我们要做图神经网络的话呢，那肯定不能够以临阶矩阵来作为输入的结果，我们必须要用其他的啊。

PPT视角

02:35:07

那其他这些知识都不大重要，因为这个时间有限呢，我这里就。呃，不细讲了，大家回去可以自己来看一看就行。那刚才讲的图呢，都比较简单，就是只有顶点跟边。那有一些时候呢，这个边。呃，上面呢可能还有东西。那有什么东西呢？比如说有权重。啊，比如说两个人。好友关系是吧，那好像就好有关系，但好友关系也是可以描述的。比如说你是这个联系比较频繁的好友关系呢？还是联系比较疏远的好友关系呢？啊，还有这个？呃。其他的一些，比如说银行里面判断两个用户之间的关系，通过转账来判断是吧？那你转账转了很多，转100万呢，还是转一块钱呢？那很明显的，对于我们判断的一些事情呢，也会有影响。那所以呢，这个边上面呢，一般我们还会有权重啊！那这个边还可以给它类型，那在知识图谱里面呢，这个边代表两个节点之间的联系，那这个边呢，可能是比如说啊，这个人是那个人的老师啊，这个人是那个人的父亲，这个人是那个人的儿子等等啊，那这个。

PPT视角

02:36:17

烟本身呢也有分类类别。等等啊，那这个是图的一些扩展的属性。这个图呢，有一些好时候呢，也会长得比我们想象的要复杂一些，比如说有一些图，它有自己连接自己的边。还有一些图呢，有多重编，这个在知识图谱里面特别常见。因为两个人的关系呢，可以是多重了，比如说他们可能是夫妻关系。他们同时也可能是同事关系。他们还可能是一些别的关系。比如说这个顾客跟老板的关系，上下级关系啊，这个都是有可能。啊，那所以这个这个多重？这种图我们也可能要考虑啊，那我们把那些没有多重编，也不会有自己到自己的编，这种图呢称之为叫简单图。

PPT视角

02:37:14

那在数学里面呢，大部分时候都是讨论简单图。但是呢，当我们讨论到知识图谱的时候呢，很多时候会面对复杂图。啊会会面对复杂图啊！这个就不说了，这个是用用来做因果推断。那还有一个概念，就是图里面的路径。那就是从一个点到另外一个点的一条路了，这个都不用解释，大家都明白是什么意思。当。一个图里面任意两个点之间都存在一条路的话呢，那我们就称这个图是联通。

PPT视角

02:37:55

那比如说像左边这个图，那你随便任何两个节点之间都会存在有一条路，对吧，但右边这个图就做不到了，比如说这个点。跟这个点那你怎么找都找不到一条路，那他们就不联通。那我们称这个？呃，一个最大的一个联通的一个子集，称为叫分量，就联通分量。那很明显的，右边这个图呢，里面包含了三个联通分量啊，那这个是一个联通分量。下面这个是一个联通分量啊，这里也是一个联通分量。那这个图的连通度怎么判断呢？那一个呢，你可以从临街矩阵里面来看。那比如说像左边？这个图对应的连接矩阵呢是这样。它是分块的？就它对角线上是分块，然后这里有很多是零。那如果你给它加上一条边，使到它连通起来的话呢，它就变成不是刚才那个样子了，它就完全变变化了。

PPT视角

02:38:56

但是这个连接矩阵是不是一定会变成这个样子呢？那实际上也不一定。当你把这些节点的顺序颠倒一下呢，也许你就看不出这个形状。那所以从这个角度来看呢，是不可靠的，就用连接矩阵来判定这个联通分量呢是不大可靠的。

PPT视角

02:39:16

那比较常用的一种方法呢，叫做。拉普拉斯举证。叫做拉普拉斯？

PPT视角

02:39:23

那什么是拉普拉斯举证呢？它的定义是？A O等于D减A？B减a什么意思？那a就是刚才所讲的邻接矩阵。那第一的话呢，叫做杜矩阵，那杜矩阵什么意思呢？那举个例子，比如说像这里这张图啊，有12344个节点。那度矩阵呢，就是一个对角矩阵。除了对角线上的元素以外，其他全都是零。然后对角线上的元素怎么定义呢？比如说一号节点它的度是四，那我们可以看到一号节点一共有四条边，对吧？所以它的度是四，那这里写个四。

PPT视角

02:40:02

然后噩耗节点？这里12号节点它的边是三，所以我们这里写个三。那其他也如此类推，其实这个每一个对角线上的元素就是它的度，那这个我们称之为叫度矩阵。然后这个拉普拉斯矩阵呢就是。杜矩阵再减去邻接矩阵。那假如你的图是无像图的话。那a是一个对称矩阵，那这个D由于它是对角矩阵，所以它也是对称。对称减对称它还是对称？那因此，对于无像图来说，这个拉普拉斯矩阵呢？就是一个对称矩阵啊！好，那这个拉普拉斯举认有什么性质呢？那第一个刚才说的它是对称，第二个它是半正定。那这些数学名词我就不给大家解释了，因为你懂的，我不需要解释，你不懂的话呢，我需要给你解释的时间太长。

PPT视角

02:40:57

那大家可以查回你之前学的高数或者线性代数的书，应该能找到什么叫半正定啊？这个是什么意思啊？好。作为一个办证定矩阵呢？它的特征值啊，刚才说过矩阵的特征值什么意思呢？它的特征值一定是大于等于零的。那其中。等于零的那个特征值，它的重数那什么重数呢？比如说特征值一共有五个，其中三个是零，还有一个是11个是二。那这三个是零，那就说明这个特征值的重数是三啊，就这个意思就是它重复了三次都是同样的值。

PPT视角

02:41:38

那这个特征值是零？的重重塑。就相当于图里面联通分量的个数。那这个是判定联通分量的一种最简单的办法。那这个拉普拉斯矩阵呢，用来做求联通分量呢是挺好用。那除了这种应用以外呢？它的那个整个特征值从高到低排列以后。那这个拉普拉斯矩阵就可以变成一个标准型。就说这个矩阵虽然很复杂。呃，你可以把它的节点变来变去的。但是最终你无论怎么变都好。你这个标准的拉普拉斯矩阵是不变？那这里就给了我们一线希望是什么呢？那我们这个希望就是当我们用拉普拉斯矩阵来描述一张图的时候。无论这个图的节点，次序怎么变？最后这个拉标准的拉普拉斯矩阵它都都是不变的。

PPT视角

02:42:38

那我们可以把这个标准的拉普拉斯矩阵作为一个图神经网络的思路。那我们终于能找到一个稳定的思路了。那有很多图神经网络的研究是基于这个事情来做的。那我之前推荐过一些图神经网络的书给大家，那其中有一本书我忘了。那本书的名字叫什么了，好像就叫做图深度学习之类的。它整本书都是围绕着这个普分析来展开的。那这个讲法呢，跟其他书有点不一样啊，跟我这里的讲法呢，也有点不一样。

PPT视角

02:43:13

但是呢，这种是这种做法呢，也有一点局限性，因为拉普拉斯矩阵能反映出来这个。图的信息是比较少。那其他还有一些图的一些整体的特征。啊什么度分布我我这里不全部讲。我不这里不全部讲啊，大家可以看换成片，因为时间不够啊，我就讲一个度分布吧啊！那杜芬布什么意思呢？那这个是这个图的一个整体特征。那这里的网络其实就是指图了，网络跟图是同一个名词啊。它的那个节点度数的总体描述。比如说我在网络里面随便选择一个节点。被选中了，节点的度为K的概率有多高？那举个例子，这里一共有十个节点。啊，这里K是度啊？杜唯一的有多少个呢？12345662。所以概率是60.6%。0.6。那杜维二的有多少个呢？

PPT视角

02:44:23

那我们看到了，这里有一个。呃，还有一个在哪？这里有一个。好像没有了吧？啊，就一就一个没错，所以这里是0.1啊，这里是0.1。那其他的也类推了，我就不讲了啊！那这个度的分布可以作为什么呢？可以作为图的一个完整的特征。

PPT视角

02:44:49

那等一下呢，我们可以把这个输入到图神经网络里面去的啊！好，那其他的这些都不难自己去理解啊，那这个只可以自己来看就行。

PPT视角

02:45:17

好，下面。我们先休息几分钟时间吧啊！

王蓓

02:50:36

黄老师可以回来上课啦！

PPT视角

02:50:47

喂，听到吗。

王蓓

02:50:56

能听到声音正常，录制正常，可以开始上课。

PPT视角

02:50:56

喂。行啊。好。好，那我们继续啊！啊，那下面呢，我们核心的地方就来了，刚才讲的是图的基本知识。那其实这些图的基本知识呢，大家在其他长河或者深圳，在你大学学习期间都已经学过了，那所以我就讲了讲了简略一点。那如果你觉得。呃，不是那么好理解。诶，这怎么会变这样？

PPT视角

02:51:22

我按了什么地方呢？帮我处理一下。我刚才好像按了一个什么地方会变成这样。好好的好，那因为大家可能都已经接触过或者学过了，所以我就简略一些。那如果你虽然没听明白的话呢，那你可以看我的幻灯片有什么问题，我们可以在群里面讨论啊。那下面呢，我们就重点是讨论这个图，机器学习它怎么做机器学习这个事情。那这里要讨论的东西呢？包括两大方面的内容，第一个呢是嵌入。那大家刚才发现的嵌入是一个很关键的东西。当我们能够把一个文字，声音。呃，视频现在是graph是图变成一个向量的时候，那我们很多后面的事情就好办了，比如说我们可以对它进行分类，进行相似性判断等等，这些事情呢都可以做了，那我们也可以在陷入的基础上呢。

PPT视角

02:52:18

完成这个呃图的一些分析的任务。然后另外一个呢，就是图怎么做神经网络的事情，下面我们重点讲这两两方面的事情。那其中嵌入了有三种方法，一种呢叫浅层嵌入法。第二种呢是，第第三种呢是？然后最后讲这个？神经网络啊，那这个是我们最后要做的事情。好，那我们首先讲一讲这个嵌入啊，这个浅层嵌入。那这个浅层嵌入了比深层嵌入还不好理解。哈哈。那第一个呢，这个浅层线路呢，需要用一种所谓叫做。编码解码方式来嵌入。他是这么来做的，第一个首先编码那编码做什么事情呢？我们把一个图里面的顶点。映射为一个低维向量D ABCD的D啊映射为一个低维向量。那这个图里面的顶点怎么表示呢？我们也可以用<外文>的方法来表示。

PPT视角

02:53:20

比如说这张图有123455个顶点对吧，那现在呢，我就用onehot向量有五个位置。那我现在如果要表示的是三号顶点。那三号顶点就表示为00100就三那个地方是一，其他都是零，那这个就是三号顶点。那所以呢，图里面的每一个顶点我都可以用问号的方法来表示它。那无非就是一个位置是一，其他位置是零就是了，对吧？好，然后呢，这个编码器的意思就是这个向量乘上一个向量以后，比如说这个是一层。五的矩阵或者说你叫一乘五的向量也可以，五为向量也可以啊，你乘上一个五乘三矩阵以后。那就变成一个三维的向量了，对吧？那这个矩阵呢，就是一个编码矩阵。

PPT视角

02:54:11

它里面的那些系数是待定的。我希望求出一种方式。求就我们希望把这些矩阵里面的系数这些元素把它求出来。当它对这个很长的向量进行编码的时候呢，我们编能够编码出一个有意思的<外文>向量出来，我们的目的是这样啊。那无论怎么样呢，这个编码这个事情我们明白了，那所谓浅层编码的意思，其实就是线性编码器。那我们把一个向量乘上一个待定的矩阵以后，我们就得到一个编码了。好，那这个编码呢？我希望是实现什么效果呢？我希望实现的效果就是。两个顶点。在映射到欧式空间以后。如果他们本来是相邻的。那这两个顶点应该比较接近。那如果他们是不相邻的，那这两个顶点应该比较疏远。假如这些边上面有权重的话呢，还要跟权重结合起来，比如说它的权重很高。或者表示它的亲密程度很高，这个权权重不就表示亲密亲密程度嘛，对吧，假如两个顶点是相邻的，并且亲密程度很高。

PPT视角

02:55:27

那我们映射到欧式空间以后呢，那这两个映射点它的距离也应该更近。那其实我们想达到这么一个目标。啊，那这里写出来就是这个样子了。那这里我们有一个解码器。

PPT视角

02:55:44

那这个解码器的意思就是给你两个向量？你能够。呃，尽量的接近这个相似的这个这个这个一个相似度。就是说我，我把这个原来的顶点编码。以以在V两个向量以后呢？那它这这两个向量做一个累积，它的转置在乘上这个做一个累积，不就是算它们的相似性嘛，对吧。那应该跟这个a UV里面的这个连接矩阵里面的那个元素，那这个元素要不就一，要不就是权重？呃，总而言之是代表了U跟V这两个向量的这个。呃，亲密程度。那我们就希望这个。跟以色的类型呢，跟auv你们这个元素要尽量的接近。那于是呢，出于这么一个目的，我们就有了一个损失函数，那这个损失函数呢就是。我们编码以后的以这个是<外文>向量他们做内集。减去这个UV在连接矩阵里面对应的那个交叉点的那个值。

PPT视角

02:56:53

那这个这个是一个损失，我们希望他们应该是相等的，但是事实上呢，你也很难做到100%相等。所以我们就减去这个元素。然后呢，我对全体的U跟V的组合遍历一下。就这里可能前面有一个sigma，把它便利一下，把这个误差平方求一个和误差平方和我现在要求这个误差平方和是最低的。而且误差平方和是最低的。那于是我便利完所有的节点以后呢，由于a UV是已知数值。那我们通过这个？一些这个梯度下降法，或者一些别的一些处理，这种简单的，像这个回归啊，什么之类的这种方法，我们都可以对它进行求解，那最后我们就能够求解出之前这个矩阵里面所有的这个元素的值。那于是我们就得到一个嵌入了。那这个矩阵本身直接就可以看出嵌入什么样子了？他不是五层三吗？五每一行刚好对应于一个顶点。那每一行的向量就是你嵌入的向量。比如说我们要看第三个顶点对应的向量是哪一个？

PPT视角

02:58:08

那我就把第三行拿出来啊，这个就是向量，这个就是你需要的。那做出来的这个过程呢？大概就是这个样子了。这个全程大概就是这个样子。比如说我现在这个。通过这个映射把一号顶点。映射到这里。然后呢，把三号顶点映射到这里。那他们两者呢？应该要尽量的接近啊，是这个意思。好，那这个是关于浅程线路。那因为刚才中间这里涉及到的一些数学。那大家看看有没有什么问题不？可以接受一些提问啊！

王蓓

02:59:00

嗯。

PPT视角

02:59:13

大家看有没有什么问题啊，能跟得上这个节奏？喂。不需要提问吗？而一。嗯嗯。对。

王文璞

02:59:49

喂，黄老师能听到吗？

PPT视角

02:59:51

啊，听到你好诶！

王文璞

02:59:52

呃，就是我问一下，那现在就是讲到嵌入之后，它等于是只是做了一次。做了一次转换嘛，然后那他不需要去做解码是吧，他直接就等于是说我。我等于是把这个。呃，全职的东西转换成了一个更低位的向量而已哦。然后我希望是说通过这个低位的向量，它的成绩和我之前那种更长的那个向量的成绩能相似就行了。

PPT视角

03:00:19

对的。去。不好意思，您能重复一下你的问题吗？我这里刚好出了一点状况。

王文璞

03:00:35

呃，就是我我我的疑问是，但它这个嵌入的话它不需要解码对吗？因为我我看到你刚才的讲述。

PPT视角

03:00:42

呃，不需要解码，刚才那个解码也就是装一些样子而已，它主要的目的还是为了嵌入。就是，就是有点像那个自编码器的样子，就有点像那个<外文>那样子，你不是输入一个东西，输出一个东西吗？那输入就是encodea输出就是decode它这个也好也好，只是做一些样子，你需要真真真正需要的是中间那个。

王文璞

03:00:56

对。

PPT视角

03:01:05

迁入城我们现在真正需要的其实就是那个编码矩阵。

王文璞

03:01:15

哦，但是它解码的话，其实等于是不需要训练的，因为它的成绩就是刚才我们那个损失函数去下降就好了。

PPT视角

03:01:23

对，没错是。

王文璞

03:01:26

Ok行我知道。

PPT视角

03:01:27

好的，谢谢嗯，大家看还有没有其他问题啊？

许涛

03:01:34

黄老师。

PPT视角

03:01:34

他没有。喂。

许涛

03:01:36

啊，就是我想我想简单说一下我对这个图神经网络的理解就是，呃，咱们说这个图神经网络指的是啥意思？就是说这个图它本身是个神经网络嘛，还是啥意思，还是说。

PPT视角

03:01:37

嘿。嗯。呃，是这样的，就是跟图分析任务有关的神经网络，你可以把整个图放进去，神经网络里面也可能把某一个节点或者某一条边放进去，这个神经网络里面。总而言之，就是你要对这些。与图有关的对象做出一些结论，比如说啊，我这个把节点放进去，那我这个节点是属属于什么类型的节点呢？我把这个边放进去，那你预测一下这个边的强度有多大呢？等等啊，那这些是用来完成这些任务的，这个相应的神经网络结构。

PPT视角

03:02:19

等一下马上就要讲到了。

许涛

03:02:23

啊，比如说我把。呃，那那就刚才咱们说的这个这这这个就是嵌入的这个词，就是这个节点的，它这个嵌，嵌入。呃，刚才讲的是一种叫针对节点做这个嵌入，什么是是就是。

PPT视角

03:02:41

呃，现在讲的这些图呢，是为明天讲那知识图谱铺垫的。就是跟chap GPT关系倒还不大。

许涛

03:02:47

啊，就是比如说咱们这个处理这这这个就是呃词嵌入这个过程完了以后，后边儿比如说我可能是把它送到一个。

PPT视角

03:02:56

对。

许涛

03:02:59

呃bp神经网络里边或者送到一个什么往神经网络里边去训练？

PPT视角

03:03:02

对，没错，等一下就讲到这个事情，今天呢要讲完这个为止，估计要拖堂了，又要从12:30拖到1:30左右，估计<笑声>就黄老师讲课每一次都要拖堂，这个不好意思<笑声>行。

许涛

03:03:06

啊好好好行行行好！好嘞好嘞行！

PPT视角

03:03:15

那当然没什么问题的话呢，我们就继续了，那刚才讲的是。

徐旭

03:03:18

诶老师老师诶！

PPT视角

03:03:19

是。哎，你好啊。

徐旭

03:03:21

嗯嗯，你好，呃，是这样子，我问个问题啊，就是你刚才说到的呃，首先是把这个就比如说你拿你这个例子，你五个节点，五个节点，一个图数据库也好，知识图谱也好吗？然后首先是one。

PPT视角

03:03:24

二。对。嗯。

徐旭

03:03:36

所以说你每一个节点应该是编码成为五维的一个向量，对吧？然后里面的话只有一个元素，对吧？呃，所以说用这种方式的<外文>的话，它其实是不具备的新的节点的预测功能，对吗？就比如说我现在五个。

PPT视角

03:03:42

对。对。啊。

徐旭

03:03:52

都已经建立好了之后我现在有第一个节点，目的是。

PPT视角

03:03:55

对。对。对。

徐旭

03:03:59

对，就那那如果是这样子，我怎么做了，第六个节点，我现在是新的。

PPT视角

03:04:02

第一。

徐旭

03:04:03

我新采集的样本。

PPT视角

03:04:04

那那那就真的要重新开始了，那就。

徐旭

03:04:07

哦，那不能够能启动？

PPT视角

03:04:09

这这个就是一个复杂，就是好像那个词典里面突然间增加了一堆词一样。

PPT视角

03:04:14

这个这个这个就一样的道理。

徐旭

03:04:17

哦，就就就没有办法了，对吧？

PPT视角

03:04:18

没有没有没有没有没有没有办法。

徐旭

03:04:23

哦哦，那行啊，那嗯好，谢谢！

PPT视角

03:04:25

啊啊，因因为时间的关系呢，我就不回答更多的问题了，我们继续往下走啊！

徐旭

03:04:27

对。

PPT视角

03:04:31

那我们继续讲这个图的嵌入？还是讲这个节点嵌入，那我们开始来比较深刻的东西了啊。那第一个要讲呢是dew？那这个是用随机游走的方法来实现嵌入的功能。呃，这个第二呢，它的原始论文是这篇，那这这篇论文其实也是很长时间，以前的，大家可以看到它的时间是2014年，那也是八年前的事情。呃，它里面采用的思想呢，跟那个<外文>呢是非常相似的。那大家可以看到这里面有两个算法。这个我都截图出来了，从那个刚才给那篇论文里面截图了，那这篇论文呢，我也放在今天。给大家在群里面的那些论文里面，那大家可以自行阅读。啊，那这个算法是从里面摘取出来的。这个算法呢，我觉得写的有点错。呃，我等一下可以告诉大家他哪里可能有点错啊！呃。我用一张图我自己画的图来解释他的意思吧，那大概什么意思呢？那大概是这样子。就是在图里面。我首先做一个随机游走，随机游走，什么意思呢？就是，我先随机抽一个节点出来。那这里如果有十个节点的话呢，每一个节点被我抽中的概率就是十分之一，这都是等概率的，我随随便抽一个，比如说我抽中了VI。

PPT视角

03:05:54

我就在图上面随机的跑啊，跑出一条路出来啊！那这个怎么随机呢？那这个vi不是有五条边？接向旁边的那些点嘛？那我就只是一个色子，这个色子可能是。

PPT视角

03:06:06

这一个投资这个投资可能是五分之一的概率。选择一条边，那我比如说我自出这这这个节点出来，我要走这边，那我就走到这里，然后我就重复这个过程。那我规定一个超三数，比如说我这个超三数是12345是五啊，就是你这条路最最远走五步，你就停止了。那我就得出一条路出来了是吧，得出一条路出来。那这条路呢？所有的下一个选择都是随机选出来的。然后它的长度是固定的。而且是可以重复的，就说你可以走回头路。你你你织完这里啊，突然间我又要走回这里，那这个也是允许的。好。大家看一看这个节点组成的这条随机路径的序列有点像什么呢？

PPT视角

03:06:56

那它不就很像我们语料库里面的一句话吗？如果你把每个节点看成一个单词的话，那这个就是一句话呀，对吧？所以呢，下一步我们就用我的skipgram来对它进行训练。比如说。这个<外文>是这样的。终于可以用一下我这个。呃，首先我这里输入一个。这个比如说00100啊，我把这个三号叠节点。输进去。好。那我现在比如说啊，这个是三号节点。这个是五号节点？这个是一号节点，这个是二号节点。这个是。四号节点。这个是六号节点啊？

PPT视角

03:07:53

那我现在。要预测的是，就比如说我这个窗口W就是哦！我这里不要输三号节点吧？我还是输这个二号节点比较好啊！对不起，这里是一，这里其他改成是零。二号节点这里是一啊？好噩耗节点的前两个是哪一个呢？那一个是。一。还有一个是五。啊，那这里分别就是。一。零零零零，还有五这个零零。001啊。然后其他的这个它下面两个节点是什么呢？那就四号跟六号节点。啊，这个我我我不写，大家知道怎么写？那这个就是<外文>嘛，对吧，我就用这个<外文>。来对他进行训练就行，跟跟骆驼是一模一样的，就是你。随机走出一条路以后，我针对这条路上的每一个节点，就把它当成一个词，一句话里面的一个词。我就用这个方式来对他进行训练。最后我就得到一个中间的隐藏层。啊，最后我就得到一个中间的隐藏层，那这个隐藏层呢就是。我用来提取这个。就是我塞一个顶点进去。那我就可以把这个中间的隐藏层拿出来作为embed。

PPT视角

03:09:27

那这个随机游走可以起到什么作用呢？那它可以分析这个网络的结构。就它通过随机游走，可以知道哪些点跟哪些点是连在一起的，它可以分析出这个网络的结构。啊，它就起到这样的一个作用啊，那这个是叫做deepwalk。那这个是比较简单的，这个随机游走。那大家先听着啊，然后我们等一下再来讲。再来回答大家的问题啊！好，那我们再看一个比较复杂一点的随机游走。叫做。就节点到。从节点到向量。那这个文章也给了大家，大家可以自己来看啊！呃，首先给大家讲两个概念。分别是BF S，还有DFS那BF S，而就是。呃，宽度优先搜索，DFS呢是深度优先搜索。

PPT视角

03:10:26

那红色这里的大家可以看到。从这个优节点出发，那我们总是访问一下旁边的节点。就回到U节点，又在访问另外一个节点又回到U节点。就说他总是比较喜欢在自己身边来探索，就一个身边的这个相邻的这些都先探索完了远处的，我先不管，我先探索完我这边。那这个我们叫做宽度优先搜搜索啊，那这个大家在学图论里面呢，应该学过这个。还有一个呢是dfs那dfs呢，就是我尽量不走回头路，我尽量往后面走。都不回头啊，那一直这么走下去。那这个。BF S呢，主要是探索节点附近的结构。而这个深度优先搜索呢，主要是探索远方的结构，就一个一个比较宏观的。一个整个头的图的结构，但很明显呢，它并没有足够的时间去了解细节。

PPT视角

03:11:25

就是bfs了，更倾向于了解自己旁边的细节。而dfs呢，更倾向于了解远处的比较粗糙一点的信息啊，你这么理解就好了。好，那下面呢，我们可以。这个理解<外文>了，这个也是一个好精彩的这个作品呢？那这个ltovita相当于什么呢？它其实跟这个D的思想是一致的。他也是通过随机游走。来获得一个序列。那只是这个游走的方式有点不一样啊，获得序列以后呢，它也是一样用skipgram来得到最终的embedding。那只这个差别跟<外文>的差别是什么呢？只是差别在于它所采用的这个随机游走的策略是不一样的啊，它增加了两个超参数，分别是P跟Q，它用来。这个控制游走的方式。那这个不同的P跟Q的选择呢，可以使得你更倾向于BF，S呢？还是更倾向于dfs。啊，那可以起到起到这样的作用。好，那我们首先看一下这个定义啊！那这里什么意思呢？

PPT视角

03:12:39

这里的意思就是说前一个节点是V。后一个节点是X？我们这个色子是怎么定义的呢？就是说我前一个节点是V下一个节点选择X。游走的概率有多高呢？它是等于。如果是有边的话。它就有一个概率，没有编的话呢，概率为零了，没？没编的话你还走个**啊！那你你连走都走不过去，因因为连路都没有是吧，那在有边的情况底下呢？它是由PVX决定的，下面那个椅子呢是归一化因子。

PPT视角

03:13:16

就是使到它看起来是一个概率。就是它全部加起来是一。那这个你可以忽略这个以下主要是看拍。这个VX就够了。好，那PVX试者怎么定义呢？是等于阿尔法PQTX乘上W？那wvx呢，就是这个边的权重就vx这条边的权重。那我们可以想象，我们要根据权重来选择，我们应该走哪条路，那这个也是很正常的，对吧？啊，那这个随机游走本身也带也要受到权重的影响。那这里很关键呢，还有一个。这个阿尔法是什么呢？好，这里来。这个阿尔法PQ txt是这么定义的。

PPT视角

03:13:59

当DTX等于零？就T跟X是同一个点的时候，它们的距离为零了，不就是同一个节点吗？它是等于P分之一？当TX等于一的时候，它们是相邻的节点，有路通的时候。那它就是等于一？然后他们不通的时候，T跟X的距离等于二。那就超过二了，这个这个距离太远了。那这个是等于Q分？那大家看这个图就更能明白看右边这个图啊！我们当前节点呢是V？我们当前在这个地方。上一个节点呢再踢？

PPT视角

03:14:36

我们是从梯这里走过来的。好，那我们下面应该走哪里去呢？X一X二X三跟T都是我们的选择。那我们该走哪里呢？我们这么来定义它的概率。第一个我们首先看T。V跟T是相邻的对吧？呃，不是。我们首先看T。T跟T就是同一个节点。就是我们之前是从这里来的，我们下面要判别我们要走到哪里去。那我们就拿T作为参照物来判定。T跟T是同一个节点，那所以。我们就选择。这个。它是这个。DETX等于零，它同一节点，它是P分之一，它是P分之一啊。

PPT视角

03:15:25

好，我们再看，如果我们去X二呢？X二跟T是不相邻的对吧？不相邻的话呢，就是等于二。那等于二的Q分值，所以这里就是Q分值。同样的，X三跟T也是不相邻，所以这里也是Q分之一。然后从X到X一这里为什么是一呢？因为X一跟T是相邻。所以这里就定义为一啊！那这些值都定义下来了？这些值是不是概率了？它很明显不是概率，因为这里有个一，那概率是一了不就100%了吗？那所以你这里还要做一个归一化，就是你把它们加起来。作为分布。那用一，比如说这个分母是？这个叫Y吧，那你用一啊分母就叫以色吧，你用一除以以色。

PPT视角

03:16:14

这个Q分之一或者P分之一再除以以上，那它们全部加起来就等于一了。那这里就是这这个？这里面这个以下符号的意思啊，就代表这个。好，那这里就定义了我们游走的概率的方式。那我们可以看一看P跟Q是怎么影响我们游走的？第一个。P如果越小就P相对于Q来说，如果越来越小的话，那是不是这个V就倾向于走回头路？

PPT视角

03:16:45

好，那这个时候他就倾向于DFS。啊，不说错了，倾向于BF S。它倾向于深度优先搜索。他更倾向于搜索原先的节点范围的那些邻居的细节信息。好。如果Q。更小的话就Q比P要小的多的话呢？他就更倾向于走向远方那些没有探索过的区域。这个连。这个X一跟这个？这个T是相邻的，我都不去探索了，我要探索那些，我从来没见过那个T都没见过的地方，它就会越走越远，越走越远。他就倾向于往远方去探索。

PPT视角

03:17:28

早会走过的路。那这个P跟Q就是综合起来，就是这么来控制它的游走方式。然后后面的事情呢，就不用再解释了。那大家都明白是什么意思哈，就后面就是用类似于我的那个思路来得到它的embedding。那得到节点的embedding以后做什么事情呢？那第一个我可以给节点做一个分类，那我把这个音标点送到一个分类器里面去分类就行了，对吧？另外，我可以计算这些节点的相似程度啊，我对这个embed等向量呢，求它一个夹角余弦或者求它的距离，那我不就可以求相似度了吗？啊，那所以这个问题就被我们这样解决了啊！那这个的作者呢？呃，这个作者是斯坦福大学的。这两个作者都是斯坦福大学的。那他们曾经在斯坦福大学里面讲过一门很有名的课程。那这门课程呢，就叫就叫做突击去学习。

PPT视角

03:18:27

那大家应该可以在斯坦福大学网站上面呢，看到这个课程。那当然你要听懂它那个就有点不容易了，它里面这个起点比较高，又是用英语讲课，又用到这种很多各种方面的这个知识，听懂它不容易。这个，而且这个作者呢，感觉讲课有点差劲，就是讲的不是很容易明白，但是大家如果好奇这个突击学习，你想学更多的更深入的东西。那你可以找一找那个课程来看啊，那这个次短服的那个课程的讲师就刚好就是ltovt的作者。哈哈。好，那我一口气就讲了<外文>，那我们先停一下，看看大家有没有什么问题啊，关于这个图嵌入。可以接受问题了哈！

PPT视角

03:19:33

喂喂。做嵌入的目的就是为了他的下游任务。就上游任务就相当于嵌入，你把能够把图里面的一个节点。能够变成一个向量？那这个我们下游任务呢？比如说我们要做节点分类。或者说做节点相似度计算，那我们就用这个嵌入来完成它。

PPT视角

03:20:00

嗯。啊，有语义的，它里面体现了网络的结构，它那个图的结构。

PPT视角

03:20:22

啊，那个是很容易理解，因为这个图稍微抽象，我明天讲到知识图谱的时候，我可以给大家讲讲具体的意义。

PPT视角

03:20:28

比如说我判断这个两个节点的相似度的话呢，那主要就是判断这个，呃，这两个实体是不是是同一个实体，我们要把它合并。

PPT视角

03:20:38

哈哈。喂贝贝听到吗？难道麦关了吗？

PPT视角

03:21:01

啊，这个那我们先不提问了，那我们就继续吧。好，下面呢，我们可以讲这个图神经网络了。那这个是我们今天的这个最后一个内容哈，最后一个六。那图神经网络的意思呢？就是把图。或者图里面的元素，比如说这个节点跟边这些东西作为输入的东西。输进去一个神经网络里面去进行推断。那我们就推断了，可能是节点的类型啊，编的类型啊，或者说给他推推这个推算出一些数值。啊，或者一些这种。呃，其他的性质等等啊，啊，或者做嵌入等等啊，那这些都可以用图神经网络来做。那刚才也说过了，这个图神经的主要困难在于什么地方呢？那在于这个图比较常见的这种。表达形式呢是临街举证。而连接矩阵呢，是一个极其不稳定的结构，当你图里面的节点发生一点点变化的时候，那这个连接矩阵呢，就可能变成一个我们完全不能辨别的样子。

PPT视角

03:22:14

那所以如果我们要把临街矩阵作为输入的东西。那我很可能这个神经网络呢，就要遍历所有的这种节点的置换。的相应的这个连接矩阵来加以训练。

PPT视角

03:22:27

那我很难想象，这里面的计算量会有多大，以及训练出来这个神经网络呢，最后能得到什么样稳定的效果，那这些都可以预想到它是。呃，应该是很差的。那所以这个不是一个很好的方法。那当然，后来呢，还有一种所谓的基于哺育的方法，就是基于那个拉普拉斯。这个举证的这个方法。啊，那这个呃，这个也是一个技巧，但是呢，由于这个拉普拉斯举着呢，它上市了这个图里面的很多的基本的信息。大概保留了只有关于这个联通性的一些分析。那所以它能够体现的信息量呢也比较有限。那因此这个用途呢也非常的狭窄啊，那这个我们就不用这个方法了。那下面呢，我们看一看这个真正的方法应该是什么样子了？那我在这里讲的内容啊？我就不介绍论文了，因为这个思想，它的是一个整个的发展过程，而不是某一天论文突然间天才的提出来的想法。那我这里有一个链接是这个<外文>这个网站里面的，这里面网，这个网站里面有很多精品。它里面的一个博客是2021年发表的。它是关于这个图神经网络的一个入门性的介绍。

PPT视角

03:23:43

那大家可以看一下这一篇。博文。那这篇博文呢？我相信这个作者呢，是花了很多很多时间去做了。那何以见得呢？他做了很多动画。当你用鼠标点向其中一个节点的时候呢，它其他节点会发生相应的变化，使到你很容易明白它的计算的逻辑是怎么样的啊，那这个相当的花心思啊，那这个大家可以看看。另外呢，在这个youtube上面呢，那个李木。关于这篇博文，还有一个讲解文章，那这个有点搞笑了，那这个博文他还这个煞有介事的去讲解。那大家如果想听有人讲这篇文章，而不是自己看的话呢，你也可以到到这个油管上面去搜索一下，你找个礼物。

PPT视角

03:24:26

然后gnn你搜索一下李木gnn啊，那大概就能够找到找找到他那个视频，那你也可以看一看那个视频。那个视频呢？讲了很啰嗦，这个讲了差不多一个小时吧，哎呀，我觉得根本没必要那么啰嗦，我下面用几分钟时间，我就可以把这个图神经网络的思想给大家讲完啊。好，那我举个例子吧！我就比如说我是这个银行金融机构。我现在要挖掘优质客户。那这个优质客户呢？首先你每个客户都有一些自己的特征是吧？那这个特征比如说啊，你存款多少钱了？呃，这个你是什么工作的性质啊？你是不是公务员啊？你你是不是一个企业主啊？呃，还有这个你的这个消费平均每个月这个签信用卡欠了多少钱啊？等等。那这个是你自己个人的特征？那这里我用一条蓝线来表示的这个特征向量。就每个人都有自己的一些特征。

PPT视角

03:25:24

那理论上呢，我依靠这些你自己的特征。呃，其实也能判断你是不是一个优质用户了？但是呢，我希望能够做的更准确。因为有一些人可能是才不露眼。或者说他的消费行为可能跟其他人很不一样。因为实话说的花的多钱的人未必是富豪。花的钱少的人未必不是富豪。那我也见过很多很抠门的富豪，他一天也不用花多少钱，但是他确实很有钱啊，那所以这个光是凭这些特征信息来这个分析呢，还是不够完善。

PPT视角

03:26:02

那是我们还希望根据什么根据朋友圈。那我们的结论就是，如果有一个人，他朋友圈里面的人都很有钱。那理论上他应该也会挺有钱。那大概是这个意思，那至于这个朋友圈在银行数据里面怎么体现呢？那这个就不关我的事了，那你可能是买回来的数据。

PPT视角

03:26:21

比如说这个某个银行像腾讯买回来的一堆数据啊，知道这朋友圈有什么人。或者说我根据这个转账记录，那因为你这个朋友之间可能有一些转账的关系啊，等等啊，对吧，那我也可以建立这种朋友圈的数据。那我们就形成一幅图啊，这里abcde是五个人，每个人都有他自己的一套特征向量来描述他的这个性质。然后他们之间有一条线就代表他们之间是朋友啊！他们这个是好友关系。那我们甚至可以给这个边给它一些性质，给它一些强度。来描述这个事情等等。那下面呢，我们看一下这个图神经网络是怎么做的？好，这个图神经网络呢？出乎大家意料以外，它不是一个网络。我先看这个针对顶点的。这个图神经网络。

PPT视角

03:27:12

它是每个顶点都有一个图神经网络。每一个顶点都有一个神经网络，那这个神经网络是什么呢？他也不是什么。非常奇怪的，像什么卷积神经网络之类的。就是一个简单的ML P。就是一个多层的bp神经网络就可以了。一个多层的壁，每一个节点都有一个。好，那我们再看下一步啊！再看下一步。好，那现在关键的操作来了。比如说我现在要推断B一顶点这个人，他是不是一个优质用户？那优质用户就我们这个神经网络就倾向于输出一。不是优质用户就输出零，那大概率它会输出一个零点几这样的数字。那大概就是它有多少的概率，是一个优级用户这个意思啊。好，我们看一看这个图，神经网络图，神经网络跟其他的神经网络有什么不一样呢？首先每个顶点一个bp神经网络。

PPT视角

03:28:23

这些bp神经网络的全职全部是一样的。就是每一个B。都是一样。都是一样的，它的权重是相同的。啊，权重相同！好，然后呢，在这个BP神经网络进行推断的时候。比如说我输入这个特征向量以后得到第一层。然后第一层啊，第一层再推进去，第二层是吧？在第二场的时候呢？这个他不光接受从第一层传过来的信息。他还会接受从其他相邻节点。送过来的第一层的信息，比如说B的相邻节点。

PPT视角

03:29:07

有a跟C。那其他不相邻的，我们就不考虑跟我没关系。那我要把a跟C的第一层。首先做一个汇聚的工作，那最简单就加起来，或者求平均值。那也可以求？那大家一听这个诶，这个不就是卷积神经网络里面那个迟化的那个工作差不多嘛，是吧，那其实它就是一个迟化。那我这个这个名词也叫词。它会把相邻节点的上层数据汇集在一起，做一个磁化的工作。然后把这个迟化后的结果。作为一个输入也输入到这边的第二层。就是说在这个节点里面的第二层呢，不光接受第一层的输入，也接受其他顶点的网络，第一层传过来的信息。那这些信息呢？你可以？对他做一个加权也行。就说哪些节点重要，哪些节点不重要，那这个加权可以是你指定的。也可以是学习回来的，那如果是学习回来的话呢，就称之为叫图神经网络的注意力机制。

PPT视角

03:30:20

这里又有一个注意力机制啊，学习回来的话就叫注意力机制。还有一点。是不是一定相邻的节点我们才汇聚呢？这个其实也是很灵活的，还真不一定。你可以不一定是相邻的节点，你可以是隔两层的节点都可以，就是我画一个范围啊。这个范围里面呢，可能是有些跟我相邻一跳的。有些是相邻两跳的。那这些都可以汇聚过来？然后汇聚过来的时候呢，可以赋予不同的权重。

PPT视角

03:30:54

就比如说相邻一圈的，我给你权重大一些。相邻两圈的我给你权重小一些。

PPT视角

03:31:00

那这个权重可以人为指定，也可以通过学习来进行，这些都是很灵活的。总而言之，这个GNN的核心巧妙之处就在于汇聚。就在于汇聚。而汇聚就体现了图形。而它通过每一个顶点设置一个BP的方法。避免了考虑连接举证这么复杂的东西。我们就不需要林杰举证了，不需要管林杰举证了。那我只需要在汇聚的时候找出相邻的节点，那这个可以通过图计算来进行，对吧，然后把他们的这个相关的信息汇聚过来。然后我们再训练下一步就可以了。那这个汇聚以后的数据送到这里来的话呢，其实可以经过一个矩阵的计算。呃，我这里没有写。这里汇聚以后。这里哦，这里是WE，这里应该有一个W二哦，这里写了W二。就WE是第一层到第二层啊。W二呢是这个汇聚过来的，经过一个矩阵的转换以后，才汇聚到第二层，再加起来，那一直往上传，然后每一层呢都重复我刚才所说的过程，既有下层往上层的传递，又有从旁边的节点汇聚过来的信息。

PPT视角

03:32:17

不光这个节点是这样，所有的节点都是这样。那最后呢，我就可以。得到一个这个。啊。这个最后就完成了这个mlp。然后最后我们怎么判断它的类别呢？一般来说，我们会接一个全年阶层。我们会接一个全年阶层。那这个全连接层呢？最后出来的结果就是一个soft mass向量反映的就是它的分类了。啊，那大致上呢就是。这样的一个情况。那这个最后的全连接层在其他每个节点都有，而且呢是共用。那这个gnn的思想呢？大概就是这个样子啊！那我们这里顺便也可以说一下这个。其他的。见。刚才是对节点来进行。今天。其实你对边也可以进行。甚至对这个整个图都可以做dna啊，那是怎么样做对边做gna呢？这个也是有点随意的。

PPT视角

03:33:18

比如说我给编。都定义相应的图神经网络每条边都有。那这个红色这个部分呢？代表的就是对应于边的这个图，神经网络图，神经网络。好，然后呢，我现在要对这条边做分类啊，或者做其他这种链接判别的事情。那我可以把。卷进来的那些东西都给它建网络，那卷进来的东西呢？包括这条边的两个端点啊，是吧？两个端点啊，分别这里有蓝色的这个网络来对应。对应于端点的网络跟对于边的网络可以是不一样的，但是对应于边的网络，11般来说是全部都是一样的，对应于顶点的网络一般来说也是全部都是一样的。好，那这里的话呢，这个呃。

PPT视角

03:34:08

那我们要卷入的话呢，就是这条边还有跟它相邻的边。就是有公共节点，那边还有这个它的两个顶点。那这些网络呢，我们都都是。然后它的整个推断的过程呢，也跟刚才是一样，那我们在。考虑一层的那个呃数据的时候呢，除了考虑从上层传过来的数据以外呢，还要考虑。从其他节点或者编汇聚过来的上一层的数据的信息。那我们一起来进行下一层的推断。那这个GNN就是这个样子啊！那你你也不用太死板，那刚才说过了，这个是很灵活的，你要汇聚哪些节点，哪些边这个完全是由你自己来判定。有时候呢，甚至会汇聚整张图的信息。就整张图它可能有一些特征啊，前面不是讲过图的一些。整体的特征吗？比如说这个度的分布。呃，什么还有聚集分布？啊，这个等等啊，那这些都是图整体的特征，那你也可以把它作为这个神经网络的一部分来考虑啊，那这些都是可以的。那这个GNN大概就是这么一种结构。那这里特别要说一下呢，那这个GNN也是可以拿来做嵌入的。

PPT视角

03:35:29

那相应于刚才所讲过的这个浅层嵌入，我们可以称之为叫深层嵌入。那这个深层嵌入呢，可以得到比随机游走。更丰富的信息，那随机游走的话呢？你大概只能够得到这个图结构的信息。你很难想象他还能得到其他什么信息？那除非那个权重是体现了一定的实际意义，对吧？但是呢，这个基于GNN的深层嵌入了。除了这个图结构的信息以外呢，还有它的一些个人的一些特征，向量的信息也会被包含在内。因为这些信息都是卷入的。那这个怎么拿到它的这个嵌入呢？很简单，最后不是有一个全年阶层吗？

PPT视角

03:36:15

那我在全连接层里面拿出它的这个倒数第二个层就softmas前面那个层把它拿出来。那个就是一个<外文>啊，那这样做就可以了。

PPT视角

03:36:28

好，那这个关于图机器学习就这个样子。那图机器学习听起来好像好复杂，每一本书呢，都讲了很多东西，那它其实讲了无非就是这么几种。几种内容，第一个图的概念，它的例子，它的一些基本的性质，一些传统的计算像度啊，什么联通性啊等等啊，这些大家一看就会了，就不用啰嗦了。那第二个问题呢，是那个拉普拉斯矩阵，那很多书都花了很多篇幅去讲。啊，那这个也是值得一讲。那第三个呢，就是图怎么嵌入，有刚才讲的有浅层嵌入，有随机游走的嵌入。然后第四个呢，就是图神经网络这么一个事情。那这些神经网络呢，还可以进一步的这个。呃，推广到更远的范围，比如说我们不用一般的mlp我们用。这个卷积神经网络。那又又又演化出一种所谓图卷积神经网络等等，那这里面还是有很多这个可以发展的技巧的，但是它整个思路呢，大家应该是清楚了啊。

PPT视角

03:37:32

好，那我们今天上午的内容呢，就讲到这里啊，那大家看。有没有什么问题我们可以对上午呢做一个回顾。哈哈。啊，没完全没有关系，完全没有关系。呃，你可以这样子，就是你可以把这个这个。刚才不是说这里面有mlp嘛，对吧，你可以把mp变成一个transform了，这个肯定也是可以的。啊，这个穿石方法是统一天下，但是它没有统一到图。就是它不能体现，呃，对对，它不能体现图的连接关系，就图的这种关系它是很难。在这个神经网络这种类型里面体现出来的。他他没有用，我刚才所说这种每一个顶点呢，每条边一个神经网络，这种方法才是它的最佳的体现。

PPT视角

03:38:29

哈哈。好，大家看还有没有问题啊？就看起来，目前它跟其他的网络都相差的比较远。呃，也有一些文章呢分析啊，某个某个网络呢，其实就是gnn。但是呢，我真的没有怎么去仔细研究这些东西。哈哈哈。那至于我们今天讲的这些图七学习有什么用？那明天我们讲到知识图谱了，就大家就可以看到了。那知识图谱里面呢，基本上就用这一套东西了啊。下午呢，何老师会给大家讲一个图，基金学习的包叫pypyg。这个python里面专门用来做继续学习的。那这个大家到时候可以看一看。那像这个ltovt啊，这些都有现成的包了，有现成的函数可以使用。大家还有没有问题啊？那没有问题的话呢，我们今天早上就先到这了，下午两点钟开始啊，大家准时过来，谢谢啊！那我们明天。再继续。

王文璞

03:40:01

所以。喂，老师。

PPT视角

03:40:04

诶，听到。

王文璞

03:40:08

呃，我我查一个问题啊，就是。呃，就刚才咱们讲到这个图，神经网络的时候，每个节点它都要去做一个自己的身，呃，网络，然后边也要做一个这样的话，它如果说我们的节点数量，比如说。

PPT视角

03:40:11

哦。对。对，就是你认为嗯！对。

王文璞

03:40:25

呃，成千上万之后那他这个训练量岂不是就爆炸了？

PPT视角

03:40:25

嗯。呃，因为你只需要考虑相邻的，所以也没多少不相邻的，你不需要考虑。

王文璞

03:40:35

什么叫相邻的考虑？

PPT视角

03:40:37

对，你只需要考虑相邻的就行。

王文璞

03:40:48

但是你有一些他可能是都都相邻。

PPT视角

03:40:49

那这个就考虑的太多了，它它应该会有控制。好啊。

王文璞

03:40:53

他可有他，因为那你是说就比如说那种比较深度的节点，我就不去再考虑了是吧，我只是考虑不常在。

PPT视角

03:40:57

嗯。对，就是这样的，我刚才所讲的这个图神经网络呢，它只是做做，比如说它做了一个节点判别，我说的是优质用户判别嘛，对吧，那就是针对某一个用户判别它是不是优质。那在判别这个用户的时候呢，除了他自己这里有一个神经网络以外呢，我们还要考虑的是跟他相邻的那些用户的这个神经网络，那考虑这么几个神经网络就可以了。那除非他有很多朋友，否则这个计算量的增加应该还不算很大。那当然，你要把整张图里面的每一个用户都判断一下来呢，这个也还是需要有一些时间。

PPT视角

03:41:41

我不知道我讲明白了没有？

王文璞

03:41:45

哦，好的好的。

PPT视角

03:41:47

好，大家还有问题吗啊？嗯。你。

王蓓

03:42:05

暂时没有同学的，还再提举手提问了。

PPT视角

03:42:11

贝贝是不是有点不同步啊？这个我刚还在跟一个同学对话，那你又说没有同学提问，你说有点不同步啊，这个。

王蓓

03:42:18

对。

PPT视角

03:42:23

喂，刚才那个同学？

王蓓

03:42:24

<外文>那位同学提问之后的话呢，没有新的同学在线上再举手提问了。

PPT视角

03:42:29

刚才不是正在讲吗？

王蓓

03:42:31

对对对。

王蓓

03:42:32

刚刚讲完了。

PPT视角

03:42:35

刚才那个问题是什么来着，就不用预先选定优质的用户，这个你可以在全全量的用户来来算都行，不一定要选定。嗯。啊，训练的时候你当然要标定了，就是要标定一些已知它是优质的，或者已知是劣质的，你要先把它训练好，然后才能做推断。嗯。对，这个张琳是一个判别器的。

PPT视角

03:43:05

对，没错，是是是就是这个意思对。说得对啊。对，可以啊，这样就是说你是首先人工甄别一批。优质或者劣质的客户，然后我们就可以训练出一个分类器。然后可以进行这个推荐系统啊，这类事情。好的，谢谢。没有其他问题的是吧，那我们就先到这了，这个明天再见啊，谢谢大家，再见。

PPT视角

03:43:55

呃，那这个你要考虑一下你这个设计的就是说，比如说你这个图很大，里面有商品，有用户，那你可以把它过滤到只剩下用户。然后呢？这个怎么选？啊，我这里说的只是银行的例子，对用户只跟商品有关系，那这个可能有有那个场景来设计。就是说你，你取得这些所谓的相邻啊，这些都是可以由你自己来定义的，刚才说过这个事情。好的。哦，对。对。啊，这个有可能啊，就是就是可能有一些其他的纽带吧，这个。呃，虽然确实这个挺广，用的挺广，就是因为因为这个自然语言本身其实就是图上的一个序列嘛。那所以它肯定跟图本身也有联系。序列不就是一个特别的，就是一个特殊的土木。哦，行，我回家吃饭。O. 行，好好再见啊，行，那没有问题就先到这了哈，请明天明天再回答问题，谢谢啊！